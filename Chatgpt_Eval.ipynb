{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1CKSbDm4lYYiVedurzxIpWHD70aJl_JlL","authorship_tag":"ABX9TyOLvjSi4M310dn9XwPamT5W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"42c7fa63eb134de2bef881f79d5a5fbc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd876aa1e3384c778e2da7f1f33093d8","IPY_MODEL_301ecbf283bf472081bac8dfe0d723e1","IPY_MODEL_b138be0d5a0d4a9aa97f5fc78fc327b7"],"layout":"IPY_MODEL_c7cd98e5f72a4c0683dbdd5bdb87ca06"}},"dd876aa1e3384c778e2da7f1f33093d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_563fa65ae1304d5fa1308f6eaa27aee4","placeholder":"​","style":"IPY_MODEL_51a156f8e8b040cd966d04ccc407d569","value":"Fetching 5 files: 100%"}},"301ecbf283bf472081bac8dfe0d723e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbcca26f64134c1198f988e6b760d358","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac4f0495e0ef47bf9d4d1ce1dab57be0","value":5}},"b138be0d5a0d4a9aa97f5fc78fc327b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75603090807447f38c02a749c820d782","placeholder":"​","style":"IPY_MODEL_4d05454883f24b8e8363deea5f7c5466","value":" 5/5 [00:32&lt;00:00,  8.80s/it]"}},"c7cd98e5f72a4c0683dbdd5bdb87ca06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"563fa65ae1304d5fa1308f6eaa27aee4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51a156f8e8b040cd966d04ccc407d569":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbcca26f64134c1198f988e6b760d358":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac4f0495e0ef47bf9d4d1ce1dab57be0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75603090807447f38c02a749c820d782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d05454883f24b8e8363deea5f7c5466":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e033168d88e64b54a636499bfc6426b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffd19a63ccbd4b28b941e180ca5154a7","IPY_MODEL_139645b0aeb74c90b1f9582938287c45","IPY_MODEL_a393eea97c3a4d6c9b7b28c7057a7ae9"],"layout":"IPY_MODEL_e487d8c239d04706b9f746f014a5eaaa"}},"ffd19a63ccbd4b28b941e180ca5154a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86094d409aa54b9ea54cca9a13f5d19b","placeholder":"​","style":"IPY_MODEL_0e5964090d9e4ebab6ad62d500e97fe9","value":".gitattributes: 100%"}},"139645b0aeb74c90b1f9582938287c45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_35febd1f8ec54683b48eb8bacff75209","max":1477,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51ef05663000482e9646c9d136fd34b4","value":1477}},"a393eea97c3a4d6c9b7b28c7057a7ae9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3e105b6fee0486690f390f853331b49","placeholder":"​","style":"IPY_MODEL_fa1c387fa0f54811b1506fd9bc779dad","value":" 1.48k/1.48k [00:00&lt;00:00, 75.1kB/s]"}},"e487d8c239d04706b9f746f014a5eaaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86094d409aa54b9ea54cca9a13f5d19b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e5964090d9e4ebab6ad62d500e97fe9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35febd1f8ec54683b48eb8bacff75209":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51ef05663000482e9646c9d136fd34b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3e105b6fee0486690f390f853331b49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa1c387fa0f54811b1506fd9bc779dad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ef691287f4d4428905bd9bec8c7559b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02d782f9f9074a3291150114a1ecdfb8","IPY_MODEL_27dcabeaa2634c1c916a0e47a8254fa3","IPY_MODEL_3aab1f6b2eff4720b5c100f83346b6ee"],"layout":"IPY_MODEL_8c4301709853407b869e74ec3378cc97"}},"02d782f9f9074a3291150114a1ecdfb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b80a084ee8ac42eb8e150744ce870e14","placeholder":"​","style":"IPY_MODEL_a71c1e42a5734318bd2fa87908a839e5","value":"hparams.yaml: 100%"}},"27dcabeaa2634c1c916a0e47a8254fa3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9718a84cb13f4f8881b7c7c41a6d4cb4","max":567,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f4afa38500846dcaab3e0596f1422a7","value":567}},"3aab1f6b2eff4720b5c100f83346b6ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bf01b8d639a405db41eb3923fbc3389","placeholder":"​","style":"IPY_MODEL_f6afaa04b10546b3a948f877d52b74cb","value":" 567/567 [00:00&lt;00:00, 16.3kB/s]"}},"8c4301709853407b869e74ec3378cc97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b80a084ee8ac42eb8e150744ce870e14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a71c1e42a5734318bd2fa87908a839e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9718a84cb13f4f8881b7c7c41a6d4cb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f4afa38500846dcaab3e0596f1422a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bf01b8d639a405db41eb3923fbc3389":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6afaa04b10546b3a948f877d52b74cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"850d2a8086824293b4a5afbb88e07b67":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8989cbbd31f04d48b9e4a82824dc7971","IPY_MODEL_91b8ce3dfc464052b8ac9bde72785c60","IPY_MODEL_59cc93dc24e84d229e3fb467683c0484"],"layout":"IPY_MODEL_e65c7a8efc724c3f9f64dc9e2d469335"}},"8989cbbd31f04d48b9e4a82824dc7971":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86b94d6b288547999c282726ff0519b5","placeholder":"​","style":"IPY_MODEL_00bb9f7c047641fb814bc4804b311425","value":"LICENSE: 100%"}},"91b8ce3dfc464052b8ac9bde72785c60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8c57aa942284ab9ba1240d1d2e04661","max":9693,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1e9bd98d1884655a5002beef2e4eb85","value":9693}},"59cc93dc24e84d229e3fb467683c0484":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad9b44b4017a4548bc1abab025893c54","placeholder":"​","style":"IPY_MODEL_a01f5490f83f4f278f0c846b261b212a","value":" 9.69k/9.69k [00:00&lt;00:00, 240kB/s]"}},"e65c7a8efc724c3f9f64dc9e2d469335":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86b94d6b288547999c282726ff0519b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00bb9f7c047641fb814bc4804b311425":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8c57aa942284ab9ba1240d1d2e04661":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1e9bd98d1884655a5002beef2e4eb85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad9b44b4017a4548bc1abab025893c54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a01f5490f83f4f278f0c846b261b212a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b982d60659594861a8dd93f767fbdf34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7338401ad61e440391e8ff42fc42d89a","IPY_MODEL_089624fbbeb945aaaddb8e61299c27e3","IPY_MODEL_99e1b1941aa144fbb59a28c8839b1eac"],"layout":"IPY_MODEL_9d1a1fd1617449b58c7e9cff4cfdd4a5"}},"7338401ad61e440391e8ff42fc42d89a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_687fd2c3740f4caaba71733857366732","placeholder":"​","style":"IPY_MODEL_30e3a235ae544523a26513710e0bdd89","value":"README.md: 100%"}},"089624fbbeb945aaaddb8e61299c27e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95d2a39c2ea44ee1955a82f43742b6a3","max":3401,"min":0,"orientation":"horizontal","style":"IPY_MODEL_709d50da7872486fa7f5752247d5caeb","value":3401}},"99e1b1941aa144fbb59a28c8839b1eac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12f02e9901b745828fc5039289f4cbe3","placeholder":"​","style":"IPY_MODEL_f6efbafc043944169fc2f78601159167","value":" 3.40k/3.40k [00:00&lt;00:00, 56.9kB/s]"}},"9d1a1fd1617449b58c7e9cff4cfdd4a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"687fd2c3740f4caaba71733857366732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30e3a235ae544523a26513710e0bdd89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95d2a39c2ea44ee1955a82f43742b6a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"709d50da7872486fa7f5752247d5caeb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12f02e9901b745828fc5039289f4cbe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6efbafc043944169fc2f78601159167":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b371b333d67405a818c48c7fcf64936":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61f0d1cfcb72447f851f547022752d12","IPY_MODEL_a2c0b9041cdd4ca2bf3fc44d130ee2d4","IPY_MODEL_2755aa88db5946eda325d6689fec948c"],"layout":"IPY_MODEL_c0440ebf27154e2f9c26d8c2bf691410"}},"61f0d1cfcb72447f851f547022752d12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c903e186c244531b8d87504de64940f","placeholder":"​","style":"IPY_MODEL_5aa0112263b943f3a7bb88c1b3338062","value":"checkpoints/model.ckpt: 100%"}},"a2c0b9041cdd4ca2bf3fc44d130ee2d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9e81b8ad15140d9b3b452455eb15418","max":2323649073,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1ed87c5e77744cb9cf0a74f92572787","value":2323649073}},"2755aa88db5946eda325d6689fec948c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ed3649093d24b2483f025bcadfee0ce","placeholder":"​","style":"IPY_MODEL_cb2a4f828e174a3ba55392bb3d716369","value":" 2.32G/2.32G [00:32&lt;00:00, 150MB/s]"}},"c0440ebf27154e2f9c26d8c2bf691410":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c903e186c244531b8d87504de64940f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5aa0112263b943f3a7bb88c1b3338062":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9e81b8ad15140d9b3b452455eb15418":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1ed87c5e77744cb9cf0a74f92572787":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ed3649093d24b2483f025bcadfee0ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb2a4f828e174a3ba55392bb3d716369":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1645133610e545de9e32ade547dc9e39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a24896f38fc54f5ba8efb456ed77cd9e","IPY_MODEL_11cdc70e156f41598fa781f0581f73c4","IPY_MODEL_71f12034a9ae4b7fb1aafb2502b97d62"],"layout":"IPY_MODEL_e2a813b38b4a49b3a3baed70046b7ab8"}},"a24896f38fc54f5ba8efb456ed77cd9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f889d962ffed4654b74f2f4aa2f985af","placeholder":"​","style":"IPY_MODEL_81191049abb345b08d31d5d41e53ee39","value":"tokenizer_config.json: 100%"}},"11cdc70e156f41598fa781f0581f73c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d786a22d7c8346808afb9ebb536fbd19","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02d87b076cbf42c4abaf89f62887d39b","value":25}},"71f12034a9ae4b7fb1aafb2502b97d62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_136f971e750f42aea3a61dd0acb1c8f5","placeholder":"​","style":"IPY_MODEL_7026403136fe456ba9a22fe71b55f8be","value":" 25.0/25.0 [00:00&lt;00:00, 1.15kB/s]"}},"e2a813b38b4a49b3a3baed70046b7ab8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f889d962ffed4654b74f2f4aa2f985af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81191049abb345b08d31d5d41e53ee39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d786a22d7c8346808afb9ebb536fbd19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02d87b076cbf42c4abaf89f62887d39b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"136f971e750f42aea3a61dd0acb1c8f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7026403136fe456ba9a22fe71b55f8be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0188008a558482dbd36bc3fcf561aa7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa529fc334274bdbac7a7bf33d5ea72c","IPY_MODEL_554b23dddc3344db9a159f1faf33c4fc","IPY_MODEL_36dbaf40c18b4335a5c78b21e72c9351"],"layout":"IPY_MODEL_8174c7472e6347959a7b4e370e321d35"}},"aa529fc334274bdbac7a7bf33d5ea72c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd77c527ad1c48f0a14266b60ae6b991","placeholder":"​","style":"IPY_MODEL_3b7fabdff17c4705a76e9ddd86753cf2","value":"sentencepiece.bpe.model: 100%"}},"554b23dddc3344db9a159f1faf33c4fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_64ced371b57f4aa196c665b290a3aa08","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ec202c4e9f8470c8bd7090eba0cb35c","value":5069051}},"36dbaf40c18b4335a5c78b21e72c9351":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdd528d4607c431a9c8d3569b43edf34","placeholder":"​","style":"IPY_MODEL_827501faa57d47dbbd9d54d474d48a95","value":" 5.07M/5.07M [00:01&lt;00:00, 4.67MB/s]"}},"8174c7472e6347959a7b4e370e321d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd77c527ad1c48f0a14266b60ae6b991":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b7fabdff17c4705a76e9ddd86753cf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64ced371b57f4aa196c665b290a3aa08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ec202c4e9f8470c8bd7090eba0cb35c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cdd528d4607c431a9c8d3569b43edf34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"827501faa57d47dbbd9d54d474d48a95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f134a6c0f16f421f8f552d586e18a4b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_37f58ee262ba468da2d4f0e765de569f","IPY_MODEL_1dfbda072e45400c9c3d88ff6175b82d","IPY_MODEL_3aaefc643621468e817f3afe25432054"],"layout":"IPY_MODEL_e1b03fca80ed4d42a25e23f350c2ee46"}},"37f58ee262ba468da2d4f0e765de569f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4ba12a3453c4e0f87b297a2e2cf7a05","placeholder":"​","style":"IPY_MODEL_9241ba8d2f524d9db9d30c3b107fd4da","value":"tokenizer.json: 100%"}},"1dfbda072e45400c9c3d88ff6175b82d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8df68971b3924396bd9de6921709d387","max":9096718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d7c504bb55b4937a31a8ad010842b24","value":9096718}},"3aaefc643621468e817f3afe25432054":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c75e11cf5b0547ee9d9ed833665d6014","placeholder":"​","style":"IPY_MODEL_0bb9f7e4d82a468a99fdcd39ed1b3df8","value":" 9.10M/9.10M [00:00&lt;00:00, 30.6MB/s]"}},"e1b03fca80ed4d42a25e23f350c2ee46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4ba12a3453c4e0f87b297a2e2cf7a05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9241ba8d2f524d9db9d30c3b107fd4da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8df68971b3924396bd9de6921709d387":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d7c504bb55b4937a31a8ad010842b24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c75e11cf5b0547ee9d9ed833665d6014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bb9f7e4d82a468a99fdcd39ed1b3df8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"393be7e531b849568652f7d0c4811760":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39715cf821e44a298b0ef75e37ca4cc5","IPY_MODEL_992269407483458f8098ecf67015152f","IPY_MODEL_68704abd976d450a82b994ecc3ec0f11"],"layout":"IPY_MODEL_15411ce5518445bb97fbbd8a28cc7c34"}},"39715cf821e44a298b0ef75e37ca4cc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13c36725b38243c5929248cd83d8a1cf","placeholder":"​","style":"IPY_MODEL_93c37798c4534e6aa2f7885980486bb4","value":"config.json: 100%"}},"992269407483458f8098ecf67015152f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd741f582eee4c578d36ab84e8be19da","max":616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a79fb2ddcf443e0b20982483153d1eb","value":616}},"68704abd976d450a82b994ecc3ec0f11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4faffbbe4309416a90ff1afdbc3337de","placeholder":"​","style":"IPY_MODEL_75a647f651624cfba69585b62d851584","value":" 616/616 [00:00&lt;00:00, 49.7kB/s]"}},"15411ce5518445bb97fbbd8a28cc7c34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13c36725b38243c5929248cd83d8a1cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93c37798c4534e6aa2f7885980486bb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd741f582eee4c578d36ab84e8be19da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a79fb2ddcf443e0b20982483153d1eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4faffbbe4309416a90ff1afdbc3337de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75a647f651624cfba69585b62d851584":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["%pip install unbabel-comet\n","%pip install langchain_community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hh1GgZsdnjwn","executionInfo":{"status":"ok","timestamp":1750505495960,"user_tz":-330,"elapsed":133451,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"outputId":"98d4c121-f338-4d99-8723-4a17bfde319d","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unbabel-comet\n","  Downloading unbabel_comet-2.2.6-py3-none-any.whl.metadata (19 kB)\n","Collecting entmax<2.0,>=1.1 (from unbabel-comet)\n","  Downloading entmax-1.3-py3-none-any.whl.metadata (348 bytes)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet) (0.33.0)\n","Collecting jsonargparse==3.13.1 (from unbabel-comet)\n","  Downloading jsonargparse-3.13.1-py3-none-any.whl.metadata (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy<2.0.0,>=1.20.0 (from unbabel-comet)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet) (2.2.2)\n","Collecting protobuf<5.0.0,>=4.24.4 (from unbabel-comet)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting pytorch-lightning<3.0.0,>=2.0.0 (from unbabel-comet)\n","  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n","Collecting sacrebleu<3.0.0,>=2.0.0 (from unbabel-comet)\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet) (1.15.3)\n","Requirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet) (0.2.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet) (2.6.0+cu124)\n","Collecting torchmetrics<0.11.0,>=0.10.2 (from unbabel-comet)\n","  Downloading torchmetrics-0.10.3-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: transformers<5.0,>=4.17 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet) (4.52.4)\n","Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.11/dist-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (1.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.1->unbabel-comet) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n","Collecting lightning-utilities>=0.10.0 (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Collecting portalocker (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2024.11.6)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n","Collecting colorama (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (5.4.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->unbabel-comet)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->unbabel-comet) (1.3.0)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.5.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.11.15)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (75.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->unbabel-comet) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2025.6.15)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.20.1)\n","Downloading unbabel_comet-2.2.6-py3-none-any.whl (90 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonargparse-3.13.1-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading entmax-1.3-py3-none-any.whl (13 kB)\n","Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n","Installing collected packages: protobuf, portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, lightning-utilities, jsonargparse, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, entmax, pytorch-lightning, unbabel-comet\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed colorama-0.4.6 entmax-1.3 jsonargparse-3.13.1 lightning-utilities-0.14.3 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 portalocker-3.2.0 protobuf-4.25.8 pytorch-lightning-2.5.2 sacrebleu-2.5.1 torchmetrics-0.10.3 unbabel-comet-2.2.6\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","numpy"]},"id":"6f51504e7df5439781b56002f8510df8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting langchain_community\n","  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n","Collecting langchain-core<1.0.0,>=0.3.66 (from langchain_community)\n","  Downloading langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n","Collecting langchain<1.0.0,>=0.3.26 (from langchain_community)\n","  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n","  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n","Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.66-py3-none-any.whl (438 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain_community\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.65\n","    Uninstalling langchain-core-0.3.65:\n","      Successfully uninstalled langchain-core-0.3.65\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.25\n","    Uninstalling langchain-0.3.25:\n","      Successfully uninstalled langchain-0.3.25\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.26 langchain-core-0.3.66 langchain_community-0.3.26 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"]}]},{"cell_type":"markdown","source":["# 🧪 Multilingual Translation Evaluation Framework with ChatGPT & COMET\n","\n","This notebook evaluates English-to-multilingual translations using different prompting strategies with GPT-4o. It assesses both fluency (via COMET) and factual accuracy (via named entity preservation).\n","\n","---\n","\n","## 🔍 Evaluation Metrics\n","\n","### 1. COMET Score (Semantic Quality)\n","COMET (Crosslingual Optimized Metric for Evaluation of Translation) is a neural metric that compares model-generated translations with human references.  \n","- Model used: `Unbabel/wmt22-comet-da`\n","- Scores range from 0 to 1 (higher is better)\n","- Captures fluency and semantic correctness\n","\n","### 2. Meta Score (Entity Preservation)\n","A custom metric that checks if named entities (like people, places, products) are preserved correctly.  \n","- Extracts entity mentions from references\n","- Compares them against predictions\n","- Reports accuracy as a percentage\n","\n","---\n","\n","## 💬 Prompting Techniques\n","\n","### Zero-Shot (Generic)\n","Basic instruction: “Translate the following English sentences into {target_language}.”  \n","No special guidance or examples.\n","\n","### Zero-Shot (Entity-Aware)\n","Adds instructions to carefully translate named entities consistently and accurately.  \n","Improves factual precision.\n","\n","### Few-Shot\n","Includes a few example translations (source + target) in the prompt to guide the model.  \n","Helps the model mimic correct patterns and handle low-resource scenarios.\n","\n","### Chain-of-Thought (CoT)\n","Instructs the model to internally perform reasoning steps:  \n","1. Identify named entities  \n","2. Classify their types  \n","3. Translate entities contextually  \n","4. Translate the rest  \n","5. Combine everything  \n","Returns only final output in clean JSON format. Boosts consistency and control.\n","\n","---\n","\n","## ⚙️ Pipeline Summary\n","\n","1. **Load input `.jsonl` files** from validation set\n","2. **Choose a prompt type** (zero-shot, few-shot, CoT)\n","3. **Translate in batches** (50 per request) with retry & rate limiting\n","4. **Save predictions** in structured JSON\n","5. **Evaluate** using:\n","   - `calculate_comet_scores()` for semantic fluency\n","   - `calculate_meta_score()` for entity translation accuracy\n","6. **Store results** per file in a `scores/` folder\n","\n","---\n","\n","## 📊 Example Evaluation Output\n","\n","```json\n","{\n","  \"correct_instances\": 45,\n","  \"total_instances\": 50,\n","  \"comet_score\": 0.872,\n","  \"meta_score\": 90.0\n","}\n"],"metadata":{"id":"95uWX4YF7-iN"}},{"cell_type":"code","source":["# ✅ ChatGPT-Compatible Framework for Translation Evaluation\n","\n","import os\n","import re\n","import json\n","from typing import Dict, List, Set\n","from comet import download_model, load_from_checkpoint\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# === Paths and Configuration ===\n","# Define training and validation data paths along with COMET model configuration\n","TRAIN_DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/semeval.train.v2-e0d1c28b78c8dd4969d25eea5d3bc9cc/semeval/train'\n","VALIDATION_DATA_DIR = '/content/drive/MyDrive/Colab Notebooks/semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation'\n","COMET_MODEL_NAME = \"Unbabel/wmt22-comet-da\"\n","COMET_NUM_GPUS = 1\n","COMET_BATCH_SIZE = 32\n","\n","# Entity types to be evaluated in entity-level accuracy\n","ENTITY_TYPES = [\n","    \"Musical work\", \"Artwork\", \"Food\", \"Animal\", \"Plant\", \"Book\", \"Book series\",\n","    \"Fictional entity\", \"Landmark\", \"Movie\", \"Place of worship\", \"Natural place\",\n","    \"TV series\", \"Person\"\n","]\n","\n","# === Utilities ===\n","# Convert language code to full name\n","def get_language_name(short_code):\n","    lang_map = {\n","        'ar': 'Arabic', 'zh': 'Chinese (Traditional)', 'fr': 'French', 'de': 'German',\n","        'it': 'Italian', 'ja': 'Japanese', 'ko': 'Korean', 'es': 'Spanish',\n","        'th': 'Thai', 'tr': 'Turkish', 'en': 'English'\n","    }\n","    return lang_map.get(short_code, short_code)\n","\n","# Compute COMET scores between predictions and references\n","def download_comet_model():\n","    path = download_model(COMET_MODEL_NAME)\n","    return load_from_checkpoint(path)\n","\n","def calculate_comet_scores(model, references_path, predictions_path):\n","    refs = _load_jsonl_data(references_path)\n","    preds = _load_jsonl_data(predictions_path)\n","    ids = set(refs.keys()) & set(preds.keys())\n","\n","    instances, idx_map, idx = [], {}, 0\n","    for i in sorted(ids):\n","        for t in refs[i]['targets']:\n","            instances.append({\n","                \"src\": refs[i]['source'],\n","                \"ref\": t['translation'],\n","                \"mt\": preds[i]['prediction']\n","            })\n","        idx_map[i] = [idx, idx + len(refs[i]['targets'])]\n","        idx += len(refs[i]['targets'])\n","\n","    print(f\"Created {len(instances)} instances\")\n","    scores = model.predict(instances, batch_size=COMET_BATCH_SIZE, gpus=COMET_NUM_GPUS).scores\n","\n","    # Compute max score for each sample and average across all\n","    max_scores = [max(scores[start:end]) for start, end in idx_map.values()]\n","    avg_score = sum(max_scores) / (len(max_scores) + len(preds) - len(ids))\n","    print(f\"Average COMET score: {100 * avg_score:.2f}\")\n","    return avg_score\n","\n","# Evaluate entity-level name translation accuracy\n","def calculate_meta_score(ref_path, pred_path, verbose=False):\n","    refs = _load_references(ref_path, ENTITY_TYPES)\n","    mentions = _get_mentions_from_references(refs)\n","    preds = _load_predictions(pred_path)\n","    acc = _compute_entity_name_translation_accuracy(preds, mentions, verbose)\n","    return acc['correct'], acc['total'], acc['accuracy'] * 100\n","\n","# === Internal Helpers ===\n","\n","# Load JSONL data with `id` as key\n","def _load_jsonl_data(path):\n","    return {json.loads(l)['id']: json.loads(l) for l in open(path, encoding='utf-8') if l.strip()}\n","\n","# Load references with filtering based on entity type\n","def _load_references(path: str, types: List[str]) -> List[dict]:\n","    data = []\n","    for line in open(path, encoding='utf-8'):\n","        if not line.strip(): continue\n","        record = json.loads(line)\n","        if not record['targets']: continue\n","        if types and not any(e in record['entity_types'] for e in types): continue\n","        data.append(record)\n","    return data\n","\n","def _load_predictions(path: str) -> Dict[str, str]:\n","    data = {}\n","    for line in open(path, encoding='utf-8'):\n","        if not line.strip(): continue\n","        record = json.loads(line)\n","        match = re.match(r\"Q[0-9]+_[0-9]\", record['id'])\n","        if not match: raise ValueError(f\"Bad ID: {record['id']}\")\n","        data[match.group(0)] = record['prediction']\n","    return data\n","\n","# Load model predictions with validation on ID format\n","def _compute_entity_name_translation_accuracy(preds: Dict[str, str], mentions: Dict[str, Set[str]], verbose=False) -> dict:\n","    correct = sum(\n","        1 for k, m in mentions.items()\n","        if k in preds and any(v.casefold() in preds[k].casefold() for v in m)\n","    )\n","    total = len(mentions)\n","    return {\"correct\": correct, \"total\": total, \"accuracy\": correct / total if total else 0.0}\n","\n","# Extract entity mentions from reference data\n","def _get_mentions_from_references(data: List[dict]) -> Dict[str, Set[str]]:\n","    return {d['id']: set(t['mention'] for t in d['targets']) for d in data}\n"],"metadata":{"id":"xWKHEkpEOyIe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import glob\n","import tqdm\n","import os\n","import time\n","from langchain.prompts import PromptTemplate\n","from langchain.chat_models import ChatOpenAI  # ✅ ChatGPT model\n","from tenacity import (\n","    retry,\n","    stop_after_attempt,\n","    wait_random_exponential,\n","    retry_if_exception_type\n",")\n","\n","\n","# Set your OpenAI API key (ensure it's stored securely in production)\n","os.environ[\"OPENAI_API_KEY\"] = \"\"\n","model_name=\"gpt-4o\"\n","# Instantiate the ChatGPT model (can switch between gpt-3.5-turbo, gpt-4, gpt-4o)\n","llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n"],"metadata":{"id":"xNoHsDFtOKCn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750505598403,"user_tz":-330,"elapsed":2088,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"outputId":"fcb7af5b-5cec-406d-f15a-f8b49a4f2031"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2-1901089282.py:20: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n","  llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n"]}]},{"cell_type":"code","source":["# Basic zero-shot translation prompt\n","ZERO_SHOT_PROMPT_TEMPLATE_STRING_1 = \"\"\"\n","Your task is to translate the following English sentences into {target_language}.\n","Input sentences are provided below as a JSON array of objects, each with an \"id\" and a \"text\" field.\n","Provide the translations as a JSON array of objects, where each object contains the original \"id\" and its \"translation\".\n","Maintain the original order of sentences from the input. Please return the json array without any additional text or formatting.\n","\n","Input JSON:\n","{source_texts_json}\n","\"\"\"\n","\n","# Named-entity-aware zero-shot prompt\n","ZERO_SHOT_PROMPT_TEMPLATE_STRING_2 = \"\"\"\n","Your task is to translate the following English sentences into {target_language}.\n","Input sentences are provided below as a JSON array of objects, each with an \"id\" and a \"text\" field.\n","Ensure that **all named entities** (e.g., people, organizations, locations, product names) are translated **correctly and consistently** into the target language.\n","Provide the translations as a JSON array of objects, where each object contains the original \"id\" and its \"translation\".\n","Maintain the original order of sentences from the input. Please return the json array without any additional text or formatting.\n","\n","Input JSON:\n","{source_texts_json}\n","\"\"\""],"metadata":{"id":"w9eCq-7mQkYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Rate limit for API calls (OpenAI recommends <20 RPM for gpt-4/gpt-4o)\n","REQUESTS_PER_MINUTE_LIMIT = 15\n","\n","# Delay to enforce between requests (in seconds)\n","DELAY_BETWEEN_REQUESTS_SECONDS = 60 / REQUESTS_PER_MINUTE_LIMIT  # 4.0 sec\n","\n","# Number of input samples to send per prompt\n","BATCH_SIZE = 50\n"],"metadata":{"id":"T2RZ38DRQlDf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Counter for API requests\n","api_request_count = 96"],"metadata":{"id":"j1fW1bbcQoRx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Phase 5: Translation Execution Logic\n","\n"," What This Does:\n","Sends a batch of source texts to the LLM with retry support\n","\n","Parses and verifies the returned JSON translations\n","\n","Handles malformed output with clear fallback errors\n","\n","Ensures reliable translation under API limits and occasional model inconsistencies."],"metadata":{"id":"vLqd2SAsyBpU"}},{"cell_type":"code","source":["@retry(\n","    wait=wait_random_exponential(multiplier=1, min=DELAY_BETWEEN_REQUESTS_SECONDS, max=60),\n","    stop=stop_after_attempt(5),\n","    retry=retry_if_exception_type(Exception)\n",")\n","def get_translated_content_with_retries(source_records_batch, target_language, llm_instance, template):\n","    global api_request_count\n","\n","    # Prepare JSON input for the prompt\n","    input_json_for_prompt = [{\"id\": rec['id'], \"text\": rec['source']} for rec in source_records_batch]\n","    source_texts_json_str = json.dumps(input_json_for_prompt, ensure_ascii=False)\n","\n","    # Format the prompt using the selected template\n","    prompt_to_send = template.format(\n","        source_texts_json=source_texts_json_str,\n","        target_language=target_language\n","    )\n","\n","    # Invoke LLM with the prompt\n","    response = llm_instance.invoke(prompt_to_send)\n","    api_request_count += 1\n","\n","    try:\n","        # Clean potential ```json markdown wrapper\n","        cleaned_content = response.content.strip()\n","        if cleaned_content.startswith(\"```json\") and cleaned_content.endswith(\"```\"):\n","            cleaned_content = cleaned_content[7:-3].strip()\n","\n","        # Parse model output\n","        translated_outputs = json.loads(cleaned_content)\n","\n","        # Validate expected structure\n","        if not isinstance(translated_outputs, list) or \\\n","           not all(isinstance(item, dict) and 'id' in item and 'translation' in item for item in translated_outputs):\n","            raise ValueError(\"Model did not return a valid JSON array of translation objects.\")\n","\n","        # Map translations back to original IDs\n","        translated_dict = {item['id']: item['translation'] for item in translated_outputs}\n","        ordered_translations = []\n","        for record in source_records_batch:\n","            ordered_translations.append(translated_dict.get(record['id'], \"ERROR: ID not found in JSON output\"))\n","\n","        return ordered_translations\n","\n","    # Handle different error scenarios with fallback error messages\n","    except json.JSONDecodeError as e:\n","        print(f\"JSON Decode Error: {e}. Raw response: {response.content.strip()}\")\n","        return [\"ERROR: Invalid JSON response from model\"] * len(source_records_batch)\n","    except ValueError as e:\n","        print(f\"Value Error (JSON format issue): {e}. Raw response: {response.content.strip()}\")\n","        return [\"ERROR: Invalid JSON structure from model\"] * len(source_records_batch)\n","    except Exception as e:\n","        print(f\"An unexpected error occurred while processing model output: {e}. Raw response: {response.content.strip()}\")\n","        return [\"ERROR: Unexpected issue processing model output\"] * len(source_records_batch)\n"],"metadata":{"id":"nRIo4qsyQrXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Folder containing input `.jsonl` files for validation\n","input_data_folder = \"/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation\"\n","\n","# List all .jsonl files in the folder\n","jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n","\n","# Create an output folder for saving model predictions\n","output_prediction_dir = os.path.join(\"/content/drive/MyDrive/Colab Notebooks\", \"data/predictions\", model_name, \"validation\")\n","os.makedirs(output_prediction_dir, exist_ok=True)\n"],"metadata":{"id":"8quUQGfiRNYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n","jsonl_files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yY7EDD9URrr_","executionInfo":{"status":"ok","timestamp":1750505608104,"user_tz":-330,"elapsed":34,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"outputId":"9167efcd-c9e3-4bcc-fdbd-85455e6cb986"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/de_DE.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/fr_FR.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/th_TH.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/ko_KR.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/es_ES.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/it_IT.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/ja_JP.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/ar_AE.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/tr_TR.jsonl',\n"," '/content/drive/MyDrive/Colab Notebooks//semeval.validation.v2-889a1492ba6c3791baa8f4224bc8e685/validation/zh_TW.jsonl']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["def zero_shot_eval(template, template_id):\n","    overall_pbar = tqdm.tqdm(jsonl_files, desc=\"Processing files\")\n","\n","    # Create subfolder for this specific prompt variant\n","    output_prediction_dir_1 = os.path.join(output_prediction_dir, template_id)\n","    os.makedirs(output_prediction_dir_1, exist_ok=True)\n","\n","    for file_path in overall_pbar:\n","        filename = os.path.basename(file_path)\n","        outfile_path = os.path.join(output_prediction_dir_1, filename)\n","\n","        # Read input records from JSONL file\n","        data_to_translate = []\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                data_to_translate.append(json.loads(line))\n","\n","        translated_results_for_file = []\n","\n","        # Process in batches\n","        for i in tqdm.tqdm(range(0, len(data_to_translate), BATCH_SIZE), desc=f\"Translating {filename} in batches\", leave=False):\n","            batch_records = data_to_translate[i : i + BATCH_SIZE]\n","\n","            target_locale = batch_records[0]['target_locale']\n","            target_language = get_language_name(target_locale)\n","\n","            try:\n","                # Run translation with retry support\n","                translated_texts_batch = get_translated_content_with_retries(\n","                    batch_records, target_language, llm, template\n","                )\n","\n","                # Store results\n","                for j, record in enumerate(batch_records):\n","                    record_id = record['id']\n","                    source_text = record['source']\n","                    source_locale = record['source_locale']\n","                    translated_text = translated_texts_batch[j] if j < len(translated_texts_batch) else \"ERROR: Translation missing\"\n","                    translated_results_for_file.append({\n","                        \"id\": record_id,\n","                        \"source_language\": get_language_name(source_locale),\n","                        \"target_language\": target_language,\n","                        \"text\": source_text,\n","                        \"prediction\": translated_text,\n","                    })\n","\n","            except Exception as e:\n","                print(f\"\\nCRITICAL ERROR: Failed to translate a batch starting with ID '{batch_records[0]['id']}' after multiple retries. Error: {e}\")\n","                for record in batch_records:\n","                    translated_results_for_file.append({\n","                        \"id\": record['id'],\n","                        \"source_language\": get_language_name(record['source_locale']),\n","                        \"target_language\": get_language_name(record['target_locale']),\n","                        \"text\": record['source'],\n","                        \"prediction\": \"ERROR: Batch translation failed due to API issues/rate limits.\",\n","                    })\n","\n","            # Respect API rate limits between batches/files\n","            if i + BATCH_SIZE < len(data_to_translate) or overall_pbar.n < len(jsonl_files):\n","                 time.sleep(DELAY_BETWEEN_REQUESTS_SECONDS)\n","\n","        # Save translated output to file\n","        with open(outfile_path, 'w', encoding='utf-8') as f:\n","            for res in translated_results_for_file:\n","                f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n","\n","        print(f\"\\nTranslations for {filename} saved to {outfile_path}\")\n","\n","    overall_pbar.close()\n","    print(\"All files processed and translations saved.\")\n"],"metadata":{"id":"os5Yc0HcSO8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download and load the COMET model for evaluation\n","comet_model = download_comet_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392,"referenced_widgets":["42c7fa63eb134de2bef881f79d5a5fbc","dd876aa1e3384c778e2da7f1f33093d8","301ecbf283bf472081bac8dfe0d723e1","b138be0d5a0d4a9aa97f5fc78fc327b7","c7cd98e5f72a4c0683dbdd5bdb87ca06","563fa65ae1304d5fa1308f6eaa27aee4","51a156f8e8b040cd966d04ccc407d569","cbcca26f64134c1198f988e6b760d358","ac4f0495e0ef47bf9d4d1ce1dab57be0","75603090807447f38c02a749c820d782","4d05454883f24b8e8363deea5f7c5466","e033168d88e64b54a636499bfc6426b4","ffd19a63ccbd4b28b941e180ca5154a7","139645b0aeb74c90b1f9582938287c45","a393eea97c3a4d6c9b7b28c7057a7ae9","e487d8c239d04706b9f746f014a5eaaa","86094d409aa54b9ea54cca9a13f5d19b","0e5964090d9e4ebab6ad62d500e97fe9","35febd1f8ec54683b48eb8bacff75209","51ef05663000482e9646c9d136fd34b4","d3e105b6fee0486690f390f853331b49","fa1c387fa0f54811b1506fd9bc779dad","4ef691287f4d4428905bd9bec8c7559b","02d782f9f9074a3291150114a1ecdfb8","27dcabeaa2634c1c916a0e47a8254fa3","3aab1f6b2eff4720b5c100f83346b6ee","8c4301709853407b869e74ec3378cc97","b80a084ee8ac42eb8e150744ce870e14","a71c1e42a5734318bd2fa87908a839e5","9718a84cb13f4f8881b7c7c41a6d4cb4","0f4afa38500846dcaab3e0596f1422a7","3bf01b8d639a405db41eb3923fbc3389","f6afaa04b10546b3a948f877d52b74cb","850d2a8086824293b4a5afbb88e07b67","8989cbbd31f04d48b9e4a82824dc7971","91b8ce3dfc464052b8ac9bde72785c60","59cc93dc24e84d229e3fb467683c0484","e65c7a8efc724c3f9f64dc9e2d469335","86b94d6b288547999c282726ff0519b5","00bb9f7c047641fb814bc4804b311425","f8c57aa942284ab9ba1240d1d2e04661","c1e9bd98d1884655a5002beef2e4eb85","ad9b44b4017a4548bc1abab025893c54","a01f5490f83f4f278f0c846b261b212a","b982d60659594861a8dd93f767fbdf34","7338401ad61e440391e8ff42fc42d89a","089624fbbeb945aaaddb8e61299c27e3","99e1b1941aa144fbb59a28c8839b1eac","9d1a1fd1617449b58c7e9cff4cfdd4a5","687fd2c3740f4caaba71733857366732","30e3a235ae544523a26513710e0bdd89","95d2a39c2ea44ee1955a82f43742b6a3","709d50da7872486fa7f5752247d5caeb","12f02e9901b745828fc5039289f4cbe3","f6efbafc043944169fc2f78601159167","3b371b333d67405a818c48c7fcf64936","61f0d1cfcb72447f851f547022752d12","a2c0b9041cdd4ca2bf3fc44d130ee2d4","2755aa88db5946eda325d6689fec948c","c0440ebf27154e2f9c26d8c2bf691410","5c903e186c244531b8d87504de64940f","5aa0112263b943f3a7bb88c1b3338062","c9e81b8ad15140d9b3b452455eb15418","f1ed87c5e77744cb9cf0a74f92572787","5ed3649093d24b2483f025bcadfee0ce","cb2a4f828e174a3ba55392bb3d716369","1645133610e545de9e32ade547dc9e39","a24896f38fc54f5ba8efb456ed77cd9e","11cdc70e156f41598fa781f0581f73c4","71f12034a9ae4b7fb1aafb2502b97d62","e2a813b38b4a49b3a3baed70046b7ab8","f889d962ffed4654b74f2f4aa2f985af","81191049abb345b08d31d5d41e53ee39","d786a22d7c8346808afb9ebb536fbd19","02d87b076cbf42c4abaf89f62887d39b","136f971e750f42aea3a61dd0acb1c8f5","7026403136fe456ba9a22fe71b55f8be","f0188008a558482dbd36bc3fcf561aa7","aa529fc334274bdbac7a7bf33d5ea72c","554b23dddc3344db9a159f1faf33c4fc","36dbaf40c18b4335a5c78b21e72c9351","8174c7472e6347959a7b4e370e321d35","bd77c527ad1c48f0a14266b60ae6b991","3b7fabdff17c4705a76e9ddd86753cf2","64ced371b57f4aa196c665b290a3aa08","6ec202c4e9f8470c8bd7090eba0cb35c","cdd528d4607c431a9c8d3569b43edf34","827501faa57d47dbbd9d54d474d48a95","f134a6c0f16f421f8f552d586e18a4b5","37f58ee262ba468da2d4f0e765de569f","1dfbda072e45400c9c3d88ff6175b82d","3aaefc643621468e817f3afe25432054","e1b03fca80ed4d42a25e23f350c2ee46","c4ba12a3453c4e0f87b297a2e2cf7a05","9241ba8d2f524d9db9d30c3b107fd4da","8df68971b3924396bd9de6921709d387","0d7c504bb55b4937a31a8ad010842b24","c75e11cf5b0547ee9d9ed833665d6014","0bb9f7e4d82a468a99fdcd39ed1b3df8","393be7e531b849568652f7d0c4811760","39715cf821e44a298b0ef75e37ca4cc5","992269407483458f8098ecf67015152f","68704abd976d450a82b994ecc3ec0f11","15411ce5518445bb97fbbd8a28cc7c34","13c36725b38243c5929248cd83d8a1cf","93c37798c4534e6aa2f7885980486bb4","dd741f582eee4c578d36ab84e8be19da","5a79fb2ddcf443e0b20982483153d1eb","4faffbbe4309416a90ff1afdbc3337de","75a647f651624cfba69585b62d851584"]},"id":"_2dRpnsVST1Q","executionInfo":{"status":"ok","timestamp":1750505673624,"user_tz":-330,"elapsed":58273,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"outputId":"077d725c-f69d-4aa4-845a-0bb7f530be78"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42c7fa63eb134de2bef881f79d5a5fbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e033168d88e64b54a636499bfc6426b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["hparams.yaml:   0%|          | 0.00/567 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef691287f4d4428905bd9bec8c7559b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["LICENSE:   0%|          | 0.00/9.69k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"850d2a8086824293b4a5afbb88e07b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/3.40k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b982d60659594861a8dd93f767fbdf34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["checkpoints/model.ckpt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b371b333d67405a818c48c7fcf64936"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1645133610e545de9e32ade547dc9e39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0188008a558482dbd36bc3fcf561aa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f134a6c0f16f421f8f552d586e18a4b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"393be7e531b849568652f7d0c4811760"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"]}]},{"cell_type":"code","source":["def calculate_scores(template_id):\n","    # Create directory to save evaluation scores\n","    scores_dir = os.path.join(output_prediction_dir, template_id, \"scores\")\n","    if not os.path.exists(scores_dir):\n","        os.makedirs(scores_dir, exist_ok=True)\n","\n","    for file_path in jsonl_files:\n","        references_path = file_path\n","        filename = os.path.basename(file_path)\n","        predictions_path = os.path.join(output_prediction_dir, template_id, filename)\n","\n","        # Compute COMET score\n","        comet_score = calculate_comet_scores(\n","            comet_model,\n","            references_path,\n","            predictions_path\n","        )\n","\n","        # Compute entity-level accuracy\n","        correct_instances, total_instances, meta_score = calculate_meta_score(\n","            references_path,\n","            predictions_path)\n","\n","        # Package results into JSON\n","        evaluation_results = {\n","            \"correct_instances\": correct_instances,\n","            \"total_instances\": total_instances,\n","            \"comet_score\": comet_score,\n","            \"meta_score\": meta_score\n","        }\n","\n","        # Save scores to file\n","        evaluation_output_path = os.path.join(scores_dir, f\"{os.path.splitext(filename)[0]}.json\")\n","        with open(evaluation_output_path, 'w', encoding='utf-8') as json_file:\n","            json.dump(evaluation_results, json_file, ensure_ascii=False, indent=4)\n"],"metadata":{"id":"tnq4TC8OSyTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build the zero-shot prompt using template string 1 (basic translation)\n","zero_shot_prompt_template = PromptTemplate(\n","    input_variables=[\"source_texts_json\", \"target_language\"],\n","    template=ZERO_SHOT_PROMPT_TEMPLATE_STRING_1,\n",")\n","\n","# Run evaluation pipeline: translate and save predictions\n","zero_shot_eval(zero_shot_prompt_template, \"zero-shot-1\")\n","\n","# Evaluate predictions using COMET and entity-level accuracy\n","calculate_scores(\"zero-shot-1\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TAT_1tIBS-Qk","outputId":"4f3764d1-0c7c-45f2-d151-34ce3df695a0","executionInfo":{"status":"ok","timestamp":1750343493341,"user_tz":-330,"elapsed":2811837,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Processing files:   0%|          | 0/10 [00:00<?, ?it/s]\n","Translating de_DE.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating de_DE.jsonl in batches:   7%|▋         | 1/15 [00:20<04:46, 20.48s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  13%|█▎        | 2/15 [00:49<05:31, 25.54s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  20%|██        | 3/15 [01:05<04:13, 21.13s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  27%|██▋       | 4/15 [01:22<03:33, 19.39s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  33%|███▎      | 5/15 [01:50<03:47, 22.71s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  40%|████      | 6/15 [02:09<03:10, 21.20s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  47%|████▋     | 7/15 [02:38<03:12, 24.02s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  53%|█████▎    | 8/15 [03:02<02:48, 24.01s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  60%|██████    | 9/15 [03:24<02:19, 23.29s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  67%|██████▋   | 10/15 [03:40<01:44, 20.98s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  73%|███████▎  | 11/15 [04:03<01:27, 21.79s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  80%|████████  | 12/15 [04:18<00:58, 19.50s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  87%|████████▋ | 13/15 [04:32<00:36, 18.03s/it]\u001b[A\n","Translating de_DE.jsonl in batches:  93%|█████████▎| 14/15 [05:16<00:25, 25.69s/it]\u001b[A\n","Translating de_DE.jsonl in batches: 100%|██████████| 15/15 [05:35<00:00, 23.66s/it]\u001b[A\n","Processing files:  10%|█         | 1/10 [05:35<50:21, 335.68s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for de_DE.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/de_DE.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating fr_FR.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating fr_FR.jsonl in batches:   7%|▋         | 1/15 [00:16<03:46, 16.20s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  13%|█▎        | 2/15 [00:31<03:20, 15.44s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  20%|██        | 3/15 [01:02<04:32, 22.74s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  27%|██▋       | 4/15 [01:29<04:30, 24.60s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  33%|███▎      | 5/15 [01:49<03:47, 22.71s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  40%|████      | 6/15 [02:06<03:06, 20.78s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  47%|████▋     | 7/15 [02:29<02:53, 21.68s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  53%|█████▎    | 8/15 [03:20<03:36, 30.98s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  60%|██████    | 9/15 [03:43<02:49, 28.32s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  67%|██████▋   | 10/15 [04:18<02:32, 30.56s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  73%|███████▎  | 11/15 [04:45<01:57, 29.45s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  80%|████████  | 12/15 [05:11<01:25, 28.36s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  87%|████████▋ | 13/15 [05:31<00:51, 25.84s/it]\u001b[A\n","Translating fr_FR.jsonl in batches:  93%|█████████▎| 14/15 [05:50<00:23, 23.66s/it]\u001b[A\n","Translating fr_FR.jsonl in batches: 100%|██████████| 15/15 [06:03<00:00, 20.61s/it]\u001b[A\n","Processing files:  20%|██        | 2/10 [11:39<46:59, 352.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for fr_FR.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/fr_FR.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating th_TH.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating th_TH.jsonl in batches:   7%|▋         | 1/15 [00:28<06:44, 28.88s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  13%|█▎        | 2/15 [01:17<08:49, 40.76s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  20%|██        | 3/15 [02:04<08:41, 43.49s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  27%|██▋       | 4/15 [02:42<07:35, 41.39s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  33%|███▎      | 5/15 [03:38<07:45, 46.55s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  40%|████      | 6/15 [04:19<06:41, 44.56s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  47%|████▋     | 7/15 [04:49<05:19, 39.93s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  53%|█████▎    | 8/15 [05:16<04:10, 35.81s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  60%|██████    | 9/15 [05:42<03:16, 32.75s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  67%|██████▋   | 10/15 [05:59<02:18, 27.73s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  73%|███████▎  | 11/15 [06:17<01:39, 24.91s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  80%|████████  | 12/15 [06:36<01:09, 23.21s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  87%|████████▋ | 13/15 [07:00<00:46, 23.28s/it]\u001b[A\n","Translating th_TH.jsonl in batches:  93%|█████████▎| 14/15 [07:18<00:21, 21.77s/it]\u001b[A\n","Translating th_TH.jsonl in batches: 100%|██████████| 15/15 [07:26<00:00, 17.65s/it]\u001b[A\n","Processing files:  30%|███       | 3/10 [19:07<46:10, 395.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for th_TH.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/th_TH.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating ko_KR.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating ko_KR.jsonl in batches:   7%|▋         | 1/15 [00:26<06:06, 26.18s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  13%|█▎        | 2/15 [00:43<04:35, 21.18s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  20%|██        | 3/15 [01:17<05:25, 27.09s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  27%|██▋       | 4/15 [01:43<04:51, 26.46s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  33%|███▎      | 5/15 [02:09<04:23, 26.34s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  40%|████      | 6/15 [02:36<03:58, 26.49s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  47%|████▋     | 7/15 [02:54<03:10, 23.83s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  53%|█████▎    | 8/15 [03:23<02:57, 25.41s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  60%|██████    | 9/15 [03:51<02:38, 26.34s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  67%|██████▋   | 10/15 [04:26<02:25, 29.01s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  73%|███████▎  | 11/15 [04:46<01:44, 26.19s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  80%|████████  | 12/15 [05:03<01:10, 23.46s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  87%|████████▋ | 13/15 [05:36<00:52, 26.16s/it]\u001b[A\n","Translating ko_KR.jsonl in batches:  93%|█████████▎| 14/15 [06:08<00:27, 27.96s/it]\u001b[A\n","Translating ko_KR.jsonl in batches: 100%|██████████| 15/15 [06:46<00:00, 31.00s/it]\u001b[A\n","Processing files:  40%|████      | 4/10 [25:54<40:00, 400.15s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for ko_KR.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/ko_KR.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating es_ES.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating es_ES.jsonl in batches:   7%|▋         | 1/15 [00:25<05:59, 25.64s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  13%|█▎        | 2/15 [00:51<05:38, 26.05s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  20%|██        | 3/15 [01:16<05:03, 25.27s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  27%|██▋       | 4/15 [01:48<05:07, 27.99s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  33%|███▎      | 5/15 [02:16<04:40, 28.09s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  40%|████      | 6/15 [02:33<03:38, 24.25s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  47%|████▋     | 7/15 [02:51<02:56, 22.06s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  53%|█████▎    | 8/15 [03:15<02:39, 22.79s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  60%|██████    | 9/15 [03:33<02:07, 21.17s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  67%|██████▋   | 10/15 [04:01<01:57, 23.40s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  73%|███████▎  | 11/15 [04:26<01:35, 23.82s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  80%|████████  | 12/15 [04:45<01:07, 22.37s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  87%|████████▋ | 13/15 [05:11<00:47, 23.63s/it]\u001b[A\n","Translating es_ES.jsonl in batches:  93%|█████████▎| 14/15 [05:35<00:23, 23.60s/it]\u001b[A\n","Translating es_ES.jsonl in batches: 100%|██████████| 15/15 [06:00<00:00, 24.21s/it]\u001b[A\n","Processing files:  50%|█████     | 5/10 [31:55<32:10, 386.18s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for es_ES.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/es_ES.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating it_IT.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating it_IT.jsonl in batches:   7%|▋         | 1/15 [00:29<06:54, 29.59s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  13%|█▎        | 2/15 [00:56<06:01, 27.83s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  20%|██        | 3/15 [01:23<05:30, 27.52s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  27%|██▋       | 4/15 [01:42<04:27, 24.29s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  33%|███▎      | 5/15 [01:59<03:35, 21.52s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  40%|████      | 6/15 [02:20<03:11, 21.31s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  47%|████▋     | 7/15 [02:44<02:58, 22.33s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  53%|█████▎    | 8/15 [03:02<02:26, 20.98s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  60%|██████    | 9/15 [03:21<02:02, 20.35s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  67%|██████▋   | 10/15 [03:48<01:51, 22.32s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  73%|███████▎  | 11/15 [04:15<01:34, 23.69s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  80%|████████  | 12/15 [04:36<01:08, 22.86s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  87%|████████▋ | 13/15 [04:53<00:42, 21.15s/it]\u001b[A\n","Translating it_IT.jsonl in batches:  93%|█████████▎| 14/15 [05:12<00:20, 20.66s/it]\u001b[A\n","Translating it_IT.jsonl in batches: 100%|██████████| 15/15 [05:27<00:00, 18.89s/it]\u001b[A\n","Processing files:  60%|██████    | 6/10 [37:23<24:25, 366.42s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for it_IT.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/it_IT.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating ja_JP.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating ja_JP.jsonl in batches:   7%|▋         | 1/15 [00:20<04:49, 20.71s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  13%|█▎        | 2/15 [00:54<06:09, 28.45s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  20%|██        | 3/15 [01:17<05:10, 25.91s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  27%|██▋       | 4/15 [01:46<04:57, 27.06s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  33%|███▎      | 5/15 [02:09<04:15, 25.59s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  40%|████      | 6/15 [02:36<03:55, 26.16s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  47%|████▋     | 7/15 [03:16<04:05, 30.70s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  53%|█████▎    | 8/15 [03:51<03:43, 31.92s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  60%|██████    | 9/15 [04:29<03:23, 33.90s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  67%|██████▋   | 10/15 [04:52<02:33, 30.61s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  73%|███████▎  | 11/15 [05:16<01:54, 28.68s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  80%|████████  | 12/15 [05:52<01:32, 30.70s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  87%|████████▋ | 13/15 [06:42<01:13, 36.75s/it]\u001b[A\n","Translating ja_JP.jsonl in batches:  93%|█████████▎| 14/15 [07:01<00:31, 31.37s/it]\u001b[A\n","Translating ja_JP.jsonl in batches: 100%|██████████| 15/15 [07:16<00:00, 26.20s/it]\u001b[A\n","Processing files:  70%|███████   | 7/10 [44:39<19:27, 389.32s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for ja_JP.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/ja_JP.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating ar_AE.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating ar_AE.jsonl in batches:   7%|▋         | 1/15 [00:27<06:26, 27.61s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  13%|█▎        | 2/15 [00:56<06:06, 28.18s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  20%|██        | 3/15 [01:17<04:57, 24.83s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  27%|██▋       | 4/15 [01:36<04:10, 22.77s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  33%|███▎      | 5/15 [02:02<03:59, 23.91s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  40%|████      | 6/15 [02:26<03:35, 23.93s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  47%|████▋     | 7/15 [03:04<03:47, 28.40s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  53%|█████▎    | 8/15 [03:56<04:12, 36.08s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  60%|██████    | 9/15 [04:22<03:17, 32.95s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  67%|██████▋   | 10/15 [04:47<02:32, 30.42s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  73%|███████▎  | 11/15 [05:33<02:20, 35.04s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  80%|████████  | 12/15 [06:02<01:40, 33.45s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  87%|████████▋ | 13/15 [07:40<01:45, 52.89s/it]\u001b[A\n","Translating ar_AE.jsonl in batches:  93%|█████████▎| 14/15 [08:00<00:42, 42.97s/it]\u001b[A\n","Translating ar_AE.jsonl in batches: 100%|██████████| 15/15 [08:18<00:00, 35.44s/it]\u001b[A\n","Processing files:  80%|████████  | 8/10 [52:58<14:08, 424.19s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for ar_AE.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/ar_AE.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating tr_TR.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating tr_TR.jsonl in batches:   7%|▋         | 1/15 [00:32<07:38, 32.72s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  13%|█▎        | 2/15 [00:57<06:08, 28.31s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  20%|██        | 3/15 [01:41<07:04, 35.36s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  27%|██▋       | 4/15 [02:06<05:41, 31.02s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  33%|███▎      | 5/15 [02:23<04:20, 26.00s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  40%|████      | 6/15 [02:51<04:02, 26.94s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  47%|████▋     | 7/15 [03:23<03:48, 28.51s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  53%|█████▎    | 8/15 [03:56<03:29, 29.99s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  60%|██████    | 9/15 [04:14<02:37, 26.29s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  67%|██████▋   | 10/15 [04:40<02:10, 26.19s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  73%|███████▎  | 11/15 [05:07<01:44, 26.16s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  80%|████████  | 12/15 [05:25<01:11, 23.82s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  87%|████████▋ | 13/15 [05:49<00:47, 23.91s/it]\u001b[A\n","Translating tr_TR.jsonl in batches:  93%|█████████▎| 14/15 [06:11<00:23, 23.26s/it]\u001b[A\n","Translating tr_TR.jsonl in batches: 100%|██████████| 15/15 [06:26<00:00, 20.69s/it]\u001b[A\n","Processing files:  90%|█████████ | 9/10 [59:25<06:52, 412.38s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for tr_TR.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/tr_TR.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Translating zh_TW.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n","Translating zh_TW.jsonl in batches:   7%|▋         | 1/15 [00:35<08:10, 35.02s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  13%|█▎        | 2/15 [00:52<05:17, 24.43s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  20%|██        | 3/15 [01:23<05:33, 27.77s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  27%|██▋       | 4/15 [02:31<08:00, 43.67s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  33%|███▎      | 5/15 [03:07<06:48, 40.87s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  40%|████      | 6/15 [03:34<05:23, 36.00s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  47%|████▋     | 7/15 [04:13<04:55, 36.92s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  53%|█████▎    | 8/15 [04:33<03:40, 31.56s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  60%|██████    | 9/15 [05:03<03:06, 31.15s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  67%|██████▋   | 10/15 [05:21<02:15, 27.14s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  73%|███████▎  | 11/15 [05:47<01:47, 26.79s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  80%|████████  | 12/15 [06:17<01:22, 27.60s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  87%|████████▋ | 13/15 [06:47<00:56, 28.42s/it]\u001b[A\n","Translating zh_TW.jsonl in batches:  93%|█████████▎| 14/15 [07:19<00:29, 29.67s/it]\u001b[A\n","Translating zh_TW.jsonl in batches: 100%|██████████| 15/15 [07:30<00:00, 24.04s/it]\u001b[A\n","Processing files: 100%|██████████| 10/10 [1:06:56<00:00, 401.65s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for zh_TW.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-1/zh_TW.jsonl\n","All files processed and translations saved.\n","Created 1260 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [14:37<00:00, 21.94s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 89.74\n","Created 1316 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 42/42 [16:24<00:00, 23.44s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 90.08\n","Created 1654 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 52/52 [20:05<00:00, 23.19s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 82.99\n","Created 1660 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 52/52 [18:44<00:00, 21.63s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 93.44\n","Created 1229 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 39/39 [14:23<00:00, 22.15s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 92.24\n","Created 1268 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [14:03<00:00, 21.08s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 91.40\n","Created 1409 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 45/45 [17:20<00:00, 23.11s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 93.64\n","Created 1177 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 37/37 [12:04<00:00, 19.59s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 90.86\n","Created 1260 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [13:04<00:00, 19.60s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 92.65\n","Created 1544 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 49/49 [15:53<00:00, 19.45s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 92.28\n"]}]},{"cell_type":"code","source":["# Build the zero-shot prompt using template string 2 (with entity preservation)\n","zero_shot_prompt_template = PromptTemplate(\n","    input_variables=[\"source_texts_json\", \"target_language\"],\n","    template=ZERO_SHOT_PROMPT_TEMPLATE_STRING_2,\n",")\n","\n","# Run second evaluation pipeline with entity-focused prompt\n","zero_shot_eval(zero_shot_prompt_template, \"zero-shot-2\")\n","\n","# Evaluate second run using COMET and entity accuracy\n","calculate_scores(\"zero-shot-2\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1iZ5wCiETToN","executionInfo":{"status":"ok","timestamp":1750002693472,"user_tz":-330,"elapsed":1932092,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"outputId":"ba6c675d-a7c4-454e-cc88-4115789bfea0"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","Processing files:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n","\n","Translating de_DE.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:   7%|▋         | 1/15 [00:17<04:04, 17.46s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  13%|█▎        | 2/15 [00:44<04:58, 22.98s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  20%|██        | 3/15 [01:11<05:00, 25.02s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  27%|██▋       | 4/15 [01:32<04:16, 23.33s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  33%|███▎      | 5/15 [01:53<03:45, 22.52s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  40%|████      | 6/15 [02:13<03:14, 21.58s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  47%|████▋     | 7/15 [02:32<02:47, 20.95s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  53%|█████▎    | 8/15 [02:53<02:25, 20.84s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  60%|██████    | 9/15 [03:13<02:02, 20.42s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  67%|██████▋   | 10/15 [03:33<01:42, 20.42s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  73%|███████▎  | 11/15 [03:56<01:24, 21.21s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  80%|████████  | 12/15 [04:17<01:03, 21.12s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  87%|████████▋ | 13/15 [04:38<00:42, 21.01s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches:  93%|█████████▎| 14/15 [04:58<00:20, 20.94s/it]\u001b[A\u001b[A\n","\n","Translating de_DE.jsonl in batches: 100%|██████████| 15/15 [05:15<00:00, 19.55s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  10%|█         | 1/10 [05:15<47:17, 315.33s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for de_DE.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/de_DE.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating fr_FR.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:   7%|▋         | 1/15 [00:20<04:48, 20.61s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  13%|█▎        | 2/15 [00:41<04:33, 21.00s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  20%|██        | 3/15 [01:03<04:17, 21.48s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  27%|██▋       | 4/15 [01:24<03:53, 21.21s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  33%|███▎      | 5/15 [01:44<03:26, 20.62s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  40%|████      | 6/15 [02:03<03:00, 20.04s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  47%|████▋     | 7/15 [02:28<02:55, 21.89s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  53%|█████▎    | 8/15 [02:53<02:38, 22.61s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  60%|██████    | 9/15 [03:13<02:11, 21.90s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  67%|██████▋   | 10/15 [03:33<01:46, 21.25s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  73%|███████▎  | 11/15 [03:54<01:24, 21.16s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  80%|████████  | 12/15 [04:18<01:06, 22.25s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  87%|████████▋ | 13/15 [04:38<00:43, 21.52s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches:  93%|█████████▎| 14/15 [04:58<00:20, 20.95s/it]\u001b[A\u001b[A\n","\n","Translating fr_FR.jsonl in batches: 100%|██████████| 15/15 [05:12<00:00, 18.96s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  20%|██        | 2/10 [10:28<41:50, 313.83s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for fr_FR.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/fr_FR.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating th_TH.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:   7%|▋         | 1/15 [00:27<06:27, 27.66s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  13%|█▎        | 2/15 [00:52<05:37, 25.96s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  20%|██        | 3/15 [01:18<05:11, 25.96s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  27%|██▋       | 4/15 [01:44<04:47, 26.18s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  33%|███▎      | 5/15 [02:12<04:27, 26.70s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  40%|████      | 6/15 [02:36<03:50, 25.60s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  47%|████▋     | 7/15 [03:01<03:25, 25.63s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  53%|█████▎    | 8/15 [03:36<03:18, 28.42s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  60%|██████    | 9/15 [04:02<02:45, 27.65s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  67%|██████▋   | 10/15 [04:41<02:36, 31.36s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  73%|███████▎  | 11/15 [05:05<01:56, 29.03s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  80%|████████  | 12/15 [05:35<01:27, 29.25s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  87%|████████▋ | 13/15 [06:00<00:56, 28.17s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches:  93%|█████████▎| 14/15 [06:24<00:26, 26.84s/it]\u001b[A\u001b[A\n","\n","Translating th_TH.jsonl in batches: 100%|██████████| 15/15 [06:34<00:00, 21.62s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  30%|███       | 3/10 [17:02<40:53, 350.54s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for th_TH.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/th_TH.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating ko_KR.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:   7%|▋         | 1/15 [00:30<07:04, 30.29s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  13%|█▎        | 2/15 [00:56<06:00, 27.74s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  20%|██        | 3/15 [01:22<05:25, 27.10s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  27%|██▋       | 4/15 [01:45<04:40, 25.54s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  33%|███▎      | 5/15 [02:08<04:04, 24.48s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  40%|████      | 6/15 [02:29<03:30, 23.37s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  47%|████▋     | 7/15 [02:54<03:11, 23.89s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  53%|█████▎    | 8/15 [03:19<02:49, 24.25s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  60%|██████    | 9/15 [03:41<02:20, 23.45s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  67%|██████▋   | 10/15 [04:03<01:54, 22.94s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  73%|███████▎  | 11/15 [04:28<01:34, 23.64s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  80%|████████  | 12/15 [04:47<01:06, 22.28s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  87%|████████▋ | 13/15 [05:08<00:44, 22.05s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches:  93%|█████████▎| 14/15 [05:30<00:21, 22.00s/it]\u001b[A\u001b[A\n","\n","Translating ko_KR.jsonl in batches: 100%|██████████| 15/15 [05:50<00:00, 21.21s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  40%|████      | 4/10 [22:52<35:02, 350.43s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for ko_KR.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/ko_KR.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating es_ES.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:   7%|▋         | 1/15 [00:19<04:31, 19.37s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  13%|█▎        | 2/15 [00:39<04:18, 19.87s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  20%|██        | 3/15 [01:02<04:15, 21.31s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  27%|██▋       | 4/15 [01:19<03:36, 19.66s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  33%|███▎      | 5/15 [02:07<04:57, 29.73s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  40%|████      | 6/15 [02:27<03:57, 26.43s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  47%|████▋     | 7/15 [02:47<03:15, 24.45s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  53%|█████▎    | 8/15 [03:14<02:56, 25.15s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  60%|██████    | 9/15 [03:34<02:21, 23.57s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  67%|██████▋   | 10/15 [03:56<01:54, 23.00s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  73%|███████▎  | 11/15 [04:16<01:28, 22.15s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  80%|████████  | 12/15 [04:36<01:04, 21.40s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  87%|████████▋ | 13/15 [04:58<00:43, 21.78s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches:  93%|█████████▎| 14/15 [05:19<00:21, 21.46s/it]\u001b[A\u001b[A\n","\n","Translating es_ES.jsonl in batches: 100%|██████████| 15/15 [05:36<00:00, 20.03s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  50%|█████     | 5/10 [28:28<28:46, 345.32s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for es_ES.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/es_ES.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating it_IT.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:   7%|▋         | 1/15 [00:19<04:30, 19.35s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  13%|█▎        | 2/15 [00:38<04:08, 19.12s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  20%|██        | 3/15 [01:00<04:08, 20.74s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  27%|██▋       | 4/15 [01:20<03:42, 20.24s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  33%|███▎      | 5/15 [01:39<03:16, 19.66s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  40%|████      | 6/15 [01:59<02:58, 19.87s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  47%|████▋     | 7/15 [02:21<02:45, 20.73s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  53%|█████▎    | 8/15 [02:45<02:32, 21.78s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  60%|██████    | 9/15 [03:07<02:10, 21.75s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  67%|██████▋   | 10/15 [03:31<01:51, 22.37s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  73%|███████▎  | 11/15 [03:49<01:24, 21.04s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  80%|████████  | 12/15 [04:10<01:03, 21.01s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  87%|████████▋ | 13/15 [04:28<00:40, 20.27s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches:  93%|█████████▎| 14/15 [04:46<00:19, 19.58s/it]\u001b[A\u001b[A\n","\n","Translating it_IT.jsonl in batches: 100%|██████████| 15/15 [04:59<00:00, 17.60s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  60%|██████    | 6/10 [33:28<21:59, 329.87s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for it_IT.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/it_IT.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating ja_JP.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:   7%|▋         | 1/15 [00:26<06:10, 26.49s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  13%|█▎        | 2/15 [00:49<05:18, 24.49s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  20%|██        | 3/15 [01:09<04:31, 22.60s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  27%|██▋       | 4/15 [01:32<04:07, 22.50s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  33%|███▎      | 5/15 [01:54<03:44, 22.42s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  40%|████      | 6/15 [02:12<03:09, 21.05s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  47%|████▋     | 7/15 [02:33<02:46, 20.80s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  53%|█████▎    | 8/15 [02:56<02:31, 21.62s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  60%|██████    | 9/15 [03:22<02:18, 23.07s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  67%|██████▋   | 10/15 [03:47<01:57, 23.58s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  73%|███████▎  | 11/15 [04:09<01:31, 22.94s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  80%|████████  | 12/15 [04:32<01:09, 23.05s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  87%|████████▋ | 13/15 [04:52<00:44, 22.30s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches:  93%|█████████▎| 14/15 [05:15<00:22, 22.26s/it]\u001b[A\u001b[A\n","\n","Translating ja_JP.jsonl in batches: 100%|██████████| 15/15 [05:27<00:00, 19.26s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  70%|███████   | 7/10 [38:56<16:27, 329.08s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for ja_JP.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/ja_JP.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating ar_AE.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:   7%|▋         | 1/15 [00:22<05:14, 22.49s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  13%|█▎        | 2/15 [00:40<04:21, 20.14s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  20%|██        | 3/15 [01:01<04:03, 20.30s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  27%|██▋       | 4/15 [01:20<03:37, 19.74s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  33%|███▎      | 5/15 [01:40<03:17, 19.72s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  40%|████      | 6/15 [01:59<02:57, 19.74s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  47%|████▋     | 7/15 [02:18<02:34, 19.27s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  53%|█████▎    | 8/15 [02:36<02:13, 19.05s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  60%|██████    | 9/15 [03:01<02:04, 20.76s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  67%|██████▋   | 10/15 [03:20<01:41, 20.31s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  73%|███████▎  | 11/15 [03:43<01:24, 21.07s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  80%|████████  | 12/15 [04:03<01:02, 20.93s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  87%|████████▋ | 13/15 [04:28<00:43, 22.00s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches:  93%|█████████▎| 14/15 [04:47<00:21, 21.26s/it]\u001b[A\u001b[A\n","\n","Translating ar_AE.jsonl in batches: 100%|██████████| 15/15 [04:59<00:00, 18.35s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  80%|████████  | 8/10 [43:55<10:39, 319.70s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for ar_AE.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/ar_AE.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating tr_TR.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:   7%|▋         | 1/15 [00:20<04:46, 20.48s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  13%|█▎        | 2/15 [00:39<04:15, 19.67s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  20%|██        | 3/15 [01:01<04:06, 20.57s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  27%|██▋       | 4/15 [01:22<03:48, 20.77s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  33%|███▎      | 5/15 [01:45<03:37, 21.79s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  40%|████      | 6/15 [02:16<03:43, 24.86s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  47%|████▋     | 7/15 [02:40<03:17, 24.63s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  53%|█████▎    | 8/15 [03:27<03:42, 31.77s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  60%|██████    | 9/15 [03:56<03:04, 30.83s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  67%|██████▋   | 10/15 [04:36<02:48, 33.64s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  73%|███████▎  | 11/15 [05:01<02:04, 31.09s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  80%|████████  | 12/15 [05:19<01:21, 27.07s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  87%|████████▋ | 13/15 [05:51<00:57, 28.61s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches:  93%|█████████▎| 14/15 [06:10<00:25, 25.56s/it]\u001b[A\u001b[A\n","\n","Translating tr_TR.jsonl in batches: 100%|██████████| 15/15 [06:23<00:00, 21.82s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files:  90%|█████████ | 9/10 [50:19<05:39, 339.70s/it]\u001b[A"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for tr_TR.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/tr_TR.jsonl\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\n","\n","Translating zh_TW.jsonl in batches:   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:   7%|▋         | 1/15 [00:19<04:32, 19.48s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  13%|█▎        | 2/15 [00:44<04:55, 22.71s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  20%|██        | 3/15 [01:05<04:21, 21.76s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  27%|██▋       | 4/15 [01:27<04:00, 21.91s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  33%|███▎      | 5/15 [01:48<03:37, 21.70s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  40%|████      | 6/15 [02:11<03:17, 21.97s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  47%|████▋     | 7/15 [02:43<03:23, 25.43s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  53%|█████▎    | 8/15 [03:05<02:50, 24.39s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  60%|██████    | 9/15 [03:25<02:17, 22.96s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  67%|██████▋   | 10/15 [03:46<01:50, 22.19s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  73%|███████▎  | 11/15 [04:10<01:31, 22.80s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  80%|████████  | 12/15 [04:33<01:08, 22.95s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  87%|████████▋ | 13/15 [04:53<00:44, 22.14s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches:  93%|█████████▎| 14/15 [05:15<00:21, 21.96s/it]\u001b[A\u001b[A\n","\n","Translating zh_TW.jsonl in batches: 100%|██████████| 15/15 [05:27<00:00, 18.88s/it]\u001b[A\u001b[A\n","\n","                                                                                   \u001b[A\u001b[A\n","Processing files: 100%|██████████| 10/10 [55:46<00:00, 334.66s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Translations for zh_TW.jsonl saved to /content/drive/MyDrive/Colab Notebooks/data/predictions/gpt-4o/validation/zero-shot-2/zh_TW.jsonl\n","All files processed and translations saved.\n","Created 1260 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [15:00<00:00, 22.50s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 89.71\n","Created 1316 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 42/42 [17:06<00:00, 24.45s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 89.62\n","Created 1654 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 52/52 [20:50<00:00, 24.04s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 81.81\n","Created 1660 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 52/52 [19:19<00:00, 22.30s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 92.77\n","Created 1229 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 39/39 [14:36<00:00, 22.47s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 92.06\n","Created 1268 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [14:29<00:00, 21.73s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 91.06\n","Created 1409 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 45/45 [17:55<00:00, 23.90s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 93.23\n","Created 1177 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 37/37 [12:30<00:00, 20.29s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 88.55\n","Created 1260 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [13:36<00:00, 20.42s/it]\n","INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 92.57\n","Created 1544 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 49/49 [16:34<00:00, 20.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 92.11\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u5seFaw5dtof","executionInfo":{"status":"ok","timestamp":1749989471192,"user_tz":-330,"elapsed":23611,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"outputId":"df127a2e-6d4b-4f8b-83d0-eb299055e181"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LMXo6CqtfyMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a057a023"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b89c458"},"outputs":[],"source":["# Few-shot prompt template for translation with example guidance\n","FEW_SHOT_PROMPT_TEMPLATE_STRING = \"\"\"\n","Your task is to translate English sentences into {target_language}.\n","Below are a few examples of English sentences with named entities translated correctly into {target_language}.\n","Pay close attention to how named entities (e.g., people, organizations, locations, product names) are translated correctly and consistently.\n","\n","Examples:\n","{few_shot_examples_json}\n","\n","Now, translate the following new English sentences into {target_language}.\n","Provide the translations as a JSON array of objects, where each object contains the original \"id\" and its \"translation\" (ensure the key name is \"translation\").\n","Maintain the original order of sentences from the input. Please return the json array without any additional text or formatting.\n","\n","Input JSON:\n","{source_texts_json}\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"532df5a4"},"outputs":[],"source":["few_shot_examples_dict = {\n","    'ar': [\n","        {'source': 'What is the seventh tallest mountain in North America?', 'target': 'ما سابع أعلى جبل في أمريكا الشمالية؟'},\n","        {'source': 'Which actor was the star of Titanic and was born in Los Angeles, California?', 'target': 'مَنْ الممثل الذي لعب دور البطولة في فيلم \"تيتانيك\" وهو من مواليد لوس أنجلوس بكاليفورنيا؟'},\n","        {'source': 'What year was the first book of the A Song of Ice and Fire series published?', 'target': 'في أي عام تم نشر أول كتاب من سلسلة \"أغنية الجليد والنار\"؟'},\n","        {'source': 'Who is the youngest current US governor?', 'target': 'مَن أصغر حاكم ولاية أمريكي حالٍ؟'},\n","        {'source': 'How long did it take to build the Lincoln Memorial?', 'target': 'كم من الوقت استغرق بناء نصب لينكولن التذكاري؟'},\n","        {'source': 'Has Bernie Sanders ever been president of the United States?', 'target': 'هل كان بيرني ساندرز رئيسًا للولايات المتحدة يومًا ما؟'},\n","        {'source': 'Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?', 'target': 'مَن الممثل الذي وقع عليه الاختيار الأول للكاتبة ستيفاني ماير للعب دور \"إدوارد كولن\" في فيلم \"الشفق\"؟'},\n","        {'source': 'Which river is longer than the Mississippi River?', 'target': 'أي نهر أطول من نهر المسيسيبي؟'},\n","        {'source': 'What is the latest US state to be admitted to the union that is not Hawaii?', 'target': 'ما آخر ولاية أمريكية تم قبولها في الاتحاد، بخلاف هاواي؟'},\n","        {'source': 'What is the longest lake in the world?', 'target': 'ما أطول بحيرة في العالم؟'},\n","    ],\n","    'de': [\n","        {'source': 'What is the seventh tallest mountain in North America?', 'target': 'Wie heißt der siebthöchste Berg Nordamerikas?'},\n","        {'source': 'What year was the first book of the A Song of Ice and Fire series published?', 'target': 'In welchem Jahr wurde das erste Buch der Reihe \"Das Lied von Eis und Feuer\" veröffentlicht?'},\n","        {'source': 'Who is the youngest current US governor?', 'target': 'Wer ist derzeit der jüngste amerikanische Gouverneur?'},\n","        {'source': 'Has Bernie Sanders ever been president of the United States?', 'target': 'War Bernie Sanders jemals Präsident der Vereinigten Staaten?'},\n","        {'source': 'Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?', 'target': 'Welcher Schauspieler war Stephanie Meyers erste Wahl für die Rolle des Edward Cullen in dem Film Twilight – Biss zum Morgengrauen?'},\n","        {'source': 'Which river is longer than the Mississippi River?', 'target': 'Welcher Fluss ist länger als der Mississippi'},\n","        {'source': 'What is the longest lake in the world?', 'target': 'Welcher See ist der längste der Welt?'},\n","        {'source': 'Is Texas the largest state in US?', 'target': 'Ist Texas der größte Bundesstaat in den Vereinigten Staaten?'},\n","        {'source': 'Who was the president of Argentina from 1989 to 1999?', 'target': 'Wer war von 1989 - 1999 Präsident von Argentinien?'},\n","        {'source': 'Who was a member of the 2004 U.S. Olympic mens swim team and born in Baltimore, Maryland?', 'target': 'Wer gehörte 2004 zur olympischen, Schwimmstaffel der Vereinigten Staaten und wurde in Baltimore, Maryland geboren?'},\n","    ],\n","    'es': [\n","        {'source': 'Which actor was the star of Titanic and was born in Los Angeles, California?', 'target': '¿Qué actor protagonizó Titanic y nació en Los Ángeles, California?'},\n","        {'source': 'What year was the first book of the A Song of Ice and Fire series published?', 'target': '¿En qué año se publicó el primer libro de la saga Canción de hielo y fuego?'},\n","        {'source': 'Which US president has had the most votes?', 'target': '¿Qué presidente de Estados Unidos obtuvo más votos?'},\n","        {'source': 'How long did it take to build the Lincoln Memorial?', 'target': '¿En cuánto tiempo se construyó el Monumento a Lincoln?'},\n","        {'source': 'Has Bernie Sanders ever been president of the United States?', 'target': '¿Bernie Sanders ha sido alguna vez presidente de los Estados Unidos?'},\n","        {'source': 'Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?', 'target': '¿Qué actor fue la primera opción de Stephanie Meyer para interpretar a Edward Cullen en la película Crepúsculo?'},\n","        {'source': 'What is the latest US state to be admitted to the union that is not Hawaii?', 'target': '¿Cuál es el último estado de Estados Unidos en ser incorporado a la unión aparte de Hawái?'},\n","        {'source': 'What is the longest lake in the world?', 'target': '¿Cuál es el lago más largo del mundo?'},\n","        {'source': 'Is Texas the largest state in US?', 'target': '¿Tejas es el estado más grande de Estados Unidos?'},\n","        {'source': 'How many times have the Los Angeles Dodgers lost the World Series?', 'target': '¿Cuántas veces perdieron los Dodgers de Los Ángeles la Serie Mundial?'},\n","    ],\n","    'fr': [\n","        {'source': 'What is the seventh tallest mountain in North America?', 'target': 'Quelle est la septième plus haute montagne d’Amérique du Nord ?'},\n","        {'source': 'Who is the youngest current US governor?', 'target': 'Qui est l’actuel plus jeune gouverneur américain ?'},\n","        {'source': 'Has Bernie Sanders ever been president of the United States?', 'target': 'Bernie Sanders a-t-il déjà été Président des États-Unis ?'},\n","        {'source': 'Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?', 'target': 'Quel acteur Stephanie Meyer a-t-elle choisi en premier pour jouer le rôle d’Edward Cullen dans le film Twilight ?'},\n","        {'source': 'Which river is longer than the Mississippi River?', 'target': 'Quel fleuve est plus long que le Mississippi ?'},\n","        {'source': 'What is the latest US state to be admitted to the union that is not Hawaii?', 'target': 'Quel est le dernier État Américain à avoir été admis dans l’Union et qui n’est pas Hawaï ?'},\n","        {'source': 'What is the longest lake in the world?', 'target': 'Quel est le lac le plus long du monde ?'},\n","        {'source': 'How many times have the Los Angeles Dodgers lost the World Series?', 'target': 'Combien de fois les Dodgers de Los Angeles ont-ils perdu dans la série mondiale ?'},\n","        {'source': 'Who was the president of Argentina from 1989 to 1999?', 'target': 'Qui était le Président de l’Argentine, de mille neuf cent quatre-vingt-neuf à mille neuf cent quatre-vingt-dix-neuf ?'},\n","        {'source': 'Who was a member of the 2004 U.S. Olympic mens swim team and born in Baltimore, Maryland?', 'target': 'Quel membre de l’équipe olympique masculine de natation des États-Unis de 2004 est né à Baltimore, dans le Maryland ?'},\n","    ],\n","    'it': [\n","        {'source': 'What year was the first book of the A Song of Ice and Fire series published?', 'target': 'In che anno venne pubblicato il primo libro della serie Cronache del Ghiaccio e del Fuoco?'},\n","        {'source': 'Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?', 'target': 'Quale attore era stata la prima scelta di Stephanie Meyer per interpretare Edward Cullen nel film Twilight?'},\n","        {'source': 'Which river is longer than the Mississippi River?', 'target': 'Quale fiume è più lungo del Mississippi?'},\n","        {'source': 'What is the longest lake in the world?', 'target': 'Qual è il lago più lungo del mondo?'},\n","        {'source': 'How many times have the Los Angeles Dodgers lost the World Series?', 'target': 'Quante volte i Los Angeles Dodgers hanno perso la Worls Series?'},\n","        {'source': 'Who was a member of the 2004 U.S. Olympic mens swim team and born in Baltimore, Maryland?', 'target': 'Chi era un membro della squadra olimpica di nuoto maschile americana del 2004 ed è nato a Baltimora, Maryland?'},\n","        {'source': 'Has there ever been a Christian U.S. senator?', 'target': 'Cè mai stato un senatore americano cristiano?'},\n","        {'source': 'How many Pulitzer Prizes has Colson Whitehead won?', 'target': 'Quanti premi Pulitzer ha vinto Colson Whitehead?'},\n","        {'source': 'Which movie was released first, Avengers: Endgame or Joker?', 'target': 'Quale film è uscito prima, Avengers Endgame o Joker?'},\n","        {'source': 'Was Friday the 13th released the same year as Halloween?', 'target': 'Venerdì 13 è uscito lo stesso anno di Halloween?'},\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73cefe49"},"outputs":[],"source":["from tenacity import retry, stop_after_attempt, wait_random_exponential, retry_if_exception_type\n","\n","@retry(\n","    wait=wait_random_exponential(multiplier=1, min=DELAY_BETWEEN_REQUESTS_SECONDS, max=60),\n","    stop=stop_after_attempt(5),\n","    retry=retry_if_exception_type(Exception)\n",")\n","def get_translated_content_with_retries_few_shot(source_records_batch, target_language, llm_instance, template, few_shot_examples):\n","    \"\"\"\n","    Translates a batch of source_texts using the provided LLM instance and few-shot examples.\n","    Includes robust retry logic with exponential backoff for API stability.\n","    \"\"\"\n","    global api_request_count\n","\n","    # Prepare the input and examples JSON strings for templating\n","    input_json_for_prompt = [{\"id\": rec['id'], \"text\": rec['source']} for rec in source_records_batch]\n","    source_texts_json_str = json.dumps(input_json_for_prompt, ensure_ascii=False)\n","    few_shot_examples_json = json.dumps(few_shot_examples, ensure_ascii=False)\n","\n","    # Format the few-shot prompt\n","    prompt_to_send = template.format(\n","        source_texts_json=source_texts_json_str,\n","        target_language=target_language,\n","        few_shot_examples_json=few_shot_examples_json\n","    )\n","\n","    print(f\"Sending prompt to model: {prompt_to_send}\")  # Debug: Log full prompt\n","\n","    # Invoke LLM with the prompt\n","    response = llm_instance.invoke(prompt_to_send)\n","    api_request_count += 1\n","\n","    try:\n","        # Clean potential markdown wrapping\n","        cleaned_content = response.content.strip()\n","        if cleaned_content.startswith(\"```json\") and cleaned_content.endswith(\"```\"):\n","            cleaned_content = cleaned_content[7:-3].strip()\n","\n","        # Parse model output\n","        translated_outputs = json.loads(cleaned_content)\n","\n","        print(f\"Model response: {translated_outputs}\")  # Debug: Log parsed output\n","\n","        # Validate structure\n","        if not isinstance(translated_outputs, list) or \\\n","           not all(isinstance(item, dict) and 'id' in item and 'translation' in item for item in translated_outputs):\n","            raise ValueError(\"Model did not return a valid JSON array of translation objects.\")\n","\n","        # Reconstruct ordered translations\n","        translated_dict = {item['id']: item['translation'] for item in translated_outputs}\n","        ordered_translations = [translated_dict.get(record['id'], \"ERROR: ID not found in JSON output\")\n","                                for record in source_records_batch]\n","\n","        return ordered_translations\n","\n","    # Handle errors\n","    except json.JSONDecodeError as e:\n","        print(f\"JSON Decode Error: {e}. Raw response: {response.content.strip()}\")\n","        return [\"ERROR: Invalid JSON response from model\"] * len(source_records_batch)\n","    except ValueError as e:\n","        print(f\"Value Error (JSON format issue): {e}. Raw response: {response.content.strip()}\")\n","        return [\"ERROR: Invalid JSON structure from model\"] * len(source_records_batch)\n","    except Exception as e:\n","        print(f\"An unexpected error occurred while processing model output: {e}. Raw response: {response.content.strip()}\")\n","        return [\"ERROR: Unexpected issue processing model output\"] * len(source_records_batch)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3e085ea8"},"outputs":[],"source":["def few_shot_eval(template, template_id, few_shot_examples):\n","    overall_pbar = tqdm.tqdm(jsonl_files, desc=\"Processing files (few-shot)\")\n","\n","    # Create directory for saving few-shot predictions\n","    output_prediction_dir_1 = os.path.join(output_prediction_dir, template_id)\n","    os.makedirs(output_prediction_dir_1, exist_ok=True)\n","\n","    for file_path in overall_pbar:\n","        filename = os.path.basename(file_path)\n","        outfile_path = os.path.join(output_prediction_dir_1, filename)\n","\n","        # Load JSONL input file\n","        data_to_translate = []\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                data_to_translate.append(json.loads(line))\n","\n","        translated_results_for_file = []\n","\n","        # Translate in batches\n","        for i in tqdm.tqdm(range(0, len(data_to_translate), BATCH_SIZE), desc=f\"Translating {filename} in batches (few-shot)\", leave=False):\n","            batch_records = data_to_translate[i : i + BATCH_SIZE]\n","            target_locale = batch_records[0]['target_locale']\n","            target_language = get_language_name(target_locale)\n","\n","            try:\n","                # Translate batch using few-shot prompt\n","                translated_texts_batch = get_translated_content_with_retries_few_shot(\n","                    batch_records, target_language, llm, template, few_shot_examples[target_locale]\n","                )\n","\n","                # Store translations with metadata\n","                for j, record in enumerate(batch_records):\n","                    record_id = record['id']\n","                    source_text = record['source']\n","                    source_locale = record['source_locale']\n","                    translated_text = translated_texts_batch[j] if j < len(translated_texts_batch) else \"ERROR: Translation missing\"\n","                    translated_results_for_file.append({\n","                        \"id\": record_id,\n","                        \"source_language\": get_language_name(source_locale),\n","                        \"target_language\": target_language,\n","                        \"text\": source_text,\n","                        \"prediction\": translated_text,\n","                    })\n","\n","            except Exception as e:\n","                print(f\"\\nCRITICAL ERROR: Failed to translate a batch starting with ID '{batch_records[0]['id']}' after multiple retries. Error: {e}\")\n","                for record in batch_records:\n","                    translated_results_for_file.append({\n","                        \"id\": record['id'],\n","                        \"source_language\": get_language_name(record['source_locale']),\n","                        \"target_language\": get_language_name(record['target_locale']),\n","                        \"text\": record['source'],\n","                        \"prediction\": \"ERROR: Batch translation failed due to API issues/rate limits.\",\n","                    })\n","\n","            # Respect rate limits between batches/files\n","            if i + BATCH_SIZE < len(data_to_translate) or overall_pbar.n < len(jsonl_files):\n","                 time.sleep(DELAY_BETWEEN_REQUESTS_SECONDS)\n","\n","        # Save results to file\n","        with open(outfile_path, 'w', encoding='utf-8') as f:\n","            for res in translated_results_for_file:\n","                f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n","\n","        print(f\"\\nTranslations for {filename} saved to {outfile_path}\")\n","\n","    overall_pbar.close()\n","    print(\"All files processed and translations saved (few-shot).\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"e9457963","executionInfo":{"status":"error","timestamp":1750505754330,"user_tz":-330,"elapsed":80551,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7d64da2b-310f-44d2-d2dc-307c6b0575a0"},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing files (few-shot):   0%|          | 0/10 [00:00<?, ?it/s]\n","Translating de_DE.jsonl in batches (few-shot):   0%|          | 0/15 [00:00<?, ?it/s]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Sending prompt to model: \n","Your task is to translate English sentences into German.\n","Below are a few examples of English sentences with named entities translated correctly into German.\n","Pay close attention to how named entities (e.g., people, organizations, locations, product names) are translated correctly and consistently.\n","\n","Examples:\n","[{\"source\": \"What is the seventh tallest mountain in North America?\", \"target\": \"Wie heißt der siebthöchste Berg Nordamerikas?\"}, {\"source\": \"What year was the first book of the A Song of Ice and Fire series published?\", \"target\": \"In welchem Jahr wurde das erste Buch der Reihe \\\"Das Lied von Eis und Feuer\\\" veröffentlicht?\"}, {\"source\": \"Who is the youngest current US governor?\", \"target\": \"Wer ist derzeit der jüngste amerikanische Gouverneur?\"}, {\"source\": \"Has Bernie Sanders ever been president of the United States?\", \"target\": \"War Bernie Sanders jemals Präsident der Vereinigten Staaten?\"}, {\"source\": \"Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?\", \"target\": \"Welcher Schauspieler war Stephanie Meyers erste Wahl für die Rolle des Edward Cullen in dem Film Twilight – Biss zum Morgengrauen?\"}, {\"source\": \"Which river is longer than the Mississippi River?\", \"target\": \"Welcher Fluss ist länger als der Mississippi\"}, {\"source\": \"What is the longest lake in the world?\", \"target\": \"Welcher See ist der längste der Welt?\"}, {\"source\": \"Is Texas the largest state in US?\", \"target\": \"Ist Texas der größte Bundesstaat in den Vereinigten Staaten?\"}, {\"source\": \"Who was the president of Argentina from 1989 to 1999?\", \"target\": \"Wer war von 1989 - 1999 Präsident von Argentinien?\"}, {\"source\": \"Who was a member of the 2004 U.S. Olympic mens swim team and born in Baltimore, Maryland?\", \"target\": \"Wer gehörte 2004 zur olympischen, Schwimmstaffel der Vereinigten Staaten und wurde in Baltimore, Maryland geboren?\"}]\n","\n","Now, translate the following new English sentences into German.\n","Provide the translations as a JSON array of objects, where each object contains the original \"id\" and its \"translation\" (ensure the key name is \"translation\").\n","Maintain the original order of sentences from the input. Please return the json array without any additional text or formatting.\n","\n","Input JSON:\n","[{\"id\": \"Q100268160_0\", \"text\": \"Who played the lead role in The Mole – Undercover in North Korea?\"}, {\"id\": \"Q100268160_1\", \"text\": \"When was The Mole – Undercover in North Korea released?\"}, {\"id\": \"Q100268160_2\", \"text\": \"What is the subject of the TV series The Mole – Undercover in North Korea?\"}, {\"id\": \"Q1012958_0\", \"text\": \"How can visitors reach Liebenzell Castle?\"}, {\"id\": \"Q1012958_1\", \"text\": \"Where is Liebenzell Castle located?\"}, {\"id\": \"Q1012958_2\", \"text\": \"When was Liebenzell Castle built?\"}, {\"id\": \"Q1056568_0\", \"text\": \"Who starred in the 1985 film Warning Sign?\"}, {\"id\": \"Q1056568_1\", \"text\": \"What is the subject of the movie Warning Sign?\"}, {\"id\": \"Q1056568_2\", \"text\": \"When was the movie Warning Sign released?\"}, {\"id\": \"Q1062531_0\", \"text\": \"What are some ways to season a boiled egg?\"}, {\"id\": \"Q1062531_1\", \"text\": \"How do you peel a boiled egg?\"}, {\"id\": \"Q1062531_2\", \"text\": \"Can you eat a boiled egg cold?\"}, {\"id\": \"Q1072093_0\", \"text\": \"What are some of the main ingredients in United States military chocolate?\"}, {\"id\": \"Q1072093_1\", \"text\": \"How is United States military chocolate different from regular chocolate?\"}, {\"id\": \"Q1072093_2\", \"text\": \"How long has United States military chocolate been included in rations?\"}, {\"id\": \"Q10860912_0\", \"text\": \"What is the name of the talking dog that is part of the Griffin family?\"}, {\"id\": \"Q10860912_1\", \"text\": \"What is the animated series that features the fictional Griffin family?\"}, {\"id\": \"Q10860912_2\", \"text\": \"Which animated series is known for its satirical portrayal of the Griffin family's life?\"}, {\"id\": \"Q1087216_0\", \"text\": \"What is the musical genre of Christ on the Mount of Olives?\"}, {\"id\": \"Q1087216_1\", \"text\": \"Who composed the oratorio Christ on the Mount of Olives?\"}, {\"id\": \"Q1087216_2\", \"text\": \"Who wrote the libretto for Beethoven's Christ on the Mount of Olives?\"}, {\"id\": \"Q109658712_0\", \"text\": \"What genre does Anxious People belong to?\"}, {\"id\": \"Q109658712_1\", \"text\": \"When was Anxious People released?\"}, {\"id\": \"Q109658712_2\", \"text\": \"Which country is Anxious People from?\"}, {\"id\": \"Q1110797_0\", \"text\": \"Who played the main character in the movie Good Morning, Miss Dove?\"}, {\"id\": \"Q1110797_1\", \"text\": \"What is the genre of the movie Good Morning, Miss Dove?\"}, {\"id\": \"Q1110797_2\", \"text\": \"When was the movie Good Morning, Miss Dove released?\"}, {\"id\": \"Q111474934_0\", \"text\": \"How many seasons does The Lesson TV series have?\"}, {\"id\": \"Q111474934_1\", \"text\": \"Who are the main characters in The Lesson TV series?\"}, {\"id\": \"Q111474934_2\", \"text\": \"When was The Lesson TV series first aired?\"}, {\"id\": \"Q11173928_0\", \"text\": \"How does the taste of a duck egg compare to a chicken egg?\"}, {\"id\": \"Q11173928_1\", \"text\": \"Can you eat duck eggs and how are they typically used in cooking?\"}, {\"id\": \"Q11173928_2\", \"text\": \"Are duck eggs nutritionally different from chicken eggs?\"}, {\"id\": \"Q111961959_0\", \"text\": \"Where can I watch episodes of Love Like The Galaxy?\"}, {\"id\": \"Q111961959_1\", \"text\": \"Who are the main characters in the TV series Love Like The Galaxy?\"}, {\"id\": \"Q111961959_2\", \"text\": \"When did the TV series Love Like The Galaxy premiere?\"}, {\"id\": \"Q1144080_0\", \"text\": \"Which filmmaker is known for directing Till Marriage Do Us Part?\"}, {\"id\": \"Q1144080_1\", \"text\": \"Who directed the 1974 film Till Marriage Do Us Part?\"}, {\"id\": \"Q1144080_2\", \"text\": \"What is the subject of the movie Till Marriage Do Us Part?\"}, {\"id\": \"Q1152142_0\", \"text\": \"Is the DC Multiverse a concept used only in the comics or is it also explored in other media?\"}, {\"id\": \"Q1152142_1\", \"text\": \"Which DC Multiverse Earth is home to the Justice League?\"}, {\"id\": \"Q1152142_2\", \"text\": \"Can characters from different Earths in the DC Multiverse interact with each other?\"}, {\"id\": \"Q1156634_0\", \"text\": \"What is the purpose of the cone in an ice cream cone?\"}, {\"id\": \"Q1156634_1\", \"text\": \"How is an ice cream cone made?\"}, {\"id\": \"Q1156634_2\", \"text\": \"Where can you find ice cream cones for sale?\"}, {\"id\": \"Q116945637_0\", \"text\": \"When was the movie All Your Faces released?\"}, {\"id\": \"Q116945637_1\", \"text\": \"What is the genre of the film All Your Faces?\"}, {\"id\": \"Q116945637_2\", \"text\": \"How long is the movie All Your Faces?\"}, {\"id\": \"Q1190888_0\", \"text\": \"Is The Mote in God's Eye a science fiction novel?\"}, {\"id\": \"Q1190888_1\", \"text\": \"When was The Mote in God's Eye published?\"}]\n","\n","Model response: [{'id': 'Q100268160_0', 'translation': 'Wer spielte die Hauptrolle in The Mole – Undercover in North Korea?'}, {'id': 'Q100268160_1', 'translation': 'Wann wurde The Mole – Undercover in North Korea veröffentlicht?'}, {'id': 'Q100268160_2', 'translation': 'Was ist das Thema der TV-Serie The Mole – Undercover in North Korea?'}, {'id': 'Q1012958_0', 'translation': 'Wie können Besucher die Burg Liebenzell erreichen?'}, {'id': 'Q1012958_1', 'translation': 'Wo befindet sich die Burg Liebenzell?'}, {'id': 'Q1012958_2', 'translation': 'Wann wurde die Burg Liebenzell erbaut?'}, {'id': 'Q1056568_0', 'translation': 'Wer spielte in dem Film Warning Sign von 1985 mit?'}, {'id': 'Q1056568_1', 'translation': 'Was ist das Thema des Films Warning Sign?'}, {'id': 'Q1056568_2', 'translation': 'Wann wurde der Film Warning Sign veröffentlicht?'}, {'id': 'Q1062531_0', 'translation': 'Was sind einige Möglichkeiten, ein gekochtes Ei zu würzen?'}, {'id': 'Q1062531_1', 'translation': 'Wie schält man ein gekochtes Ei?'}, {'id': 'Q1062531_2', 'translation': 'Kann man ein gekochtes Ei kalt essen?'}, {'id': 'Q1072093_0', 'translation': 'Was sind einige der Hauptzutaten in der Schokolade des US-Militärs?'}, {'id': 'Q1072093_1', 'translation': 'Wie unterscheidet sich die Schokolade des US-Militärs von normaler Schokolade?'}, {'id': 'Q1072093_2', 'translation': 'Wie lange ist die Schokolade des US-Militärs bereits in Rationen enthalten?'}, {'id': 'Q10860912_0', 'translation': 'Wie heißt der sprechende Hund, der Teil der Familie Griffin ist?'}, {'id': 'Q10860912_1', 'translation': 'Wie heißt die Zeichentrickserie, die die fiktive Familie Griffin zeigt?'}, {'id': 'Q10860912_2', 'translation': 'Welche Zeichentrickserie ist bekannt für ihre satirische Darstellung des Lebens der Familie Griffin?'}, {'id': 'Q1087216_0', 'translation': 'Welches musikalische Genre hat Christus am Ölberge?'}, {'id': 'Q1087216_1', 'translation': 'Wer komponierte das Oratorium Christus am Ölberge?'}, {'id': 'Q1087216_2', 'translation': 'Wer schrieb das Libretto für Beethovens Christus am Ölberge?'}, {'id': 'Q109658712_0', 'translation': 'Welchem Genre gehört Anxious People an?'}, {'id': 'Q109658712_1', 'translation': 'Wann wurde Anxious People veröffentlicht?'}, {'id': 'Q109658712_2', 'translation': 'Aus welchem Land stammt Anxious People?'}, {'id': 'Q1110797_0', 'translation': 'Wer spielte die Hauptfigur im Film Good Morning, Miss Dove?'}, {'id': 'Q1110797_1', 'translation': 'Welches Genre hat der Film Good Morning, Miss Dove?'}, {'id': 'Q1110797_2', 'translation': 'Wann wurde der Film Good Morning, Miss Dove veröffentlicht?'}, {'id': 'Q111474934_0', 'translation': 'Wie viele Staffeln hat die TV-Serie The Lesson?'}, {'id': 'Q111474934_1', 'translation': 'Wer sind die Hauptfiguren in der TV-Serie The Lesson?'}, {'id': 'Q111474934_2', 'translation': 'Wann wurde die TV-Serie The Lesson erstmals ausgestrahlt?'}, {'id': 'Q11173928_0', 'translation': 'Wie unterscheidet sich der Geschmack eines Enteneis von einem Hühnerei?'}, {'id': 'Q11173928_1', 'translation': 'Kann man Enteneier essen und wie werden sie typischerweise in der Küche verwendet?'}, {'id': 'Q11173928_2', 'translation': 'Unterscheiden sich Enteneier ernährungsphysiologisch von Hühnereiern?'}, {'id': 'Q111961959_0', 'translation': 'Wo kann ich Episoden von Love Like The Galaxy ansehen?'}, {'id': 'Q111961959_1', 'translation': 'Wer sind die Hauptfiguren in der TV-Serie Love Like The Galaxy?'}, {'id': 'Q111961959_2', 'translation': 'Wann hatte die TV-Serie Love Like The Galaxy Premiere?'}, {'id': 'Q1144080_0', 'translation': 'Welcher Filmemacher ist bekannt für die Regie von Till Marriage Do Us Part?'}, {'id': 'Q1144080_1', 'translation': 'Wer führte Regie bei dem Film Till Marriage Do Us Part von 1974?'}, {'id': 'Q1144080_2', 'translation': 'Was ist das Thema des Films Till Marriage Do Us Part?'}, {'id': 'Q1152142_0', 'translation': 'Ist das DC-Multiversum ein Konzept, das nur in den Comics verwendet wird, oder wird es auch in anderen Medien erforscht?'}, {'id': 'Q1152142_1', 'translation': 'Welche Erde des DC-Multiversums ist die Heimat der Justice League?'}, {'id': 'Q1152142_2', 'translation': 'Können Charaktere aus verschiedenen Erden im DC-Multiversum miteinander interagieren?'}, {'id': 'Q1156634_0', 'translation': 'Was ist der Zweck der Waffel in einer Eiswaffel?'}, {'id': 'Q1156634_1', 'translation': 'Wie wird eine Eiswaffel hergestellt?'}, {'id': 'Q1156634_2', 'translation': 'Wo kann man Eiswaffeln kaufen?'}, {'id': 'Q116945637_0', 'translation': 'Wann wurde der Film All Your Faces veröffentlicht?'}, {'id': 'Q116945637_1', 'translation': 'Welches Genre hat der Film All Your Faces?'}, {'id': 'Q116945637_2', 'translation': 'Wie lang ist der Film All Your Faces?'}, {'id': 'Q1190888_0', 'translation': \"Ist The Mote in God's Eye ein Science-Fiction-Roman?\"}, {'id': 'Q1190888_1', 'translation': \"Wann wurde The Mote in God's Eye veröffentlicht?\"}]\n"]},{"output_type":"stream","name":"stderr","text":["\n","Translating de_DE.jsonl in batches (few-shot):   7%|▋         | 1/15 [00:28<06:41, 28.66s/it]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Sending prompt to model: \n","Your task is to translate English sentences into German.\n","Below are a few examples of English sentences with named entities translated correctly into German.\n","Pay close attention to how named entities (e.g., people, organizations, locations, product names) are translated correctly and consistently.\n","\n","Examples:\n","[{\"source\": \"What is the seventh tallest mountain in North America?\", \"target\": \"Wie heißt der siebthöchste Berg Nordamerikas?\"}, {\"source\": \"What year was the first book of the A Song of Ice and Fire series published?\", \"target\": \"In welchem Jahr wurde das erste Buch der Reihe \\\"Das Lied von Eis und Feuer\\\" veröffentlicht?\"}, {\"source\": \"Who is the youngest current US governor?\", \"target\": \"Wer ist derzeit der jüngste amerikanische Gouverneur?\"}, {\"source\": \"Has Bernie Sanders ever been president of the United States?\", \"target\": \"War Bernie Sanders jemals Präsident der Vereinigten Staaten?\"}, {\"source\": \"Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?\", \"target\": \"Welcher Schauspieler war Stephanie Meyers erste Wahl für die Rolle des Edward Cullen in dem Film Twilight – Biss zum Morgengrauen?\"}, {\"source\": \"Which river is longer than the Mississippi River?\", \"target\": \"Welcher Fluss ist länger als der Mississippi\"}, {\"source\": \"What is the longest lake in the world?\", \"target\": \"Welcher See ist der längste der Welt?\"}, {\"source\": \"Is Texas the largest state in US?\", \"target\": \"Ist Texas der größte Bundesstaat in den Vereinigten Staaten?\"}, {\"source\": \"Who was the president of Argentina from 1989 to 1999?\", \"target\": \"Wer war von 1989 - 1999 Präsident von Argentinien?\"}, {\"source\": \"Who was a member of the 2004 U.S. Olympic mens swim team and born in Baltimore, Maryland?\", \"target\": \"Wer gehörte 2004 zur olympischen, Schwimmstaffel der Vereinigten Staaten und wurde in Baltimore, Maryland geboren?\"}]\n","\n","Now, translate the following new English sentences into German.\n","Provide the translations as a JSON array of objects, where each object contains the original \"id\" and its \"translation\" (ensure the key name is \"translation\").\n","Maintain the original order of sentences from the input. Please return the json array without any additional text or formatting.\n","\n","Input JSON:\n","[{\"id\": \"Q1190888_2\", \"text\": \"What type of artwork is The Mote in God's Eye?\"}, {\"id\": \"Q1195553_0\", \"text\": \"When was the animated film, The Wonderful World of Puss 'n Boots, released?\"}, {\"id\": \"Q1195553_1\", \"text\": \"Who directed the 1969 animated film, The Wonderful World of Puss 'n Boots?\"}, {\"id\": \"Q1195553_2\", \"text\": \"Can you provide a brief description of the 1969 film, The Wonderful World of Puss 'n Boots?\"}, {\"id\": \"Q1203306_0\", \"text\": \"When was the first volume of Embracing Love manga released?\"}, {\"id\": \"Q1203306_1\", \"text\": \"Who is the author of the Embracing Love book series?\"}, {\"id\": \"Q1203306_2\", \"text\": \"What is the main storyline of the Embracing Love TV series?\"}, {\"id\": \"Q1204959_0\", \"text\": \"How does the Ministry of Truth control the dissemination of information in Oceania?\"}, {\"id\": \"Q1204959_2\", \"text\": \"In what ways does the Ministry of Truth manipulate historical records in the novel?\"}, {\"id\": \"Q1216303_0\", \"text\": \"How is Orpheus depicted in the literary work Sonnets to Orpheus?\"}, {\"id\": \"Q1216303_1\", \"text\": \"What is the main theme explored in the book Sonnets to Orpheus?\"}, {\"id\": \"Q1216303_2\", \"text\": \"Who is the author of the collection of sonnets titled Sonnets to Orpheus?\"}, {\"id\": \"Q1230632_0\", \"text\": \"Who are the main characters in the TV series Tinker Tailor Soldier Spy?\"}, {\"id\": \"Q1230632_1\", \"text\": \"How many episodes does the TV series Tinker Tailor Soldier Spy have?\"}, {\"id\": \"Q1230632_2\", \"text\": \"In which time period does the TV series Tinker Tailor Soldier Spy take place?\"}, {\"id\": \"Q1235125_0\", \"text\": \"Is Into the Wild a work of fiction or non-fiction?\"}, {\"id\": \"Q1235125_1\", \"text\": \"Who wrote the book Into the Wild?\"}, {\"id\": \"Q1235125_2\", \"text\": \"When was Into the Wild published?\"}, {\"id\": \"Q1242146_0\", \"text\": \"Who is the patron saint of the Donskoy Monastery?\"}, {\"id\": \"Q1242146_1\", \"text\": \"What is the architectural style of the Donskoy Monastery?\"}, {\"id\": \"Q1242146_2\", \"text\": \"When was the Donskoy Monastery founded?\"}, {\"id\": \"Q1257747_0\", \"text\": \"Does Trinity Church in Nikitniki have any historical significance?\"}, {\"id\": \"Q1257747_1\", \"text\": \"What is the architectural style of Trinity Church in Nikitniki?\"}, {\"id\": \"Q1257747_2\", \"text\": \"How old is Trinity Church in Nikitniki?\"}, {\"id\": \"Q1264983_0\", \"text\": \"Who created The Man from U.N.C.L.E. TV series?\"}, {\"id\": \"Q1264983_1\", \"text\": \"Who were the main characters in The Man from U.N.C.L.E.?\"}, {\"id\": \"Q1264983_2\", \"text\": \"What was the premise of The Man from U.N.C.L.E. TV series?\"}, {\"id\": \"Q126877_0\", \"text\": \"What is Gorky Park known for?\"}, {\"id\": \"Q126877_1\", \"text\": \"Where is Gorky Park located?\"}, {\"id\": \"Q126877_2\", \"text\": \"What type of place is Gorky Park?\"}, {\"id\": \"Q1305660_0\", \"text\": \"In the House: How many seasons of the TV series In the House were produced?\"}, {\"id\": \"Q1305660_1\", \"text\": \"In the House: Who are the main characters in the television series In the House?\"}, {\"id\": \"Q1305660_2\", \"text\": \"In the House: Which network aired the TV series In the House?\"}, {\"id\": \"Q131119_0\", \"text\": \"What is the significance of the First Epistle of Peter in Christianity?\"}, {\"id\": \"Q131119_1\", \"text\": \"Who wrote the First Epistle of Peter?\"}, {\"id\": \"Q131119_2\", \"text\": \"What is the subject matter of the First Epistle of Peter?\"}, {\"id\": \"Q13407659_0\", \"text\": \"Where can I watch Valley of the Wolves online?\"}, {\"id\": \"Q13407659_1\", \"text\": \"Who are the main characters in Valley of the Wolves?\"}, {\"id\": \"Q13407659_2\", \"text\": \"How many seasons of Valley of the Wolves are there?\"}, {\"id\": \"Q13501123_0\", \"text\": \"How many seasons were there in the TV series Worzel Gummidge?\"}, {\"id\": \"Q13501123_1\", \"text\": \"When was the TV series Worzel Gummidge first aired?\"}, {\"id\": \"Q13501123_2\", \"text\": \"What was the genre of the TV series Worzel Gummidge?\"}, {\"id\": \"Q1367269_0\", \"text\": \"What notable features does the Moscow Belorussky railway station have?\"}, {\"id\": \"Q1367269_1\", \"text\": \"What is the purpose of the Moscow Belorussky railway station?\"}, {\"id\": \"Q1367269_2\", \"text\": \"How can one reach the Moscow Belorussky railway station?\"}, {\"id\": \"Q1381934_0\", \"text\": \"Who lived in the Ipatiev House?\"}, {\"id\": \"Q1381934_1\", \"text\": \"Where is the Ipatiev House located?\"}, {\"id\": \"Q1381934_2\", \"text\": \"When was the Ipatiev House built?\"}, {\"id\": \"Q140527_0\", \"text\": \"What is the genre of The Three Musketeers?\"}, {\"id\": \"Q140527_1\", \"text\": \"In what year was The Three Musketeers published?\"}]\n","\n","Model response: [{'id': 'Q1190888_2', 'translation': 'Welche Art von Kunstwerk ist \"The Mote in God\\'s Eye\"?'}, {'id': 'Q1195553_0', 'translation': 'Wann wurde der Animationsfilm \"The Wonderful World of Puss \\'n Boots\" veröffentlicht?'}, {'id': 'Q1195553_1', 'translation': 'Wer führte Regie bei dem Animationsfilm von 1969 \"The Wonderful World of Puss \\'n Boots\"?'}, {'id': 'Q1195553_2', 'translation': 'Können Sie eine kurze Beschreibung des Films von 1969 \"The Wonderful World of Puss \\'n Boots\" geben?'}, {'id': 'Q1203306_0', 'translation': 'Wann wurde der erste Band des Mangas \"Embracing Love\" veröffentlicht?'}, {'id': 'Q1203306_1', 'translation': 'Wer ist der Autor der Buchreihe \"Embracing Love\"?'}, {'id': 'Q1203306_2', 'translation': 'Was ist die Hauptgeschichte der TV-Serie \"Embracing Love\"?'}, {'id': 'Q1204959_0', 'translation': 'Wie kontrolliert das Ministerium für Wahrheit die Verbreitung von Informationen in Ozeanien?'}, {'id': 'Q1204959_2', 'translation': 'Auf welche Weise manipuliert das Ministerium für Wahrheit historische Aufzeichnungen im Roman?'}, {'id': 'Q1216303_0', 'translation': 'Wie wird Orpheus in dem literarischen Werk \"Sonette an Orpheus\" dargestellt?'}, {'id': 'Q1216303_1', 'translation': 'Was ist das Hauptthema, das in dem Buch \"Sonette an Orpheus\" behandelt wird?'}, {'id': 'Q1216303_2', 'translation': 'Wer ist der Autor der Sammlung von Sonetten mit dem Titel \"Sonette an Orpheus\"?'}, {'id': 'Q1230632_0', 'translation': 'Wer sind die Hauptfiguren in der TV-Serie \"Dame, König, As, Spion\"?'}, {'id': 'Q1230632_1', 'translation': 'Wie viele Episoden hat die TV-Serie \"Dame, König, As, Spion\"?'}, {'id': 'Q1230632_2', 'translation': 'In welcher Zeitperiode spielt die TV-Serie \"Dame, König, As, Spion\"?'}, {'id': 'Q1235125_0', 'translation': 'Ist \"Into the Wild\" ein Werk der Fiktion oder der Non-Fiktion?'}, {'id': 'Q1235125_1', 'translation': 'Wer schrieb das Buch \"Into the Wild\"?'}, {'id': 'Q1235125_2', 'translation': 'Wann wurde \"Into the Wild\" veröffentlicht?'}, {'id': 'Q1242146_0', 'translation': 'Wer ist der Schutzpatron des Donskoi-Klosters?'}, {'id': 'Q1242146_1', 'translation': 'Welcher Architekturstil zeichnet das Donskoi-Kloster aus?'}, {'id': 'Q1242146_2', 'translation': 'Wann wurde das Donskoi-Kloster gegründet?'}, {'id': 'Q1257747_0', 'translation': 'Hat die Dreifaltigkeitskirche in Nikitniki eine historische Bedeutung?'}, {'id': 'Q1257747_1', 'translation': 'Welcher Architekturstil zeichnet die Dreifaltigkeitskirche in Nikitniki aus?'}, {'id': 'Q1257747_2', 'translation': 'Wie alt ist die Dreifaltigkeitskirche in Nikitniki?'}, {'id': 'Q1264983_0', 'translation': 'Wer hat die TV-Serie \"The Man from U.N.C.L.E.\" geschaffen?'}, {'id': 'Q1264983_1', 'translation': 'Wer waren die Hauptfiguren in \"The Man from U.N.C.L.E.\"?'}, {'id': 'Q1264983_2', 'translation': 'Was war das Konzept der TV-Serie \"The Man from U.N.C.L.E.\"?'}, {'id': 'Q126877_0', 'translation': 'Wofür ist der Gorki-Park bekannt?'}, {'id': 'Q126877_1', 'translation': 'Wo befindet sich der Gorki-Park?'}, {'id': 'Q126877_2', 'translation': 'Welche Art von Ort ist der Gorki-Park?'}, {'id': 'Q1305660_0', 'translation': 'In the House: Wie viele Staffeln der TV-Serie \"In the House\" wurden produziert?'}, {'id': 'Q1305660_1', 'translation': 'In the House: Wer sind die Hauptfiguren in der Fernsehserie \"In the House\"?'}, {'id': 'Q1305660_2', 'translation': 'In the House: Welcher Sender strahlte die TV-Serie \"In the House\" aus?'}, {'id': 'Q131119_0', 'translation': 'Welche Bedeutung hat der Erste Petrusbrief im Christentum?'}, {'id': 'Q131119_1', 'translation': 'Wer schrieb den Ersten Petrusbrief?'}, {'id': 'Q131119_2', 'translation': 'Was ist das Thema des Ersten Petrusbriefs?'}, {'id': 'Q13407659_0', 'translation': 'Wo kann ich \"Valley of the Wolves\" online sehen?'}, {'id': 'Q13407659_1', 'translation': 'Wer sind die Hauptfiguren in \"Valley of the Wolves\"?'}, {'id': 'Q13407659_2', 'translation': 'Wie viele Staffeln gibt es von \"Valley of the Wolves\"?'}, {'id': 'Q13501123_0', 'translation': 'Wie viele Staffeln gab es in der TV-Serie \"Worzel Gummidge\"?'}, {'id': 'Q13501123_1', 'translation': 'Wann wurde die TV-Serie \"Worzel Gummidge\" erstmals ausgestrahlt?'}, {'id': 'Q13501123_2', 'translation': 'Welches Genre hatte die TV-Serie \"Worzel Gummidge\"?'}, {'id': 'Q1367269_0', 'translation': 'Welche bemerkenswerten Merkmale hat der Moskauer Belorussky-Bahnhof?'}, {'id': 'Q1367269_1', 'translation': 'Was ist der Zweck des Moskauer Belorussky-Bahnhofs?'}, {'id': 'Q1367269_2', 'translation': 'Wie kann man den Moskauer Belorussky-Bahnhof erreichen?'}, {'id': 'Q1381934_0', 'translation': 'Wer lebte im Ipatiev-Haus?'}, {'id': 'Q1381934_1', 'translation': 'Wo befindet sich das Ipatiev-Haus?'}, {'id': 'Q1381934_2', 'translation': 'Wann wurde das Ipatiev-Haus gebaut?'}, {'id': 'Q140527_0', 'translation': 'Welches Genre hat \"Die drei Musketiere\"?'}, {'id': 'Q140527_1', 'translation': 'In welchem Jahr wurde \"Die drei Musketiere\" veröffentlicht?'}]\n"]},{"output_type":"stream","name":"stderr","text":["\n","Translating de_DE.jsonl in batches (few-shot):  13%|█▎        | 2/15 [00:53<05:41, 26.29s/it]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Sending prompt to model: \n","Your task is to translate English sentences into German.\n","Below are a few examples of English sentences with named entities translated correctly into German.\n","Pay close attention to how named entities (e.g., people, organizations, locations, product names) are translated correctly and consistently.\n","\n","Examples:\n","[{\"source\": \"What is the seventh tallest mountain in North America?\", \"target\": \"Wie heißt der siebthöchste Berg Nordamerikas?\"}, {\"source\": \"What year was the first book of the A Song of Ice and Fire series published?\", \"target\": \"In welchem Jahr wurde das erste Buch der Reihe \\\"Das Lied von Eis und Feuer\\\" veröffentlicht?\"}, {\"source\": \"Who is the youngest current US governor?\", \"target\": \"Wer ist derzeit der jüngste amerikanische Gouverneur?\"}, {\"source\": \"Has Bernie Sanders ever been president of the United States?\", \"target\": \"War Bernie Sanders jemals Präsident der Vereinigten Staaten?\"}, {\"source\": \"Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?\", \"target\": \"Welcher Schauspieler war Stephanie Meyers erste Wahl für die Rolle des Edward Cullen in dem Film Twilight – Biss zum Morgengrauen?\"}, {\"source\": \"Which river is longer than the Mississippi River?\", \"target\": \"Welcher Fluss ist länger als der Mississippi\"}, {\"source\": \"What is the longest lake in the world?\", \"target\": \"Welcher See ist der längste der Welt?\"}, {\"source\": \"Is Texas the largest state in US?\", \"target\": \"Ist Texas der größte Bundesstaat in den Vereinigten Staaten?\"}, {\"source\": \"Who was the president of Argentina from 1989 to 1999?\", \"target\": \"Wer war von 1989 - 1999 Präsident von Argentinien?\"}, {\"source\": \"Who was a member of the 2004 U.S. Olympic mens swim team and born in Baltimore, Maryland?\", \"target\": \"Wer gehörte 2004 zur olympischen, Schwimmstaffel der Vereinigten Staaten und wurde in Baltimore, Maryland geboren?\"}]\n","\n","Now, translate the following new English sentences into German.\n","Provide the translations as a JSON array of objects, where each object contains the original \"id\" and its \"translation\" (ensure the key name is \"translation\").\n","Maintain the original order of sentences from the input. Please return the json array without any additional text or formatting.\n","\n","Input JSON:\n","[{\"id\": \"Q140527_2\", \"text\": \"Can you name any popular adaptations of The Three Musketeers?\"}, {\"id\": \"Q1408916_0\", \"text\": \"Can visitors explore the Ivangorod Fortress today?\"}, {\"id\": \"Q1408916_1\", \"text\": \"What is the historical significance of the Ivangorod Fortress?\"}, {\"id\": \"Q1408916_2\", \"text\": \"How old is the Ivangorod Fortress?\"}, {\"id\": \"Q1413856_0\", \"text\": \"What type of musical work is Geographical Fugue?\"}, {\"id\": \"Q1413856_1\", \"text\": \"How would you describe Geographical Fugue?\"}, {\"id\": \"Q1413856_2\", \"text\": \"What is the subject of Geographical Fugue?\"}, {\"id\": \"Q1416458_0\", \"text\": \"How would you describe Stand on Zanzibar in one word?\"}, {\"id\": \"Q1416458_1\", \"text\": \"Who is the author of Stand on Zanzibar?\"}, {\"id\": \"Q1416458_2\", \"text\": \"When was Stand on Zanzibar first published?\"}, {\"id\": \"Q1417153_0\", \"text\": \"What is known about Finger Snail's tomb or burial site?\"}, {\"id\": \"Q1417153_1\", \"text\": \"What are some notable achievements or contributions attributed to Finger Snail during his reign as Pharaoh?\"}, {\"id\": \"Q1417153_2\", \"text\": \"How long did Finger Snail rule as Pharaoh in ancient Egypt?\"}, {\"id\": \"Q1417441_0\", \"text\": \"What city is St Volodymyr's Cathedral located in?\"}, {\"id\": \"Q1417441_1\", \"text\": \"What type of landmark is St Volodymyr's Cathedral?\"}, {\"id\": \"Q1417441_2\", \"text\": \"How would you describe St Volodymyr's Cathedral?\"}, {\"id\": \"Q1424212_0\", \"text\": \"What type of entity is Mr. Rossi?\"}, {\"id\": \"Q1424212_1\", \"text\": \"What is the origin of Mr. Rossi?\"}, {\"id\": \"Q1424212_2\", \"text\": \"Who created the fictional character Mr. Rossi?\"}, {\"id\": \"Q1432585_0\", \"text\": \"How many platforms does Kazansky railway station have?\"}, {\"id\": \"Q1432585_1\", \"text\": \"What is the main purpose of Kazansky railway station?\"}, {\"id\": \"Q1432585_2\", \"text\": \"Where is Kazansky railway station located in Moscow?\"}, {\"id\": \"Q1444768_0\", \"text\": \"Can you provide a brief summary of the plot of the movie Son of Godzilla?\"}, {\"id\": \"Q1444768_1\", \"text\": \"Who directed the 1967 film Son of Godzilla?\"}, {\"id\": \"Q1444768_2\", \"text\": \"When was the movie Son of Godzilla released?\"}, {\"id\": \"Q1449713_0\", \"text\": \"Who created the Hours of Philip the Bold book?\"}, {\"id\": \"Q1449713_1\", \"text\": \"How would you describe the style of the artwork in the Hours of Philip the Bold?\"}, {\"id\": \"Q1449713_2\", \"text\": \"Where can one find a copy of the Hours of Philip the Bold book?\"}, {\"id\": \"Q1504526_0\", \"text\": \"Is Comic Book Guy a real person or a fictional character?\"}, {\"id\": \"Q1504526_1\", \"text\": \"What is Comic Book Guy's occupation?\"}, {\"id\": \"Q1504526_2\", \"text\": \"Which television franchise does Comic Book Guy appear in?\"}, {\"id\": \"Q1517519_0\", \"text\": \"What is the genre of The Book of the Courtier?\"}, {\"id\": \"Q1517519_1\", \"text\": \"Who wrote The Book of the Courtier?\"}, {\"id\": \"Q1517519_2\", \"text\": \"When was The Book of the Courtier published?\"}, {\"id\": \"Q1523586_0\", \"text\": \"Who were the main actors in A Hatful of Rain?\"}, {\"id\": \"Q1523586_1\", \"text\": \"What year was the movie A Hatful of Rain released?\"}, {\"id\": \"Q1523586_2\", \"text\": \"What is the genre of the film A Hatful of Rain?\"}, {\"id\": \"Q15260610_0\", \"text\": \"What genre does One in one! belong to?\"}, {\"id\": \"Q15260610_1\", \"text\": \"How many seasons of One in one! have been aired?\"}, {\"id\": \"Q15260610_2\", \"text\": \"Who are the main characters in the Russian television show One in one!?\"}, {\"id\": \"Q15299082_0\", \"text\": \"What was the main storyline of the Turkish TV series Do not worry for me?\"}, {\"id\": \"Q15299082_1\", \"text\": \"How many seasons of the TV series Do not worry for me were released?\"}, {\"id\": \"Q15299082_2\", \"text\": \"What genre does the TV series Do not worry for me belong to?\"}, {\"id\": \"Q1545219_0\", \"text\": \"Is Father Brown a real or fictional entity?\"}, {\"id\": \"Q1545219_1\", \"text\": \"What is the profession of Father Brown?\"}, {\"id\": \"Q1545219_2\", \"text\": \"In which country was Father Brown created?\"}, {\"id\": \"Q154563_0\", \"text\": \"How would you describe the Berlin Cathedral?\"}, {\"id\": \"Q154563_1\", \"text\": \"What type of place is the Berlin Cathedral?\"}, {\"id\": \"Q1548449_0\", \"text\": \"What is the Great Synagogue known for in the Czech Republic?\"}, {\"id\": \"Q1548449_1\", \"text\": \"What type of place is the Great Synagogue in Plzeň?\"}]\n","\n","Model response: [{'id': 'Q140527_2', 'translation': 'Können Sie beliebte Adaptionen von Die drei Musketiere nennen?'}, {'id': 'Q1408916_0', 'translation': 'Können Besucher heute die Festung Iwangorod erkunden?'}, {'id': 'Q1408916_1', 'translation': 'Was ist die historische Bedeutung der Festung Iwangorod?'}, {'id': 'Q1408916_2', 'translation': 'Wie alt ist die Festung Iwangorod?'}, {'id': 'Q1413856_0', 'translation': 'Welche Art von musikalischem Werk ist Geographical Fugue?'}, {'id': 'Q1413856_1', 'translation': 'Wie würden Sie Geographical Fugue beschreiben?'}, {'id': 'Q1413856_2', 'translation': 'Was ist das Thema von Geographical Fugue?'}, {'id': 'Q1416458_0', 'translation': 'Wie würden Sie Stand on Zanzibar mit einem Wort beschreiben?'}, {'id': 'Q1416458_1', 'translation': 'Wer ist der Autor von Stand on Zanzibar?'}, {'id': 'Q1416458_2', 'translation': 'Wann wurde Stand on Zanzibar erstmals veröffentlicht?'}, {'id': 'Q1417153_0', 'translation': 'Was ist über das Grab oder die Begräbnisstätte von Finger Snail bekannt?'}, {'id': 'Q1417153_1', 'translation': 'Welche bemerkenswerten Errungenschaften oder Beiträge werden Finger Snail während seiner Herrschaft als Pharao zugeschrieben?'}, {'id': 'Q1417153_2', 'translation': 'Wie lange regierte Finger Snail als Pharao im alten Ägypten?'}, {'id': 'Q1417441_0', 'translation': 'In welcher Stadt befindet sich die St. Wolodymyr-Kathedrale?'}, {'id': 'Q1417441_1', 'translation': 'Welche Art von Wahrzeichen ist die St. Wolodymyr-Kathedrale?'}, {'id': 'Q1417441_2', 'translation': 'Wie würden Sie die St. Wolodymyr-Kathedrale beschreiben?'}, {'id': 'Q1424212_0', 'translation': 'Welche Art von Entität ist Herr Rossi?'}, {'id': 'Q1424212_1', 'translation': 'Was ist der Ursprung von Herr Rossi?'}, {'id': 'Q1424212_2', 'translation': 'Wer hat die fiktive Figur Herr Rossi erschaffen?'}, {'id': 'Q1432585_0', 'translation': 'Wie viele Bahnsteige hat der Kasaner Bahnhof?'}, {'id': 'Q1432585_1', 'translation': 'Was ist der Hauptzweck des Kasaner Bahnhofs?'}, {'id': 'Q1432585_2', 'translation': 'Wo befindet sich der Kasaner Bahnhof in Moskau?'}, {'id': 'Q1444768_0', 'translation': 'Können Sie eine kurze Zusammenfassung der Handlung des Films Sohn von Godzilla geben?'}, {'id': 'Q1444768_1', 'translation': 'Wer führte Regie bei dem Film Sohn von Godzilla aus dem Jahr 1967?'}, {'id': 'Q1444768_2', 'translation': 'Wann wurde der Film Sohn von Godzilla veröffentlicht?'}, {'id': 'Q1449713_0', 'translation': 'Wer hat das Buch Stunden von Philipp dem Kühnen erstellt?'}, {'id': 'Q1449713_1', 'translation': 'Wie würden Sie den Stil der Kunstwerke in den Stunden von Philipp dem Kühnen beschreiben?'}, {'id': 'Q1449713_2', 'translation': 'Wo kann man eine Kopie des Buches Stunden von Philipp dem Kühnen finden?'}, {'id': 'Q1504526_0', 'translation': 'Ist Comic Book Guy eine reale Person oder eine fiktive Figur?'}, {'id': 'Q1504526_1', 'translation': 'Was ist der Beruf von Comic Book Guy?'}, {'id': 'Q1504526_2', 'translation': 'In welcher Fernsehserie erscheint Comic Book Guy?'}, {'id': 'Q1517519_0', 'translation': 'Welches Genre hat Das Buch der Höflinge?'}, {'id': 'Q1517519_1', 'translation': 'Wer hat Das Buch der Höflinge geschrieben?'}, {'id': 'Q1517519_2', 'translation': 'Wann wurde Das Buch der Höflinge veröffentlicht?'}, {'id': 'Q1523586_0', 'translation': 'Wer waren die Hauptdarsteller in Ein Hut voller Regen?'}, {'id': 'Q1523586_1', 'translation': 'In welchem Jahr wurde der Film Ein Hut voller Regen veröffentlicht?'}, {'id': 'Q1523586_2', 'translation': 'Welches Genre hat der Film Ein Hut voller Regen?'}, {'id': 'Q15260610_0', 'translation': 'Welchem Genre gehört Eins zu eins! an?'}, {'id': 'Q15260610_1', 'translation': 'Wie viele Staffeln von Eins zu eins! wurden ausgestrahlt?'}, {'id': 'Q15260610_2', 'translation': 'Wer sind die Hauptfiguren in der russischen Fernsehsendung Eins zu eins!?'}, {'id': 'Q15299082_0', 'translation': 'Was war die Hauptgeschichte der türkischen TV-Serie Keine Sorge um mich?'}, {'id': 'Q15299082_1', 'translation': 'Wie viele Staffeln der TV-Serie Keine Sorge um mich wurden veröffentlicht?'}, {'id': 'Q15299082_2', 'translation': 'Welchem Genre gehört die TV-Serie Keine Sorge um mich an?'}, {'id': 'Q1545219_0', 'translation': 'Ist Father Brown eine reale oder fiktive Figur?'}, {'id': 'Q1545219_1', 'translation': 'Was ist der Beruf von Father Brown?'}, {'id': 'Q1545219_2', 'translation': 'In welchem Land wurde Father Brown erschaffen?'}, {'id': 'Q154563_0', 'translation': 'Wie würden Sie den Berliner Dom beschreiben?'}, {'id': 'Q154563_1', 'translation': 'Welche Art von Ort ist der Berliner Dom?'}, {'id': 'Q1548449_0', 'translation': 'Wofür ist die Große Synagoge in der Tschechischen Republik bekannt?'}, {'id': 'Q1548449_1', 'translation': 'Welche Art von Ort ist die Große Synagoge in Plzeň?'}]\n"]},{"output_type":"stream","name":"stderr","text":["\n","Translating de_DE.jsonl in batches (few-shot):  20%|██        | 3/15 [01:15<04:53, 24.46s/it]\u001b[A"]},{"output_type":"stream","name":"stdout","text":["Sending prompt to model: \n","Your task is to translate English sentences into German.\n","Below are a few examples of English sentences with named entities translated correctly into German.\n","Pay close attention to how named entities (e.g., people, organizations, locations, product names) are translated correctly and consistently.\n","\n","Examples:\n","[{\"source\": \"What is the seventh tallest mountain in North America?\", \"target\": \"Wie heißt der siebthöchste Berg Nordamerikas?\"}, {\"source\": \"What year was the first book of the A Song of Ice and Fire series published?\", \"target\": \"In welchem Jahr wurde das erste Buch der Reihe \\\"Das Lied von Eis und Feuer\\\" veröffentlicht?\"}, {\"source\": \"Who is the youngest current US governor?\", \"target\": \"Wer ist derzeit der jüngste amerikanische Gouverneur?\"}, {\"source\": \"Has Bernie Sanders ever been president of the United States?\", \"target\": \"War Bernie Sanders jemals Präsident der Vereinigten Staaten?\"}, {\"source\": \"Which actor was Stephenie Meyers first choice to play Edward Cullen in the movie Twilight?\", \"target\": \"Welcher Schauspieler war Stephanie Meyers erste Wahl für die Rolle des Edward Cullen in dem Film Twilight – Biss zum Morgengrauen?\"}, {\"source\": \"Which river is longer than the Mississippi River?\", \"target\": \"Welcher Fluss ist länger als der Mississippi\"}, {\"source\": \"What is the longest lake in the world?\", \"target\": \"Welcher See ist der längste der Welt?\"}, {\"source\": \"Is Texas the largest state in US?\", \"target\": \"Ist Texas der größte Bundesstaat in den Vereinigten Staaten?\"}, {\"source\": \"Who was the president of Argentina from 1989 to 1999?\", \"target\": \"Wer war von 1989 - 1999 Präsident von Argentinien?\"}, {\"source\": \"Who was a member of the 2004 U.S. Olympic mens swim team and born in Baltimore, Maryland?\", \"target\": \"Wer gehörte 2004 zur olympischen, Schwimmstaffel der Vereinigten Staaten und wurde in Baltimore, Maryland geboren?\"}]\n","\n","Now, translate the following new English sentences into German.\n","Provide the translations as a JSON array of objects, where each object contains the original \"id\" and its \"translation\" (ensure the key name is \"translation\").\n","Maintain the original order of sentences from the input. Please return the json array without any additional text or formatting.\n","\n","Input JSON:\n","[{\"id\": \"Q1548449_2\", \"text\": \"What city is home to the Great Synagogue?\"}, {\"id\": \"Q15635278_0\", \"text\": \"When did the TV series Very Good Times first air in South Korea?\"}, {\"id\": \"Q15635278_1\", \"text\": \"How many seasons does the TV series Very Good Times have?\"}, {\"id\": \"Q15635278_2\", \"text\": \"Who are the main actors in the South Korean TV series Very Good Times?\"}, {\"id\": \"Q1579157_0\", \"text\": \"What material is the Mount Royal Cross made of?\"}, {\"id\": \"Q1579157_1\", \"text\": \"Who designed the Mount Royal Cross sculpture?\"}, {\"id\": \"Q1579157_2\", \"text\": \"When was the Mount Royal Cross installed on the mountain?\"}, {\"id\": \"Q1579960_0\", \"text\": \"Who starred in the movie I Never Sang for My Father?\"}, {\"id\": \"Q1579960_1\", \"text\": \"When was the movie I Never Sang for My Father released?\"}, {\"id\": \"Q1579960_2\", \"text\": \"What is the genre of the film I Never Sang for My Father?\"}, {\"id\": \"Q15804357_0\", \"text\": \"Can you provide a brief summary of the plot of Quality Time?\"}, {\"id\": \"Q15804357_1\", \"text\": \"Who directed the 2013 film Quality Time?\"}, {\"id\": \"Q15804357_2\", \"text\": \"When was the movie Quality Time released?\"}, {\"id\": \"Q1585564_0\", \"text\": \"What is the architectural style of Sakya Monastery?\"}, {\"id\": \"Q1585564_1\", \"text\": \"Where is Sakya Monastery located?\"}, {\"id\": \"Q1585564_2\", \"text\": \"What is the significance of Sakya Monastery in the People's Republic of China?\"}, {\"id\": \"Q160341_0\", \"text\": \"Who was Edward IV of England and when did he rule?\"}, {\"id\": \"Q161106_0\", \"text\": \"Who was Grand Duke Konstantin Pavlovich of Russia and what years did he live?\"}, {\"id\": \"Q16150399_1\", \"text\": \"Who are some of the main characters in the TV series American Gangster?\"}, {\"id\": \"Q16150399_2\", \"text\": \"How many seasons of the TV series American Gangster have been released?\"}, {\"id\": \"Q162935_0\", \"text\": \"When was the Piano Concerto No. 1 first performed?\"}, {\"id\": \"Q162935_1\", \"text\": \"What is the genre of the Piano Concerto No. 1?\"}, {\"id\": \"Q162935_2\", \"text\": \"How many movements are there in the Piano Concerto No. 1?\"}, {\"id\": \"Q163493_0\", \"text\": \"When was Symphony No. 1 first performed?\"}, {\"id\": \"Q163493_1\", \"text\": \"What is the musical genre of Symphony No. 1?\"}, {\"id\": \"Q163493_2\", \"text\": \"How many movements are there in Symphony No. 1?\"}, {\"id\": \"Q163600_0\", \"text\": \"How many movements are there in Symphony No. 1?\"}, {\"id\": \"Q163600_1\", \"text\": \"Who composed Symphony No. 1?\"}, {\"id\": \"Q163600_2\", \"text\": \"What is the genre of Symphony No. 1?\"}, {\"id\": \"Q16387049_0\", \"text\": \"What genre does Mistborn: Shadows of Self belong to?\"}, {\"id\": \"Q16387049_1\", \"text\": \"Who is the author of Mistborn: Shadows of Self?\"}, {\"id\": \"Q16387049_2\", \"text\": \"When was Mistborn: Shadows of Self published?\"}, {\"id\": \"Q164216_0\", \"text\": \"What religious denomination is associated with the Monastery of Saint Naum?\"}, {\"id\": \"Q164216_1\", \"text\": \"Where is the Monastery of Saint Naum located?\"}, {\"id\": \"Q164216_2\", \"text\": \"How old is the Monastery of Saint Naum?\"}, {\"id\": \"Q1648509_0\", \"text\": \"How does Marya Bolkonskaya's relationship with her brother, Andrei, affect her character development?\"}, {\"id\": \"Q1648509_1\", \"text\": \"How does Marya Bolkonskaya contribute to the storyline?\"}, {\"id\": \"Q1648509_2\", \"text\": \"What are some key personality traits of Marya Bolkonskaya?\"}, {\"id\": \"Q172077_0\", \"text\": \"What is the significance of the Dome of the Rock?\"}, {\"id\": \"Q172077_1\", \"text\": \"Where is the Dome of the Rock located?\"}, {\"id\": \"Q172077_2\", \"text\": \"What type of place is the Dome of the Rock?\"}, {\"id\": \"Q1739780_0\", \"text\": \"Who are the main characters in the film Dirty Mary, Crazy Larry?\"}, {\"id\": \"Q1739780_1\", \"text\": \"What is the genre of the movie Dirty Mary, Crazy Larry?\"}, {\"id\": \"Q1739780_2\", \"text\": \"When was Dirty Mary, Crazy Larry released?\"}, {\"id\": \"Q1751487_0\", \"text\": \"What is the specialty of Platov South-Russian State Polytechnic University?\"}, {\"id\": \"Q1751487_1\", \"text\": \"What type of educational institution is Platov South-Russian State Polytechnic University?\"}, {\"id\": \"Q1751487_2\", \"text\": \"Where is Platov South-Russian State Polytechnic University situated?\"}, {\"id\": \"Q1762893_0\", \"text\": \"How can Romaine lettuce be stored to keep it fresh?\"}, {\"id\": \"Q1762893_1\", \"text\": \"How is Romaine lettuce different from other types of lettuce?\"}, {\"id\": \"Q1762893_2\", \"text\": \"Can Romaine lettuce be used in salads and sandwiches?\"}]\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n","Processing files (few-shot):   0%|          | 0/10 [01:20<?, ?it/s]\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/tmp/ipython-input-16-2405482697.py\", line 6, in <cell line: 0>\n","    few_shot_eval(\n","  File \"/tmp/ipython-input-15-2179269205.py\", line 25, in few_shot_eval\n","    translated_texts_batch = get_translated_content_with_retries_few_shot(\n","                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 338, in wrapped_f\n","    return copy(f, *args, **kw)\n","           ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 477, in __call__\n","    do = self.iter(retry_state=retry_state)\n","         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 378, in iter\n","    result = action(retry_state)\n","             ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 400, in <lambda>\n","    self._add_action_func(lambda rs: rs.outcome.result())\n","                                     ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n","    return self.__get_result()\n","           ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n","    raise self._exception\n","  File \"/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\", line 480, in __call__\n","    result = fn(*args, **kwargs)\n","             ^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-14-1486642628.py\", line 27, in get_translated_content_with_retries_few_shot\n","    response = llm_instance.invoke(prompt_to_send)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n","    self.generate_prompt(\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n","    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n","    self._generate_with_cache(\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n","    result = self._generate(\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\", line 476, in _generate\n","    response = self.completion_with_retry(\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\", line 387, in completion_with_retry\n","    return self.client.create(**kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\", line 287, in wrapper\n","    return func(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n","    return self._post(\n","           ^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1242, in post\n","    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n","                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 972, in request\n","    response = self._client.send(\n","               ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 914, in send\n","    response = self._send_handling_auth(\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 942, in _send_handling_auth\n","    response = self._send_handling_redirects(\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n","    response = self._send_single_request(request)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpx/_client.py\", line 1014, in _send_single_request\n","    response = transport.handle_request(request)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\", line 250, in handle_request\n","    resp = self._pool.handle_request(req)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n","    raise exc from None\n","  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n","    response = connection.handle_request(\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\", line 103, in handle_request\n","    return self._connection.handle_request(request)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\", line 136, in handle_request\n","    raise exc\n","  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\", line 106, in handle_request\n","    ) = self._receive_response_headers(**kwargs)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\", line 177, in _receive_response_headers\n","    event = self._receive_event(timeout=timeout)\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\", line 217, in _receive_event\n","    data = self._network_stream.read(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\", line 128, in read\n","    return self._sock.recv(max_bytes)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/ssl.py\", line 1295, in recv\n","    return self.read(buflen)\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/ssl.py\", line 1168, in read\n","    return self._sslobj.read(len)\n","           ^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n","    traceback_info = getframeinfo(tb, context)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","               ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n","    module = getmodule(object, filename)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line None, in getmodule\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"TypeError","evalue":"object of type 'NoneType' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-16-2405482697.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m few_shot_eval(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfew_shot_prompt_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-15-2179269205.py\u001b[0m in \u001b[0;36mfew_shot_eval\u001b[0;34m(template, template_id, few_shot_examples)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 translated_texts_batch = get_translated_content_with_retries_few_shot(\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0mbatch_records\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfew_shot_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_locale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-14-1486642628.py\u001b[0m in \u001b[0;36mget_translated_content_with_retries_few_shot\u001b[0;34m(source_records_batch, target_language, llm_instance, template, few_shot_examples)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_to_send\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mapi_request_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 results.append(\n\u001b[0;32m--> 776\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         }\n\u001b[0;32m--> 476\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    973\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}],"source":["# Build few-shot prompt template with support for in-context examples\n","few_shot_prompt_template = PromptTemplate(\n","    input_variables=[\"source_texts_json\", \"target_language\", \"few_shot_examples_json\"],\n","    template=FEW_SHOT_PROMPT_TEMPLATE_STRING,\n",")\n","\n","# Run the few-shot evaluation pipeline using provided examples\n","few_shot_eval(\n","    few_shot_prompt_template,\n","    \"few-shot\",\n","    few_shot_examples_dict\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"7b8958c6","executionInfo":{"status":"ok","timestamp":1750488370465,"user_tz":-330,"elapsed":3710583,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a51475e-1bff-4f59-be50-6b29a04f0421"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1260 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [15:27<00:00, 23.19s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 89.88\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1316 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","Predicting DataLoader 0: 100%|██████████| 42/42 [17:32<00:00, 25.05s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 89.92\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1654 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 52/52 [20:12<00:00, 23.31s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 30.63\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1660 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 52/52 [18:22<00:00, 21.21s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 31.88\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1229 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 39/39 [14:59<00:00, 23.06s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 92.62\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1268 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [14:52<00:00, 22.30s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 91.40\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Created 1409 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 45/45 [16:25<00:00, 21.90s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 32.04\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Created 1177 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 37/37 [13:04<00:00, 21.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 90.96\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Created 1260 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [13:14<00:00, 19.87s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 33.04\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Created 1544 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 49/49 [16:09<00:00, 19.79s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 32.44\n"]}],"source":["calculate_scores(\"few-shot\")"]},{"cell_type":"markdown","metadata":{"id":"44215089"},"source":["# Chain of Thought Prompting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6e1d56ae"},"outputs":[],"source":["COT_PROMPT_TEMPLATE_STRING = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"\"\"You are an expert linguist and translator. Your primary goal is to translate English sentences into {target_language}.\n","            You must pay special attention to identifying and accurately translating named entities within the sentence.\n","\n","            Here's your step-by-step Chain-of-Thought process for each translation.\n","            Perform these steps internally, and then provide only the final JSON output as requested below.\n","\n","            1.  **Identify Named Entities:**\n","                Go through each English sentence and meticulously identify all named entities.\n","                Named entities include, but are not limited to, persons, organizations, locations, dates, times, and specific titles.\n","\n","            2.  **Determine Entity Type:**\n","                For each named entity identified in step 1, classify its specific type.\n","                Examples of types include: Person, Organization (ORG), Location (LOC), Date, Time, Title, Musical work, Artwork,\n","                Food, Animal, Plant, Book, Book series, Fictional entity, Landmark, Movie, Place of worship,\n","                Natural place, TV series.\n","                This classification provides crucial context for accurate translation.\n","\n","            3.  **Contextual Translation of Named Entities:**\n","                For each identified named entity, determine its most appropriate translation into {target_language}.\n","                Consider the entity's type and the overall context of the sentence.\n","                - For proper nouns (like names of people or specific places), often a transliteration or the commonly accepted international form is best.\n","                - For organizations, use their official translated name if available, otherwise transliterate or provide a descriptive translation.\n","                - For dates and times, translate them according to the {target_language}'s conventions.\n","\n","            4.  **Translate Remaining Sentence:**\n","                Translate all parts of the English sentence that are NOT named entities.\n","                Ensure that this translation is grammatically correct, natural-sounding, and culturally appropriate for {target_language}.\n","\n","            5.  **Combine and Finalize Translation:**\n","                Integrate the accurately translated named entities (from step 3) back into the translated sentence (from step 4).\n","                Construct the final, complete, and fluent translation of the original English sentence.\n","\n","            Your final output MUST be a JSON array of objects. Each object MUST contain two fields: \"id\" (the original sentence ID) and \"translation\" (the complete translated sentence).\n","            Do NOT include any other text, reasoning steps, or formatting outside of the JSON array.\n","            Example for JSON output:\n","            ```json\n","            [\n","              {{\"id\": \"sentence_id_1\", \"translation\": \"Translated sentence 1.\"}},\n","              {{\"id\": \"sentence_id_2\", \"translation\": \"Translated sentence 2.\"}}\n","            ]\n","            ```\n","            \"\"\"\n","        ),\n","        (\"human\", \"Translate the following English sentences into {target_language}:\\n{source_texts_json}\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87e43be3"},"outputs":[],"source":["# Retry-enabled CoT translation handler\n","@retry(\n","    wait=wait_random_exponential(multiplier=1, min=DELAY_BETWEEN_REQUESTS_SECONDS, max=60),\n","    stop=stop_after_attempt(5),\n","    retry=retry_if_exception_type(Exception)\n",")\n","def get_translated_content_with_retries_using_cot(source_records_batch, target_language, llm_instance, template):\n","    global api_request_count\n","\n","    # Prepare prompt input\n","    input_json_for_prompt = [{\"id\": rec['id'], \"text\": rec['source']} for rec in source_records_batch]\n","    source_texts_json_str = json.dumps(input_json_for_prompt, ensure_ascii=False)\n","\n","    # Handle both ChatPromptTemplate and string template\n","    if isinstance(template, ChatPromptTemplate):\n","        response = llm_instance.invoke(template.format_messages(\n","            target_language=target_language,\n","            source_texts_json=source_texts_json_str\n","        ))\n","        raw_content = response.content\n","    else:\n","        prompt_to_send = template.format(\n","            source_texts_json=source_texts_json_str,\n","            target_language=target_language\n","        )\n","        response = llm_instance.invoke(prompt_to_send)\n","        raw_content = response.content\n","\n","    api_request_count += 1\n","\n","    try:\n","        # Clean markdown-wrapped JSON (```json ... ```)\n","        cleaned_content = raw_content.strip()\n","        if cleaned_content.startswith(\"```json\") and cleaned_content.endswith(\"```\"):\n","            cleaned_content = cleaned_content[7:-3].strip()\n","\n","        translated_outputs = json.loads(cleaned_content)\n","        print(translated_outputs)  # Debug output\n","\n","        # Validate format\n","        if not isinstance(translated_outputs, list) or \\\n","           not all(isinstance(item, dict) and 'id' in item and 'translation' in item for item in translated_outputs):\n","            raise ValueError(\"Model did not return a valid JSON array of translation objects.\")\n","\n","        # Reorder results to match input\n","        translated_dict = {item['id']: item['translation'] for item in translated_outputs}\n","        ordered_translations = [translated_dict.get(record['id'], \"ERROR: ID not found in JSON output\")\n","                                for record in source_records_batch]\n","\n","        return ordered_translations\n","\n","    except json.JSONDecodeError as e:\n","        print(f\"JSON Decode Error: {e}. Raw response: {raw_content.strip()}\")\n","        return [\"ERROR: Invalid JSON response from model\"] * len(source_records_batch)\n","    except ValueError as e:\n","        print(f\"Value Error (JSON format issue): {e}. Raw response: {raw_content.strip()}\")\n","        return [\"ERROR: Invalid JSON structure from model\"] * len(source_records_batch)\n","    except Exception as e:\n","        print(f\"An unexpected error occurred while processing model output: {e}. Raw response: {raw_content.strip()}\")\n","        return [\"ERROR: Unexpected issue processing model output\"] * len(source_records_batch)\n","\n","# Main evaluation loop for CoT or other flexible templates\n","def translation_eval_cot(template, template_id, few_shot_examples=None):\n","    overall_pbar = tqdm.tqdm(jsonl_files, desc=f\"Processing files ({template_id})\")\n","\n","    output_prediction_sub_dir = os.path.join(output_prediction_dir, template_id)\n","    os.makedirs(output_prediction_sub_dir, exist_ok=True)\n","\n","    for file_path in overall_pbar:\n","        filename = os.path.basename(file_path)\n","        outfile_path = os.path.join(output_prediction_sub_dir, filename)\n","\n","        # Load input file\n","        data_to_translate = []\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                data_to_translate.append(json.loads(line))\n","\n","        translated_results_for_file = []\n","\n","        for i in tqdm.tqdm(range(0, len(data_to_translate), BATCH_SIZE), desc=f\"Translating {filename} in batches\", leave=False):\n","            batch_records = data_to_translate[i : i + BATCH_SIZE]\n","            target_locale = batch_records[0]['target_locale']\n","            target_language = get_language_name(target_locale)\n","\n","            # Handle dynamic few-shot partial or passthrough\n","            if isinstance(template, ChatPromptTemplate):\n","                current_template = template\n","            else:\n","                if few_shot_examples:\n","                    lang_specific_examples = [\n","                        ex for ex in few_shot_examples if get_language_name(ex['target_locale']) == target_language\n","                    ]\n","                    formatted_examples = json.dumps([\n","                        {\"source\": ex['source'], \"target\": ex['target']} for ex in lang_specific_examples\n","                    ], ensure_ascii=False)\n","                    current_template = template.partial(few_shot_examples_json=formatted_examples)\n","                else:\n","                    current_template = template\n","\n","            try:\n","                # Run translation using selected template\n","                translated_texts_batch = get_translated_content_with_retries_using_cot(\n","                    batch_records, target_language, llm, current_template\n","                )\n","\n","                # Format output\n","                for j, record in enumerate(batch_records):\n","                    record_id = record['id']\n","                    source_text = record['source']\n","                    source_locale = record['source_locale']\n","                    translated_text = translated_texts_batch[j] if j < len(translated_texts_batch) else \"ERROR: Translation missing\"\n","                    translated_results_for_file.append({\n","                        \"id\": record_id,\n","                        \"source_language\": get_language_name(source_locale),\n","                        \"target_language\": target_language,\n","                        \"text\": source_text,\n","                        \"prediction\": translated_text,\n","                    })\n","\n","            except Exception as e:\n","                print(f\"\\nCRITICAL ERROR: Failed to translate a batch starting with ID '{batch_records[0]['id']}' after multiple retries. Error: {e}\")\n","                for record in batch_records:\n","                    translated_results_for_file.append({\n","                        \"id\": record['id'],\n","                        \"source_language\": get_language_name(record['source_locale']),\n","                        \"target_language\": get_language_name(record['target_locale']),\n","                        \"text\": record['source'],\n","                        \"prediction\": \"ERROR: Batch translation failed due to API issues/rate limits.\",\n","                    })\n","\n","            # Respect rate limits\n","            if i + BATCH_SIZE < len(data_to_translate) or overall_pbar.n < len(jsonl_files):\n","                time.sleep(DELAY_BETWEEN_REQUESTS_SECONDS)\n","\n","        # Save predictions to file\n","        with open(outfile_path, 'w', encoding='utf-8') as f:\n","            for res in translated_results_for_file:\n","                f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n","\n","        print(f\"\\nTranslations for {filename} saved to {outfile_path}\")\n","\n","    overall_pbar.close()\n","    print(f\"All files processed for {template_id} and translations saved.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cc0db505"},"outputs":[],"source":["translation_eval_cot(COT_PROMPT_TEMPLATE_STRING, \"cot\")\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95ac4c01","executionInfo":{"status":"ok","timestamp":1750515540851,"user_tz":-330,"elapsed":3067045,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"outputId":"92e00b65-2e16-4236-be20-0998e02e114a"},"source":["calculate_scores(\"cot\")"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1260 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","Predicting DataLoader 0: 100%|██████████| 40/40 [14:51<00:00, 22.28s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 89.89\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1316 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 42/42 [16:57<00:00, 24.23s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 89.86\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1654 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 52/52 [20:39<00:00, 23.83s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 83.40\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1660 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 52/52 [19:08<00:00, 22.09s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 93.49\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1229 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 39/39 [14:35<00:00, 22.46s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 92.30\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1268 instances\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [14:30<00:00, 21.76s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Average COMET score: 91.19\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Created 1409 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 45/45 [18:01<00:00, 24.03s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 93.31\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Created 1177 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 37/37 [12:38<00:00, 20.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 89.12\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Created 1260 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 40/40 [13:33<00:00, 20.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 92.24\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n","INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]},{"output_type":"stream","name":"stdout","text":["Created 1544 instances\n"]},{"output_type":"stream","name":"stderr","text":["Predicting DataLoader 0: 100%|██████████| 49/49 [16:36<00:00, 20.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average COMET score: 92.53\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03a59fb6","executionInfo":{"status":"ok","timestamp":1750422017472,"user_tz":-330,"elapsed":8755,"user":{"displayName":"Ayush Jha","userId":"17551614966798331438"}},"outputId":"97369625-d388-4c36-a15d-a5f7584354e7"},"source":[],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_community\n","  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.65)\n","Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n","  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.7)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_community) (4.14.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n","Downloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.25 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"]}]}]}