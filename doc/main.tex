%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    INSTANTIATE YOUR DOCUMENT CLASS WITH THE ECAI OPTION.
%    For final camera-ready copy, replace 'report' with 'final'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{ecai} 
%\documentclass[doubleblind]{ecai} 
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{hyperref}
\usepackage{natbib}


\begin{document}

\begin{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    TITLE SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Team21 - Entity Matters: A Comparative Study of Machine Translation Fidelity in Large Language Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    AUTHOR SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{\fnms{Aastik}~\snm{Shrivastava}}
\author{\fnms{Abhitosh}}
\author{\fnms{Amit}~\snm{Nitin Joshi}}
\author{\fnms{Ayush}~\snm{Ravindra Jha}}
\author{\fnms{Raghavendra}~\snm{Naik}}
\author{\fnms{Sibashis}~\snm{Kumar Sahu}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    ABSTRACT SECTION

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Translation of named entities is challenging for traditional machine translation systems, as there may be cultural 
or domain-specific references that may not be easily translated. 
This impacts the effectiveness of such systems in real-world scenarios. 
We draw inspiration from the SemEval 2025 Task 2 on Entity-Aware Machine Translation for this paper. 
The task was to translate an input sentence containing named entities from English to multiple target languages. 
In this paper, we attempted the task using various techniques and present our findings. 
We used prompt engineering to evaluate some closed-source models like Google's Gemini and OpenAI's ChatGPT. 
For open-sourced models such as Gemma, Llama and Mistral, we use prompt engineering followed by fine-tuning
to asses and improve their performance on the task.
We used Crosslingual Optimized Metric for Evaluation of Translation (COMET; Rei et al., 2020)\cite{conia-etal-2024-m-eta}
and Manual Entity Translation Accuracy (M-ETA; Conia et al., 2024)\cite{rei-etal-2020-comet} 
as metrics for evaluation of the quality of translation generated by these systems.
\end{abstract}
\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    MAIN BODY OF THE PAPER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
The accurate and contextually appropriate translation of named entities remain a formidable challenge
for conventional machine translation systems, as these specific linguistic elements often lack direct
lexical equivalents in target languages. This necessitates a sophisticated comprehension of their real-world
referents and inherent semantic roles. This persistent difficult underscores the critical importance of Entity-Aware
Machine Translation (EAMT). EAMT represents a significant advancement, empowering machine translation systems to actively
recognize, classify and leverage an entity's specific type and attribute. By integrating this nuanced entity intelligence, 
EAMT can effectively resolve ambiguities, ensure consistent rendering across diverse linguistic contexts, 
and generate translations that are not only grammatically sound but also factually precise, 
thereby critically enhancing the overall fidelity and trustworthiness of machine-generated content
 in sensitive and high-stakes applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related}
Our work builds upon several key advancements in machine translation and large language models. 
The field of machine translation was revolutionized by the introduction of Neural Machine Translation (NMT), 
particularly with the advent of the Transformer architecture \cite{vaswani2017attention}, 
which replaced recurrent and convolutional models with a purely attention-based mechanism, 
setting a new standard for performance.
More recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in translation, 
often in a zero-shot or few-shot setting \cite{brown2020language}. 
Models pretrained on vast text corpora using a unified text-to-text framework, 
such as T5 \cite{raffel2020exploring}, have shown that a single model can be effectively prompted 
to perform a wide range of NLP tasks, including high-quality translation without task-specific fine-tuning.
A significant challenge in machine translation is the correct rendering of named entities,
which often carry critical semantic weight. This has been a long-standing area of research, 
with various approaches proposed to make NMT systems more "entity-aware." 
Early work focused on integrating external knowledge or explicitly marking entities to guide the translation process 
\cite{gazzola2019named, ennis2021improving}. 
These methods aim to improve translation fidelity by preventing entities from being misinterpreted 
as common words or being transliterated incorrectly.
To further enhance the contextual understanding of LLMs for specialized tasks, 
Retrieval-Augmented Generation (RAG) has emerged as a powerful technique \cite{lewis2020retrieval}. 
By retrieving relevant documents or knowledge snippets and providing them as context to the generator model, 
RAG has been successfully applied to knowledge-intensive NLP tasks. Its application to machine translation is 
particularly promising for handling domain-specific terminology and entities, 
as it allows the model to ground its translation in factual, retrieved data \cite{cai2021retrieval}, 
a principle that underpins our RAG-based experiments with Wikidata. 
Finally, for evaluation, we rely on established metrics like COMET \cite{rei-etal-2020-comet}, 
a reference-based metric that leverages cross-lingual pretrained models to achieve high correlation
with human judgments of translation quality, and M-ETA \cite{conia-etal-2024-m-eta},
which specifically measures the accuracy of entity translation in machine-generated text.

\section{Datasets}
\label{sec:datasets}
The datasets used in this study are derived from the SemEval 2025 Task 2 on Entity-Aware Machine Translation.
All sets, except for the blind test set, included English source texts and their translations in ten 
different languages (Italian, Spanish, French, German, Arabic, Japanese, Chinese, Korean, Thai, and Turkish). 
Each data entry typically featured an English sentence, at least one translation in a target language, 
and a relevant Wikidata ID. 
For example, an English query like "What year was The Great Gatsby published?" 
would be linked to its Korean translation and the Wikidata ID Q214371. We also referred the mintaka\cite{sen-etal-2022-mintaka} dataset,
high-quality knowledge base of Wikidata facts, primarily used for question answering. It can be used to create
datasets for fine-tuning Named Entity Recognition (NER) models.
\begin{table}[h!]
\centering
\begin{tabular}{ll@{\hspace{8mm}}ll}
\hline
\textbf{Language} & \textbf{Train} & \textbf{Valid} & \textbf{Test} \\
\hline
Italian & 3,739 & 730 & 5,097 \\
Spanish & 5,160 & 739 & 5,337 \\
French & 5,531 & 724 & 5,464 \\
German & 4,087 & 731 & 5,875 \\
Arabic & 7,220 & 722 & 4,546 \\
Japanese & 7,225 & 723 & 5,107 \\
Chinese & - & 722 & 5,181 \\
Korean & - & 745 & 5,081 \\
Thai & - & 710 & 3,446 \\
Turkish & - & 732 & 4,472 \\
\hline
\textbf{Total} & \textbf{32,962} & \textbf{7,278} & \textbf{49,606} \\
\hline
\end{tabular}
\caption{Dataset distribution across languages}
\label{tab:dataset_distribution}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:methodology}
Describe the methods you have used in your study. You can have subsections for different models.

\subsection{Closed-Source Models}
Detail the prompt engineering techniques you used for Google's Gemini and OpenAI's ChatGPT. Include examples of your prompts if possible.

\subsection{Open-Weight Models}
Explain the fine-tuning procedures for Gemma, Llama, and Qwen. Mention the datasets used, the fine-tuning parameters, and any prompt engineering strategies applied.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Experiments and Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments and Results}
\label{sec:experiments}
Present the experiments you conducted. This section should be detailed and clearly explain how you evaluated the models.

\subsection{Evaluation Metrics}
Briefly explain the COMET and M-ETA metrics.
\begin{itemize}
    \item \textbf{COMET (Crosslingual Optimized Metric for Evaluation of Translation):} Rei et al. (2020).\cite{conia-etal-2024-m-eta}
    \item \textbf{M-ETA (Manual Entity Translation Accuracy):} Conia et al. (2024).\cite{rei-etal-2020-comet}
\end{itemize}

\subsection{Results}
Present your results in tables and figures for clarity. For example:

\begin{table}[h]
\centering
\label{tab:comet_scores}
\begin{tabular}{ll@{\hspace{8mm}}ll}
\hline
\textbf{Model} & \textbf{Language 1} & \textbf{Language 2} & \textbf{Language 3} \\
\hline
Gemini         & Score 1a            & Score 1b            & Score 1c            \\
ChatGPT        & Score 2a            & Score 2b            & Score 2c            \\
Gemma (fine-tuned) & Score 3a            & Score 3b            & Score 3c            \\
Llama (fine-tuned) & Score 4a            & Score 4b            & Score 4c            \\
Qwen (fine-tuned)  & Score 5a            & Score 5b            & Score 5c            \\
\hline
\end{tabular}
\caption{COMET scores for different models and languages.}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Analysis and Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis and Discussion}
\label{sec:analysis}
Provide a detailed comparative analysis of your results. Discuss the strengths and weaknesses of each model and approach. Why did some models perform better than others? What are the implications of your findings?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}
Summarize your key findings and contributions. You can also discuss the limitations of your study and suggest directions for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    ACKNOWLEDGEMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{ack}
We would like to thank... % (optional)
\end{ack}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    BIBLIOGRAPHY SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%