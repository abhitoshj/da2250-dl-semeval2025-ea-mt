%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    INSTANTIATE YOUR DOCUMENT CLASS WITH THE ECAI OPTION.
%    For final camera-ready copy, replace 'report' with 'final'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{ecai} 
%\documentclass[doubleblind]{ecai} 
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{multirow}


\begin{document}

\begin{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    TITLE SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Team21 - Entity Matters: A Comparative Study of Machine Translation Fidelity in Large Language Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    AUTHOR SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{\fnms{Aastik}~\snm{Shrivastava}}
\author{\fnms{Abhitosh}}
\author{\fnms{Amit}~\snm{Nitin Joshi}}
\author{\fnms{Ayush}~\snm{Ravindra Jha}}
\author{\fnms{Raghavendra}~\snm{Naik}}
\author{\fnms{Sibashis}~\snm{Kumar Sahu}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    ABSTRACT SECTION

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Translation of named entities is challenging for traditional machine translation systems, as there may be cultural 
or domain-specific references that may not be easily translated. 
This impacts the effectiveness of such systems in real-world scenarios. 
We draw inspiration from the SemEval 2025 Task 2 on Entity-Aware Machine Translation for this paper. 
The task was to translate an input sentence containing named entities from English to multiple target languages. 
In this paper, we attempted the task using various techniques and present our findings. 
We used prompt engineering to evaluate some closed-source models like Google's Gemini and OpenAI's ChatGPT. 
For open-sourced models such as Gemma, Llama and Mistral, we use prompt engineering followed by fine-tuning
to asses and improve their performance on the task.
We used Crosslingual Optimized Metric for Evaluation of Translation (COMET; Rei et al., 2020)\cite{conia-etal-2024-m-eta}
and Manual Entity Translation Accuracy (M-ETA; Conia et al., 2024)\cite{rei-etal-2020-comet} 
as metrics for evaluation of the quality of translation generated by these systems.
\end{abstract}
\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    MAIN BODY OF THE PAPER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
The accurate and contextually appropriate translation of named entities remain a formidable challenge
for conventional machine translation systems, as these specific linguistic elements often lack direct
lexical equivalents in target languages. This necessitates a sophisticated comprehension of their real-world
referents and inherent semantic roles. This persistent difficult underscores the critical importance of Entity-Aware
Machine Translation (EAMT). EAMT represents a significant advancement, empowering machine translation systems to actively
recognize, classify and leverage an entity's specific type and attribute. By integrating this nuanced entity intelligence, 
EAMT can effectively resolve ambiguities, ensure consistent rendering across diverse linguistic contexts, 
and generate translations that are not only grammatically sound but also factually precise, 
thereby critically enhancing the overall fidelity and trustworthiness of machine-generated content
 in sensitive and high-stakes applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related}
Our work builds upon several key advancements in machine translation and large language models. 
The field of machine translation was revolutionized by the introduction of Neural Machine Translation (NMT), 
particularly with the advent of the Transformer architecture \cite{vaswani2017attention}, 
which replaced recurrent and convolutional models with a purely attention-based mechanism, 
setting a new standard for performance.
More recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in translation, 
often in a zero-shot or few-shot setting \cite{brown2020language}. 
Models pretrained on vast text corpora using a unified text-to-text framework, 
such as T5 \cite{raffel2020exploring}, have shown that a single model can be effectively prompted 
to perform a wide range of NLP tasks, including high-quality translation without task-specific fine-tuning.
A significant challenge in machine translation is the correct rendering of named entities,
which often carry critical semantic weight. This has been a long-standing area of research, 
with various approaches proposed to make NMT systems more "entity-aware." 
Early work focused on integrating external knowledge or explicitly marking entities to guide the translation process. 
These methods aim to improve translation fidelity by preventing entities from being misinterpreted 
as common words or being transliterated incorrectly.
To further enhance the contextual understanding of LLMs for specialized tasks, 
Retrieval-Augmented Generation (RAG) has emerged as a powerful technique \cite{lewis2020retrieval}. 
By retrieving relevant documents or knowledge snippets and providing them as context to the generator model, 
RAG has been successfully applied to knowledge-intensive NLP tasks. Its application to machine translation is 
particularly promising for handling domain-specific terminology and entities, 
as it allows the model to ground its translation in factual, retrieved data
a principle that underpins our RAG-based experiments with Wikidata. 
Finally, for evaluation, we rely on established metrics like COMET \cite{rei-etal-2020-comet}, 
a reference-based metric that leverages cross-lingual pretrained models to achieve high correlation
with human judgments of translation quality, and M-ETA,
which specifically measures the accuracy of entity translation in machine-generated text.

\section{Datasets}
\label{sec:datasets}

\begin{table}[h!]
\centering
\begin{tabular}{ll@{\hspace{8mm}}ll}
\hline
\textbf{Language} & \textbf{Train} & \textbf{Valid} & \textbf{Test} \\
\hline
Italian & 3,739 & 730 & 5,097 \\
Spanish & 5,160 & 739 & 5,337 \\
French & 5,531 & 724 & 5,464 \\
German & 4,087 & 731 & 5,875 \\
Arabic & 7,220 & 722 & 4,546 \\
Japanese & 7,225 & 723 & 5,107 \\
Chinese & - & 722 & 5,181 \\
Korean & - & 745 & 5,081 \\
Thai & - & 710 & 3,446 \\
Turkish & - & 732 & 4,472 \\
\hline
\textbf{Total} & \textbf{32,962} & \textbf{7,278} & \textbf{49,606} \\
\hline
\end{tabular}
\caption{Dataset distribution across languages}
\label{tab:dataset_distribution}
\end{table}

The datasets used in this study are derived from the SemEval 2025 Task 2 on Entity-Aware Machine Translation.
All sets, except for the blind test set, included English source texts and their translations in ten 
different languages (Italian, Spanish, French, German, Arabic, Japanese, Chinese, Korean, Thai, and Turkish). 
Each data entry typically featured an English sentence, at least one translation in a target language, 
and a relevant Wikidata ID. 
For example, an English query like "What year was The Great Gatsby published?" 
would be linked to its Korean translation and the Wikidata ID Q214371. We also referred the mintaka\cite{sen-etal-2022-mintaka} dataset,
high-quality knowledge base of Wikidata facts, primarily used for question answering. It can be used to create
datasets for fine-tuning Named Entity Recognition (NER) models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:methodology}
Our experimental methodology evaluated open-weight and closed-source Large Language Models (LLMs) on the SemEval 2025 Task 2 Entity-Aware Machine Translation (EAMT) task. 
We explored direct translation through various prompting strategies and enhanced these with Retrieval-Augmented Generation (RAG)-inspired pipelines, focusing on robust named entity handling.

For direct translation, we established a baseline for open-weight models (e.g., Gemma, Llama, Mistral) and closed-source models (Google's Gemini-2.0-Flash and OpenAI's GPT-4o). 
\textbf{Zero-shot Translation} evaluated baseline fluency, often struggling with named entity recognition. \textbf{Few-shot Translation} used limited in-context examples to guide models, 
showing modest improvements despite prompt complexity challenges. \textbf{Chain-of-Thought (CoT) Prompting} was applied to closed-source models, instructing intermediate reasoning 
steps to enhance logical processing and entity handling. Prompt templates varied to include direct translation, prepended examples for few-shot, and reasoning guidance for CoT.

To mitigate named entity translation challenges, we implemented RAG-inspired pipelines. For open-weight models, this involved an LLM-based zero-shot NER approach 
(or spaCy for non-prompt-amenable models like NLLB), followed by prompt-based verification, deduplication, and Wikidata linking for accurate translations.
Key enhancements like \textbf{Entity Merging and Type Filtering} improved lookup precision and reduced ambiguity. Translated entities augmented 
the final translation prompt, incorporating a retry mechanism for fidelity. With Gemini-2.0-Flash, a similar spaCy-based RAG-guided translation approach 
was successfully attempted. This multi-faceted methodology enabled comprehensive evaluation of both direct translation and external knowledge 
integration's impact on EAMT across contemporary LLMs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
\label{sec:experiments}
Our system evaluations were conducted using the metrics specified for the shared task: COMET 
(Crosslingual Optimized Metric for Evaluation of Translation)\cite{rei-etal-2020-comet} and 
Manual Entity Translation Accuracy (M-ETA). 
COMET is a machine translation evaluation metric that utilizes a pre-trained model to derive 
quality scores by comparing machine-generated outputs against human reference translations. 
M-ETA specifically quantifies the precision of entity translations within machine-translated text, 
calculated as the ratio of correctly translated entities against a gold standard.
To provide a comprehensive assessment, the overall performance score, presented in Equation \ref{eq:overall_score}, 
is computed as the harmonic mean of the COMET and M-ETA metrics. This combined metric ensures that systems are 
evaluated not only on their general translation quality but also on their ability to accurately translate named entities.
\begin{equation}
\label{eq:overall_score}
\text{Overall Score} = 2 \times \frac{\text{COMET} \times \text{M-ETA}}{\text{COMET} + \text{M-ETA}}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{eq:results}

\begin{table}[h!]
\centering
\small % Using small font size for compactness
\label{tab:combined_methods_avg_scores}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method:Model-Attempt} & \textbf{M-ETA} & \textbf{COMET} & \textbf{Overall Score} \\
\hline
RAG:gemma3\_4b & 66.52 & 91.12 & 76.79 \\
RAG:gemini-2.0-flash & 59.99 & 90.62 & 72.16 \\
RAG:fb\_nllb\_200\_3.3b & 48.75 & 89.08 & 62.51 \\
RAG:fb\_nllb\_200\_3.3b & 48.95 & 89.12 & 62.79 \\
RAG:mistral7b & 54.68 & 82.12 & 65.42 \\
RAG:mistral7b & 12.63 & 76.27 & 20.01 \\
\hline
ZS:gemini-2.0-flash-0 & 46.57 & 91.10 & 61.49 \\
ZS:gemini-2.0-flash-1 & 45.58 & 90.37 & 60.45 \\
ZS:fb\_nllb\_200\_3.3b & 23.66 & 88.19 & 35.47 \\
ZS:gpt-4o-0 & 39.67 & 90.93 & 54.55 \\
ZS:gpt-4o-1 & 37.33 & 90.35 & 52.11 \\
ZS:gemma3\_4b & 21.02 & 87.85 & 33.10 \\
ZS:llama3.1\_8b-0 & 17.70 & 83.99 & 28.16 \\
ZS:llama3.1\_8b-1 & 16.44 & 79.07 & 26.09 \\
ZS:mistral7b & 12.88 & 76.19 & 20.31 \\
\hline
FS:gemini-2.0-flash & 46.45 & 90.91 & 61.38 \\
FS:gpt-4o & 22.87 & 61.48 & 30.34 \\
FS:llama3.1\_8b-0 & 17.59 & 79.88 & 27.76 \\
FS:llama3.1\_8b-1 & 14.41 & 76.73 & 22.71 \\
FS:llama3.1\_8b-2 & 14.52 & 75.79 & 23.25 \\
\hline
CoT:gemini-2.0-flash & 43.55 & 90.65 & 58.70 \\
CoT:gpt-4o & 39.58 & 90.73 & 54.40 \\
\hline
\end{tabular}
\caption{Average M-ETA, COMET and Overall (Harmonic Mean) scores across languages for various approaches and models.}
\end{table}

Among all evaluated approaches, Retrieval-Augmented Generation (RAG) with models like Gemma 3B 
and Gemini 2.0 Flash demonstrates the most effective performance for the Entity-Aware Machine Translation (EA-MT) task. 
These configurations consistently outperform zero-shot, few-shot, and chain-of-thought baselines in terms of both adequacy
and fluency. The strength of RAG lies in its ability to leverage retrieved external knowledge, allowing models to 
better handle the translation of named entities—particularly those requiring disambiguation or that lack sufficient 
in-context information. This grounding is crucial for EA-MT, where preserving entity fidelity is a core requirement.

In contrast, zero-shot and few-shot prompting with large models such as Gemini 2.0 Flash, GPT-4o, 
and Facebook's NLLB 3 perform moderately well but show reduced reliability, especially when translating unseen or 
ambiguous entities. LLaMA 3.1 8B and Mistral 7B underperform across setups, indicating limited capability without retrieval 
support.

Chain-of-thought (CoT) prompting with Gemini 2.0 Flash and GPT-4o yields slight gains in adequacy but does not 
close the gap with RAG-based systems. Additionally, enhanced prompting strategies (e.g., variations in prompt design) 
do not consistently improve performance and occasionally lead to regressions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Future Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}
\label{sec:future_work}
While RAG-based approaches with models like \textit{Gemma 3B} and \textit{Gemini 2.0 Flash} have shown strong performance, 
several directions can further enhance EA-MT systems. One promising path is fine-tuning models specifically for 
entity-aware translation, using annotated corpora or synthetic data that emphasize named entity handling. 
Techniques such as instruction tuning, LoRA-based adaptation, or reinforcement learning with entity-focused rewards 
can improve entity fidelity without requiring full retraining. 
Additionally, knowledge distillation from large models (e.g., \textit{Gemini} or \textit{GPT-4o}) into smaller, 
more efficient models offers a scalable way to preserve entity translation quality. On the retrieval side,
enhancements such as context-aware or cross-lingual retrieval—particularly those focused on entity-rich segments 
could improve grounding and reduce ambiguity. Incorporating entity tagging or canonicalization before translation, 
followed by post-generation substitution, may further reduce hallucinations. 
Finally, moving beyond adequacy and fluency, future work should adopt entity-aware evaluation metrics that explicitly 
assess correctness and faithfulness in named entity translation.
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}
In this work, we evaluated a range of strategies for the Entity-Aware Machine Translation (EA-MT) 
task as part of SemEval 2025 Task 2. Our experiments demonstrate that Retrieval-Augmented 
Generation (RAG) methods, particularly those using high-capacity models like \textit{Gemma 3B} 
and \textit{Gemini 2.0 Flash}, substantially outperform zero-shot, few-shot, and chain-of-thought 
prompting approaches. RAG systems effectively leverage external knowledge to improve adequacy and 
fluency, especially in the translation of named entities.
We also observed that enhanced prompts and reasoning-based strategies, while helpful in certain 
cases, fall short of the gains achieved through retrieval. Our findings highlight the importance
of grounding language models in entity-relevant context to improve translation quality in 
multilingual settings.
This analysis establishes a strong foundation for further advancements in EA-MT. 
Future directions include targeted fine-tuning, knowledge distillation, improved retrieval 
strategies, and the development of entity-aware evaluation metrics. By integrating these 
improvements, we aim to build more accurate, robust, and deployable translation systems that 
are sensitive to entity correctness and cross-lingual consistency.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    BIBLIOGRAPHY SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%