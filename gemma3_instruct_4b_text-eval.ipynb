{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Change to your desired working directory\n",
        "os.chdir('/content/drive/MyDrive/IISc/DL')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdofEcD01dw_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Io35bD-J1qL2"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqHd4WvN8ofP",
        "outputId": "95f2310f-0fa8-4efd-95bb-3285d15fda1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/876.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m809.0/876.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.5/876.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-hub\n",
        "!pip install -q -U keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-3XQFEnd1toO"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkGLe2bk1y0w",
        "outputId": "d5658a8d-2238-48b2-c4c7-c8d129e49b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/config.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 968/968 [00:00<00:00, 2.47MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/task.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3.23k/3.23k [00:00<00:00, 6.45MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/assets/tokenizer/vocabulary.spm...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.47M/4.47M [00:00<00:00, 10.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/model.weights.h5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7.23G/7.23G [02:24<00:00, 53.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_4b_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw17QZyhsj4L",
        "outputId": "e84e1d09-55d3-4f78-e383-c1475bd4cbee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  8%|▊         | 60/722 [00:35<06:28,  1.70it/s]\n",
            "\n",
            "  3%|▎         | 20/724 [00:48<28:16,  2.41s/it]\u001b[A\n",
            "  6%|▌         | 40/724 [00:54<13:16,  1.16s/it]\u001b[A\n",
            "  8%|▊         | 60/724 [01:00<08:43,  1.27it/s]\u001b[A\n",
            " 11%|█         | 80/724 [01:06<06:13,  1.72it/s]\u001b[A\n",
            " 14%|█▍        | 100/724 [01:12<05:00,  2.08it/s]\u001b[A\n",
            " 17%|█▋        | 120/724 [01:17<04:08,  2.43it/s]\u001b[A\n",
            " 19%|█▉        | 140/724 [01:25<03:51,  2.52it/s]\u001b[A\n",
            " 22%|██▏       | 160/724 [01:31<03:31,  2.67it/s]\u001b[A\n",
            " 25%|██▍       | 180/724 [01:38<03:17,  2.76it/s]\u001b[A\n",
            " 28%|██▊       | 200/724 [01:45<03:06,  2.81it/s]\u001b[A\n",
            " 30%|███       | 220/724 [01:50<02:49,  2.97it/s]\u001b[A\n",
            " 33%|███▎      | 240/724 [01:56<02:33,  3.15it/s]\u001b[A\n",
            " 36%|███▌      | 260/724 [02:02<02:22,  3.25it/s]\u001b[A\n",
            " 39%|███▊      | 280/724 [02:07<02:12,  3.34it/s]\u001b[A\n",
            " 41%|████▏     | 300/724 [02:13<02:02,  3.45it/s]\u001b[A\n",
            " 44%|████▍     | 320/724 [02:19<02:01,  3.33it/s]\u001b[A\n",
            " 47%|████▋     | 340/724 [02:25<01:56,  3.28it/s]\u001b[A\n",
            " 50%|████▉     | 360/724 [02:32<01:54,  3.17it/s]\u001b[A\n",
            " 52%|█████▏    | 380/724 [02:40<01:55,  2.98it/s]\u001b[A\n",
            " 55%|█████▌    | 400/724 [02:45<01:42,  3.17it/s]\u001b[A\n",
            " 58%|█████▊    | 420/724 [02:52<01:36,  3.15it/s]\u001b[A\n",
            " 61%|██████    | 440/724 [02:57<01:23,  3.39it/s]\u001b[A\n",
            " 64%|██████▎   | 460/724 [03:02<01:17,  3.42it/s]\u001b[A\n",
            " 66%|██████▋   | 480/724 [03:08<01:09,  3.50it/s]\u001b[A\n",
            " 69%|██████▉   | 500/724 [03:14<01:06,  3.38it/s]\u001b[A\n",
            " 72%|███████▏  | 520/724 [03:21<01:02,  3.24it/s]\u001b[A\n",
            " 75%|███████▍  | 540/724 [03:28<00:58,  3.16it/s]\u001b[A\n",
            " 77%|███████▋  | 560/724 [03:34<00:51,  3.20it/s]\u001b[A\n",
            " 80%|████████  | 580/724 [03:39<00:43,  3.28it/s]\u001b[A\n",
            " 83%|████████▎ | 600/724 [03:45<00:36,  3.42it/s]\u001b[A\n",
            " 86%|████████▌ | 620/724 [03:50<00:30,  3.42it/s]\u001b[A\n",
            " 88%|████████▊ | 640/724 [03:56<00:23,  3.50it/s]\u001b[A\n",
            " 91%|█████████ | 660/724 [04:01<00:18,  3.52it/s]\u001b[A\n",
            " 94%|█████████▍| 680/724 [04:07<00:12,  3.57it/s]\u001b[A\n",
            " 97%|█████████▋| 700/724 [04:13<00:07,  3.39it/s]\u001b[A\n",
            " 99%|█████████▉| 720/724 [04:20<00:01,  3.23it/s]\u001b[A\n",
            "100%|██████████| 724/724 [04:23<00:00,  2.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/fr_FR.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 722/722 [04:05<00:00,  2.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/zh_TW.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [03:53<00:00,  3.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/th_TH.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 745/745 [04:26<00:00,  2.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/ko_KR.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 723/723 [04:20<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/ja_JP.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 730/730 [03:33<00:00,  3.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/it_IT.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 732/732 [04:14<00:00,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/tr_TR.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 722/722 [03:42<00:00,  3.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/ar_AE.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 731/731 [04:11<00:00,  2.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/de_DE.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [04:11<00:00,  2.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/es_ES.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import json\n",
        "import glob\n",
        "import sys\n",
        "import tqdm\n",
        "import torch\n",
        "import os\n",
        "\n",
        "prompt_template = '''<start_of_turn>user\n",
        "    Translate the following sentence to {language}.\n",
        "    Text: {text}\n",
        "    Only output the translated text.\n",
        "    Do not include any additional text or explanations.<end_of_turn>\n",
        "    <start_of_turn>model\n",
        "    '''\n",
        "\n",
        "# Language mapping function\n",
        "def get_language_name(short_code):\n",
        "    lang_map = {\n",
        "        'ar': 'Arabic',\n",
        "        'zh': 'Chinese (Traditional)',\n",
        "        'fr': 'French',\n",
        "        'de': 'German',\n",
        "        'it': 'Italian',\n",
        "        'ja': 'Japanese',\n",
        "        'ko': 'Korean',\n",
        "        'es': 'Spanish',\n",
        "        'th': 'Thai',\n",
        "        'tr': 'Turkish',\n",
        "        'en': 'English',\n",
        "        # Add more as needed\n",
        "    }\n",
        "    return lang_map.get(short_code, short_code)\n",
        "\n",
        "# File processing setup\n",
        "input_data_folder = \"./data/references/validation/\"\n",
        "jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n",
        "model_name = \"gemma3_4b-text-keras\"\n",
        "output_prediction_dir = os.path.join(\"./data/predictions\", model_name, \"validation\")\n",
        "os.makedirs(output_prediction_dir, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 20  # Define your batch size\n",
        "\n",
        "for file_path in jsonl_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    outfile_path = os.path.join(output_prediction_dir, filename)\n",
        "\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "    results = []\n",
        "    pbar = tqdm.tqdm(total=len(data))\n",
        "\n",
        "    # Process data in batches\n",
        "    for i in range(0, len(data), BATCH_SIZE):\n",
        "        batch_data = data[i:i + BATCH_SIZE]\n",
        "        batch_prompts = []\n",
        "        batch_records = []\n",
        "\n",
        "        for record in batch_data:\n",
        "            source = record['source']\n",
        "            target_locale = record['target_locale']\n",
        "            target_language = get_language_name(target_locale)\n",
        "            prompt = prompt_template.format(text=source, language=target_language)\n",
        "            batch_prompts.append(prompt)\n",
        "            batch_records.append(record) # Keep track of original records for result mapping\n",
        "\n",
        "        # Generate translations for the batch\n",
        "        # print(f\"Processing batch starting at index {i}...\")\n",
        "\n",
        "        batch_outputs = gemma_lm.generate(batch_prompts, max_length=300)\n",
        "\n",
        "        # Process each output in the batch\n",
        "        for j, output in enumerate(batch_outputs):\n",
        "            record = batch_records[j] # Get the corresponding record for this output\n",
        "            id = record['id']\n",
        "            source = record['source']\n",
        "            source_locale = record['source_locale']\n",
        "            source_language = get_language_name(source_locale)\n",
        "            target_locale = record['target_locale']\n",
        "            target_language = get_language_name(target_locale)\n",
        "\n",
        "            translated = output.strip()\n",
        "\n",
        "            start_tag = \"<start_of_turn>model\\n\"\n",
        "            end_tag = \"<end_of_turn>\"\n",
        "\n",
        "            start_index = output.find(start_tag)\n",
        "            end_index = output.find(end_tag, start_index + len(start_tag))\n",
        "\n",
        "            if start_index != -1 and end_index != -1:\n",
        "                extracted_text = output[start_index + len(start_tag):end_index]\n",
        "            else:\n",
        "                print(f\"Tags not found in the {output}.\")\n",
        "\n",
        "            translated = extracted_text.strip()\n",
        "            #print(translated)\n",
        "\n",
        "            results.append({\n",
        "                \"id\": id,\n",
        "                \"source_language\": source_language,\n",
        "                \"target_language\": target_language,\n",
        "                \"text\": source,\n",
        "                \"prediction\": translated,\n",
        "        })\n",
        "\n",
        "        pbar.update(len(batch_data))\n",
        "\n",
        "        # Periodic writing\n",
        "        with open(outfile_path, 'w', encoding='utf-8') as f:\n",
        "            for res in results:\n",
        "                f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    pbar.close()\n",
        "    print(f\"Translations saved to {outfile_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/raghavendra/da2250-dl-semeval2025-ea-mt/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/raghavendra/da2250-dl-semeval2025-ea-mt/.venv/lib/python3.12/site-packages/torchmetrics/utilities/imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import DistributionNotFound, get_distribution\n",
            "2025-06-18 13:43:22.292343: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-18 13:43:22.786106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750254202.955441    3229 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750254203.004781    3229 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1750254203.406778    3229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750254203.406822    3229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750254203.406824    3229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750254203.406825    3229 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-06-18 13:43:23.455940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 58254.22it/s]\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../../../../home/raghavendra/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
            "Encoder model frozen.\n",
            "/home/raghavendra/da2250-dl-semeval2025-ea-mt/.venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
          ]
        }
      ],
      "source": [
        "from framework import download_comet_model\n",
        "comet_model = download_comet_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "from framework import calculate_comet_scores, calculate_meta_score\n",
        "\n",
        "model_name = \"gemma3_instruct_4b_text\"\n",
        "output_prediction_dir = os.path.join(\"data/predictions\", model_name, \"validation\")\n",
        "os.makedirs(output_prediction_dir, exist_ok=True)\n",
        "\n",
        "input_data_folder = \"data/references/validation\"\n",
        "jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n",
        "\n",
        "def calculate_scores(template_id):\n",
        "    scores_dir = os.path.join(output_prediction_dir, template_id, \"scores\")\n",
        "\n",
        "    if not os.path.exists(scores_dir):\n",
        "        os.makedirs(scores_dir, exist_ok=True)\n",
        "\n",
        "    for file_path in jsonl_files:\n",
        "        references_path = file_path\n",
        "        filename = os.path.basename(file_path)\n",
        "        predictions_path = os.path.join(output_prediction_dir, template_id, filename)\n",
        "\n",
        "        comet_score = calculate_comet_scores(\n",
        "            comet_model, \n",
        "            references_path, \n",
        "            predictions_path\n",
        "        )\n",
        "\n",
        "        correct_instances, total_instances, meta_score = calculate_meta_score(\n",
        "            references_path,\n",
        "            predictions_path)\n",
        "\n",
        "        evaluation_results = {\n",
        "            \"correct_instances\": correct_instances,\n",
        "            \"total_instances\": total_instances,\n",
        "            \"comet_score\": comet_score,\n",
        "            \"meta_score\": meta_score\n",
        "        }\n",
        "\n",
        "        evaluation_output_path = os.path.join(scores_dir, f\"{os.path.splitext(filename)[0]}.json\")\n",
        "        with open(evaluation_output_path, 'w', encoding='utf-8') as json_file:\n",
        "            json.dump(evaluation_results, json_file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All references have a corresponding prediction\n",
            "Created 1177 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 37/37 [00:15<00:00,  2.42it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 87.07\n",
            "Loaded 722 instances.\n",
            "Loaded 722 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1260 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 40/40 [00:17<00:00,  2.27it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 87.17\n",
            "Loaded 731 instances.\n",
            "Loaded 731 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1229 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 39/39 [00:17<00:00,  2.23it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 91.17\n",
            "Loaded 739 instances.\n",
            "Loaded 739 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1316 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 42/42 [00:20<00:00,  2.08it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 87.54\n",
            "Loaded 724 instances.\n",
            "Loaded 724 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1268 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 40/40 [00:17<00:00,  2.29it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 89.00\n",
            "Loaded 730 instances.\n",
            "Loaded 730 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1409 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 45/45 [00:21<00:00,  2.05it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 89.06\n",
            "Loaded 723 instances.\n",
            "Loaded 723 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1660 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 52/52 [00:23<00:00,  2.22it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 89.77\n",
            "Loaded 745 instances.\n",
            "Loaded 745 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1654 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 52/52 [00:25<00:00,  2.03it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 80.77\n",
            "Loaded 710 instances.\n",
            "Loaded 710 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1260 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 40/40 [00:16<00:00,  2.41it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 88.83\n",
            "Loaded 732 instances.\n",
            "Loaded 732 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1544 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 49/49 [00:20<00:00,  2.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 88.09\n",
            "Loaded 722 instances.\n",
            "Loaded 722 predictions.\n"
          ]
        }
      ],
      "source": [
        "calculate_scores(\"zero-shot\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
