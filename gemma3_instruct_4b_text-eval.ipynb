{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Change to your desired working directory\n",
        "os.chdir('/content/drive/MyDrive/IISc/DL')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdofEcD01dw_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Io35bD-J1qL2"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqHd4WvN8ofP",
        "outputId": "95f2310f-0fa8-4efd-95bb-3285d15fda1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/876.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m809.0/876.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.5/876.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-hub\n",
        "!pip install -q -U keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-3XQFEnd1toO"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkGLe2bk1y0w",
        "outputId": "d5658a8d-2238-48b2-c4c7-c8d129e49b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/config.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 968/968 [00:00<00:00, 2.47MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/task.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3.23k/3.23k [00:00<00:00, 6.45MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/assets/tokenizer/vocabulary.spm...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.47M/4.47M [00:00<00:00, 10.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/model.weights.h5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7.23G/7.23G [02:24<00:00, 53.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_4b_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw17QZyhsj4L",
        "outputId": "e84e1d09-55d3-4f78-e383-c1475bd4cbee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  8%|▊         | 60/722 [00:35<06:28,  1.70it/s]\n",
            "\n",
            "  3%|▎         | 20/724 [00:48<28:16,  2.41s/it]\u001b[A\n",
            "  6%|▌         | 40/724 [00:54<13:16,  1.16s/it]\u001b[A\n",
            "  8%|▊         | 60/724 [01:00<08:43,  1.27it/s]\u001b[A\n",
            " 11%|█         | 80/724 [01:06<06:13,  1.72it/s]\u001b[A\n",
            " 14%|█▍        | 100/724 [01:12<05:00,  2.08it/s]\u001b[A\n",
            " 17%|█▋        | 120/724 [01:17<04:08,  2.43it/s]\u001b[A\n",
            " 19%|█▉        | 140/724 [01:25<03:51,  2.52it/s]\u001b[A\n",
            " 22%|██▏       | 160/724 [01:31<03:31,  2.67it/s]\u001b[A\n",
            " 25%|██▍       | 180/724 [01:38<03:17,  2.76it/s]\u001b[A\n",
            " 28%|██▊       | 200/724 [01:45<03:06,  2.81it/s]\u001b[A\n",
            " 30%|███       | 220/724 [01:50<02:49,  2.97it/s]\u001b[A\n",
            " 33%|███▎      | 240/724 [01:56<02:33,  3.15it/s]\u001b[A\n",
            " 36%|███▌      | 260/724 [02:02<02:22,  3.25it/s]\u001b[A\n",
            " 39%|███▊      | 280/724 [02:07<02:12,  3.34it/s]\u001b[A\n",
            " 41%|████▏     | 300/724 [02:13<02:02,  3.45it/s]\u001b[A\n",
            " 44%|████▍     | 320/724 [02:19<02:01,  3.33it/s]\u001b[A\n",
            " 47%|████▋     | 340/724 [02:25<01:56,  3.28it/s]\u001b[A\n",
            " 50%|████▉     | 360/724 [02:32<01:54,  3.17it/s]\u001b[A\n",
            " 52%|█████▏    | 380/724 [02:40<01:55,  2.98it/s]\u001b[A\n",
            " 55%|█████▌    | 400/724 [02:45<01:42,  3.17it/s]\u001b[A\n",
            " 58%|█████▊    | 420/724 [02:52<01:36,  3.15it/s]\u001b[A\n",
            " 61%|██████    | 440/724 [02:57<01:23,  3.39it/s]\u001b[A\n",
            " 64%|██████▎   | 460/724 [03:02<01:17,  3.42it/s]\u001b[A\n",
            " 66%|██████▋   | 480/724 [03:08<01:09,  3.50it/s]\u001b[A\n",
            " 69%|██████▉   | 500/724 [03:14<01:06,  3.38it/s]\u001b[A\n",
            " 72%|███████▏  | 520/724 [03:21<01:02,  3.24it/s]\u001b[A\n",
            " 75%|███████▍  | 540/724 [03:28<00:58,  3.16it/s]\u001b[A\n",
            " 77%|███████▋  | 560/724 [03:34<00:51,  3.20it/s]\u001b[A\n",
            " 80%|████████  | 580/724 [03:39<00:43,  3.28it/s]\u001b[A\n",
            " 83%|████████▎ | 600/724 [03:45<00:36,  3.42it/s]\u001b[A\n",
            " 86%|████████▌ | 620/724 [03:50<00:30,  3.42it/s]\u001b[A\n",
            " 88%|████████▊ | 640/724 [03:56<00:23,  3.50it/s]\u001b[A\n",
            " 91%|█████████ | 660/724 [04:01<00:18,  3.52it/s]\u001b[A\n",
            " 94%|█████████▍| 680/724 [04:07<00:12,  3.57it/s]\u001b[A\n",
            " 97%|█████████▋| 700/724 [04:13<00:07,  3.39it/s]\u001b[A\n",
            " 99%|█████████▉| 720/724 [04:20<00:01,  3.23it/s]\u001b[A\n",
            "100%|██████████| 724/724 [04:23<00:00,  2.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/fr_FR.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 722/722 [04:05<00:00,  2.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/zh_TW.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [03:53<00:00,  3.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/th_TH.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 745/745 [04:26<00:00,  2.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/ko_KR.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 723/723 [04:20<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/ja_JP.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 730/730 [03:33<00:00,  3.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/it_IT.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 732/732 [04:14<00:00,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/tr_TR.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 722/722 [03:42<00:00,  3.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/ar_AE.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 731/731 [04:11<00:00,  2.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/de_DE.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 739/739 [04:11<00:00,  2.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translations saved to /content/drive/MyDrive/IISc/DL/data/predictions/gemma3_4b-text-keras/validation/es_ES.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import json\n",
        "import glob\n",
        "import sys\n",
        "import tqdm\n",
        "import torch\n",
        "import os\n",
        "\n",
        "prompt_template = '''<start_of_turn>user\n",
        "    Translate the following sentence to {language}.\n",
        "    Text: {text}\n",
        "    Only output the translated text.\n",
        "    Do not include any additional text or explanations.<end_of_turn>\n",
        "    <start_of_turn>model\n",
        "    '''\n",
        "\n",
        "# Language mapping function\n",
        "def get_language_name(short_code):\n",
        "    lang_map = {\n",
        "        'ar': 'Arabic',\n",
        "        'zh': 'Chinese (Traditional)',\n",
        "        'fr': 'French',\n",
        "        'de': 'German',\n",
        "        'it': 'Italian',\n",
        "        'ja': 'Japanese',\n",
        "        'ko': 'Korean',\n",
        "        'es': 'Spanish',\n",
        "        'th': 'Thai',\n",
        "        'tr': 'Turkish',\n",
        "        'en': 'English',\n",
        "        # Add more as needed\n",
        "    }\n",
        "    return lang_map.get(short_code, short_code)\n",
        "\n",
        "# File processing setup\n",
        "input_data_folder = \"./data/references/validation/\"\n",
        "jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n",
        "model_name = \"gemma3_4b-text-keras\"\n",
        "output_prediction_dir = os.path.join(\"./data/predictions\", model_name, \"validation\")\n",
        "os.makedirs(output_prediction_dir, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 20  # Define your batch size\n",
        "\n",
        "for file_path in jsonl_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    outfile_path = os.path.join(output_prediction_dir, filename)\n",
        "\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "    results = []\n",
        "    pbar = tqdm.tqdm(total=len(data))\n",
        "\n",
        "    # Process data in batches\n",
        "    for i in range(0, len(data), BATCH_SIZE):\n",
        "        batch_data = data[i:i + BATCH_SIZE]\n",
        "        batch_prompts = []\n",
        "        batch_records = []\n",
        "\n",
        "        for record in batch_data:\n",
        "            source = record['source']\n",
        "            target_locale = record['target_locale']\n",
        "            target_language = get_language_name(target_locale)\n",
        "            prompt = prompt_template.format(text=source, language=target_language)\n",
        "            batch_prompts.append(prompt)\n",
        "            batch_records.append(record) # Keep track of original records for result mapping\n",
        "\n",
        "        # Generate translations for the batch\n",
        "        # print(f\"Processing batch starting at index {i}...\")\n",
        "\n",
        "        batch_outputs = gemma_lm.generate(batch_prompts, max_length=300)\n",
        "\n",
        "        # Process each output in the batch\n",
        "        for j, output in enumerate(batch_outputs):\n",
        "            record = batch_records[j] # Get the corresponding record for this output\n",
        "            id = record['id']\n",
        "            source = record['source']\n",
        "            source_locale = record['source_locale']\n",
        "            source_language = get_language_name(source_locale)\n",
        "            target_locale = record['target_locale']\n",
        "            target_language = get_language_name(target_locale)\n",
        "\n",
        "            translated = output.strip()\n",
        "\n",
        "            start_tag = \"<start_of_turn>model\\n\"\n",
        "            end_tag = \"<end_of_turn>\"\n",
        "\n",
        "            start_index = output.find(start_tag)\n",
        "            end_index = output.find(end_tag, start_index + len(start_tag))\n",
        "\n",
        "            if start_index != -1 and end_index != -1:\n",
        "                extracted_text = output[start_index + len(start_tag):end_index]\n",
        "            else:\n",
        "                print(f\"Tags not found in the {output}.\")\n",
        "\n",
        "            translated = extracted_text.strip()\n",
        "            #print(translated)\n",
        "\n",
        "            results.append({\n",
        "                \"id\": id,\n",
        "                \"source_language\": source_language,\n",
        "                \"target_language\": target_language,\n",
        "                \"text\": source,\n",
        "                \"prediction\": translated,\n",
        "        })\n",
        "\n",
        "        pbar.update(len(batch_data))\n",
        "\n",
        "        # Periodic writing\n",
        "        with open(outfile_path, 'w', encoding='utf-8') as f:\n",
        "            for res in results:\n",
        "                f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    pbar.close()\n",
        "    print(f\"Translations saved to {outfile_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
