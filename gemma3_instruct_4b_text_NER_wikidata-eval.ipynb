{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTS71Va5RQYW",
        "outputId": "7c73f25e-6e3a-4ec4-b835-e981bd4b377b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/IISc/DL')"
      ],
      "metadata": {
        "id": "ca_KMSCi1r8A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QdofEcD01dw_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Io35bD-J1qL2"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqHd4WvN8ofP",
        "outputId": "9f482605-fb49-4e63-a951-ad1c2dfc3bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/876.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.0/876.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.5/876.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-hub\n",
        "!pip install -q -U keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-3XQFEnd1toO"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_hub\n",
        "import requests\n",
        "from typing import List, Dict, Tuple\n",
        "import time\n",
        "import json\n",
        "import glob\n",
        "import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CkGLe2bk1y0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e597455a-75c7-40db-fc97-4bce263731cd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/config.json...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 968/968 [00:00<00:00, 1.87MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/task.json...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3.23k/3.23k [00:00<00:00, 5.39MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/assets/tokenizer/vocabulary.spm...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.47M/4.47M [00:00<00:00, 9.93MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/model.weights.h5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7.23G/7.23G [02:18<00:00, 56.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_4b_text\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_wikidata_cache(cache_path):\n",
        "    if os.path.exists(cache_path):\n",
        "        try:\n",
        "            df = pd.read_csv(cache_path)\n",
        "            cache = {(row['entity'], row['target_lang']): row['translation'] for _, row in df.iterrows()}\n",
        "            return cache\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}"
      ],
      "metadata": {
        "id": "L9GOBFhlz-PP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_wikidata_cache(cache, cache_path):\n",
        "    # cache: dict[(entity, target_lang)] -> translation\n",
        "    rows = [\n",
        "        {'entity': k[0], 'target_lang': k[1], 'translation': v}\n",
        "        for k, v in cache.items()\n",
        "    ]\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(cache_path, index=False)"
      ],
      "metadata": {
        "id": "QJ3QUlOn0Aor"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_wikidata(entity: str, target_lang: str = \"fr\", cache=None) -> str:\n",
        "    if cache is not None and (entity, target_lang) in cache:\n",
        "        return cache[(entity, target_lang)]\n",
        "    url = \"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"search\": entity,\n",
        "        \"language\": \"en\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json().get(\"search\", [])\n",
        "\n",
        "    if not results:\n",
        "        translation = entity\n",
        "    else:\n",
        "        entity_id = results[0][\"id\"]\n",
        "        label_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n",
        "        label_resp = requests.get(label_url).json()\n",
        "        try:\n",
        "            labels = label_resp[\"entities\"][entity_id][\"labels\"]\n",
        "            translation = labels[target_lang][\"value\"] if target_lang in labels else entity\n",
        "        except Exception:\n",
        "            translation = entity\n",
        "    if cache is not None:\n",
        "        cache[(entity, target_lang)] = translation\n",
        "    return translation"
      ],
      "metadata": {
        "id": "gNF7J91f0Krz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(text: str) -> List[Dict[str, str]]:\n",
        "    # Prompt for entity extraction\n",
        "    prompt_template = '''<start_of_turn>user\n",
        "    Extract all named entities from the following text.\n",
        "    For each entity, output a JSON object with keys: text, type (PER, LOC, ORG, MISC), and score (confidence 0-1).\n",
        "    Output a JSON array.\n",
        "    Do not include ```json or ``` in the output.\n",
        "    Text: {text} <end_of_turn>\n",
        "    <start_of_turn>model\n",
        "    '''\n",
        "    response = gemma_lm.generate(prompt_template.format(text=text), max_length=500)\n",
        "\n",
        "    start_tag = \"<start_of_turn>model\"\n",
        "    end_tag = \"<end_of_turn>\"\n",
        "\n",
        "    start_index = response.find(start_tag)\n",
        "    end_index = response.find(end_tag, start_index + len(start_tag))\n",
        "\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        extracted_text = response[start_index + len(start_tag):end_index]\n",
        "    else:\n",
        "        extracted_text = response.strip()\n",
        "    extracted = extracted_text.strip()\n",
        "\n",
        "    try:\n",
        "        entities = json.loads(extracted)\n",
        "    except Exception:\n",
        "        entities = []\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "Mxyx4QdH2Q1l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enrich_entities(entities: List[Dict[str, str]], target_lang: str, cache=None) -> List[Tuple[str, str]]:\n",
        "    enriched = []\n",
        "    for ent in entities:\n",
        "        translated = query_wikidata(ent[\"text\"], target_lang, cache)\n",
        "        enriched.append((ent[\"text\"], translated))\n",
        "    return enriched"
      ],
      "metadata": {
        "id": "Wsfytae-0TR1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_translation_prompt(text: str, enriched_entities: List[Tuple[str, str]], target_lang: str) -> str:\n",
        "    entity_list = \"\\n\".join([f\"{orig} → {trans}\" for orig, trans in enriched_entities if orig != trans])\n",
        "\n",
        "    prompt_template = '''<start_of_turn>user\n",
        "    Translate the following sentence to {target_lang}.\n",
        "    Use the following known entity translations:\n",
        "    {entity_list}\n",
        "\n",
        "    Text: {text}\n",
        "    Only output the translated text.\n",
        "    Do not include any additional text or explanations.<end_of_turn>\n",
        "    <start_of_turn>model'''\n",
        "\n",
        "    prompt = prompt_template.format(text=text, target_lang=target_lang, entity_list=entity_list)\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "RxySKNZe0Wiv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_with_gemma(prompt: str) -> str:\n",
        "    response = gemma_lm.generate(prompt,  max_length=500)\n",
        "\n",
        "    start_tag = \"<start_of_turn>model\"\n",
        "    end_tag = \"<end_of_turn>\"\n",
        "\n",
        "    start_index = response.find(start_tag)\n",
        "    end_index = response.find(end_tag, start_index + len(start_tag))\n",
        "\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        extracted_text = response[start_index + len(start_tag):end_index]\n",
        "    else:\n",
        "        extracted_text = response.strip()\n",
        "    extracted = extracted_text.strip()\n",
        "    return extracted"
      ],
      "metadata": {
        "id": "3DPVQyZw0aJP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_language_name(short_code):\n",
        "    lang_map = {\n",
        "        'ar': 'Arabic',\n",
        "        'zh': 'Chinese (Traditional)',\n",
        "        'fr': 'French',\n",
        "        'de': 'German',\n",
        "        'it': 'Italian',\n",
        "        'ja': 'Japanese',\n",
        "        'ko': 'Korean',\n",
        "        'es': 'Spanish',\n",
        "        'th': 'Thai',\n",
        "        'tr': 'Turkish',\n",
        "        'en': 'English',\n",
        "    }\n",
        "    return lang_map.get(short_code, short_code)"
      ],
      "metadata": {
        "id": "yOf2XhuV0fDA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data_folder = \"./data/references/validation/\"\n",
        "jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n",
        "model_name = \"gemma3_instruct_4b_text_NER_wikidata\"\n",
        "output_prediction_dir = os.path.join(\"data/predictions\", model_name, \"validation\")\n",
        "os.makedirs(output_prediction_dir, exist_ok=True)\n",
        "\n",
        "wikidata_cache_path = os.path.join(\"./data\", \"wikidata_cache.csv\")\n",
        "wikidata_cache = load_wikidata_cache(wikidata_cache_path)"
      ],
      "metadata": {
        "id": "thERE8Yy9bJi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "log_path = os.path.join(output_prediction_dir, \"run.log\")\n",
        "logf = open(log_path, \"a\", encoding=\"utf-8\")  # Changed to append mode\n",
        "\n",
        "def log(message: str):\n",
        "    timestamp = datetime.datetime.now().isoformat()\n",
        "    logf.write(f\"[{timestamp}] {message}\\n\")\n"
      ],
      "metadata": {
        "id": "PJOW3AFY81L7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_path in jsonl_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    outfile_path = os.path.join(output_prediction_dir, filename)\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    pbar = tqdm.tqdm(total=len(data))\n",
        "    results = []\n",
        "    for idx, record in enumerate(data, 1):\n",
        "        id = record['id']\n",
        "        source = record['source']\n",
        "        source_locale = record['source_locale']\n",
        "        source_language = get_language_name(source_locale)\n",
        "        target_locale = record['target_locale']\n",
        "        target_language = get_language_name(target_locale)\n",
        "\n",
        "        log(f\"\\nProcessing ID: {id} | Text : {source} | Source : {source_language} | Target: {target_language}\\n\")\n",
        "        # --- Entity-aware translation ---\n",
        "        entities = extract_entities(source)\n",
        "\n",
        "        log(\"\\n🔎 Named Entities:\\n\")\n",
        "        for e in entities:\n",
        "            log(f\"- {e['text']} ({e['type']})\\n\")\n",
        "\n",
        "        enriched = enrich_entities(entities, target_locale[:2], wikidata_cache)  # pass cache\n",
        "\n",
        "        logf.write(\"\\n🌐 Wikidata Enriched Entities:\\n\")\n",
        "        for orig, trans in enriched:\n",
        "            log(f\"- {orig} → {trans}\\n\")\n",
        "\n",
        "        prompt = create_translation_prompt(source, enriched, target_language)\n",
        "\n",
        "        log(f\"\\n📝 Prompt Sent to Gemma:\\n{prompt}\\n\")\n",
        "\n",
        "        model_translation = translate_with_gemma(prompt).strip()\n",
        "\n",
        "\n",
        "\n",
        "        log(f\"\\n🗣️ Final Translated Output:\\n{model_translation}\\n\")\n",
        "\n",
        "        results.append({\n",
        "            \"id\": id,\n",
        "            \"source_language\": source_language,\n",
        "            \"target_language\": target_language,\n",
        "            \"text\": source,\n",
        "            \"prediction\": model_translation,\n",
        "        })\n",
        "        pbar.update(1)\n",
        "        if idx % 10 == 0 or idx == len(data):\n",
        "            with open(outfile_path, 'w', encoding='utf-8') as f:\n",
        "                for res in results:\n",
        "                    f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n",
        "            # Periodically persist cache\n",
        "            save_wikidata_cache(wikidata_cache, wikidata_cache_path)\n",
        "    log(f\"Translations saved to {outfile_path}\\n\")\n",
        "    pbar.close()\n",
        "save_wikidata_cache(wikidata_cache, wikidata_cache_path)\n",
        "logf.flush()\n",
        "logf.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9YgWcY30moM",
        "outputId": "b79e3fec-520b-47e9-8dbc-862a3a7e58e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 732/732 [1:00:27<00:00,  4.96s/it]\n",
            "100%|██████████| 722/722 [59:48<00:00,  4.97s/it]\n",
            "100%|██████████| 731/731 [59:21<00:00,  4.87s/it]\n",
            "100%|██████████| 739/739 [1:01:24<00:00,  4.99s/it]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}