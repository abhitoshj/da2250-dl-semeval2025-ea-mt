{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTS71Va5RQYW",
        "outputId": "7c73f25e-6e3a-4ec4-b835-e981bd4b377b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ca_KMSCi1r8A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/IISc/DL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QdofEcD01dw_"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Io35bD-J1qL2"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqHd4WvN8ofP",
        "outputId": "9f482605-fb49-4e63-a951-ad1c2dfc3bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/876.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.0/876.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.5/876.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-hub\n",
        "!pip install -q -U keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-3XQFEnd1toO"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_hub\n",
        "import requests\n",
        "from typing import List, Dict, Tuple\n",
        "import time\n",
        "import json\n",
        "import glob\n",
        "import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkGLe2bk1y0w",
        "outputId": "e597455a-75c7-40db-fc97-4bce263731cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/config.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 968/968 [00:00<00:00, 1.87MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/task.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3.23k/3.23k [00:00<00:00, 5.39MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/assets/tokenizer/vocabulary.spm...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.47M/4.47M [00:00<00:00, 9.93MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma3/keras/gemma3_instruct_4b_text/3/download/model.weights.h5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7.23G/7.23G [02:18<00:00, 56.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_4b_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L9GOBFhlz-PP"
      },
      "outputs": [],
      "source": [
        "def load_wikidata_cache(cache_path):\n",
        "    if os.path.exists(cache_path):\n",
        "        try:\n",
        "            df = pd.read_csv(cache_path)\n",
        "            cache = {(row['entity'], row['target_lang']): row['translation'] for _, row in df.iterrows()}\n",
        "            return cache\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QJ3QUlOn0Aor"
      },
      "outputs": [],
      "source": [
        "def save_wikidata_cache(cache, cache_path):\n",
        "    # cache: dict[(entity, target_lang)] -> translation\n",
        "    rows = [\n",
        "        {'entity': k[0], 'target_lang': k[1], 'translation': v}\n",
        "        for k, v in cache.items()\n",
        "    ]\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(cache_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gNF7J91f0Krz"
      },
      "outputs": [],
      "source": [
        "def query_wikidata(entity: str, target_lang: str = \"fr\", cache=None) -> str:\n",
        "    if cache is not None and (entity, target_lang) in cache:\n",
        "        return cache[(entity, target_lang)]\n",
        "    url = \"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"search\": entity,\n",
        "        \"language\": \"en\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    results = response.json().get(\"search\", [])\n",
        "\n",
        "    if not results:\n",
        "        translation = entity\n",
        "    else:\n",
        "        entity_id = results[0][\"id\"]\n",
        "        label_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n",
        "        label_resp = requests.get(label_url).json()\n",
        "        try:\n",
        "            labels = label_resp[\"entities\"][entity_id][\"labels\"]\n",
        "            translation = labels[target_lang][\"value\"] if target_lang in labels else entity\n",
        "        except Exception:\n",
        "            translation = entity\n",
        "    if cache is not None:\n",
        "        cache[(entity, target_lang)] = translation\n",
        "    return translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mxyx4QdH2Q1l"
      },
      "outputs": [],
      "source": [
        "def extract_entities(text: str) -> List[Dict[str, str]]:\n",
        "    # Prompt for entity extraction\n",
        "    prompt_template = '''<start_of_turn>user\n",
        "    Extract all named entities from the following text.\n",
        "    For each entity, output a JSON object with keys: text, type (PER, LOC, ORG, MISC), and score (confidence 0-1).\n",
        "    Output a JSON array.\n",
        "    Do not include ```json or ``` in the output.\n",
        "    Text: {text} <end_of_turn>\n",
        "    <start_of_turn>model\n",
        "    '''\n",
        "    response = gemma_lm.generate(prompt_template.format(text=text), max_length=500)\n",
        "\n",
        "    start_tag = \"<start_of_turn>model\"\n",
        "    end_tag = \"<end_of_turn>\"\n",
        "\n",
        "    start_index = response.find(start_tag)\n",
        "    end_index = response.find(end_tag, start_index + len(start_tag))\n",
        "\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        extracted_text = response[start_index + len(start_tag):end_index]\n",
        "    else:\n",
        "        extracted_text = response.strip()\n",
        "    extracted = extracted_text.strip()\n",
        "\n",
        "    try:\n",
        "        entities = json.loads(extracted)\n",
        "    except Exception:\n",
        "        entities = []\n",
        "\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Wsfytae-0TR1"
      },
      "outputs": [],
      "source": [
        "def enrich_entities(entities: List[Dict[str, str]], target_lang: str, cache=None) -> List[Tuple[str, str]]:\n",
        "    enriched = []\n",
        "    for ent in entities:\n",
        "        translated = query_wikidata(ent[\"text\"], target_lang, cache)\n",
        "        enriched.append((ent[\"text\"], translated))\n",
        "    return enriched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RxySKNZe0Wiv"
      },
      "outputs": [],
      "source": [
        "def create_translation_prompt(text: str, enriched_entities: List[Tuple[str, str]], target_lang: str) -> str:\n",
        "    entity_list = \"\\n\".join([f\"{orig} → {trans}\" for orig, trans in enriched_entities if orig != trans])\n",
        "\n",
        "    prompt_template = '''<start_of_turn>user\n",
        "    Translate the following sentence to {target_lang}.\n",
        "    Use the following known entity translations:\n",
        "    {entity_list}\n",
        "\n",
        "    Text: {text}\n",
        "    Only output the translated text.\n",
        "    Do not include any additional text or explanations.<end_of_turn>\n",
        "    <start_of_turn>model'''\n",
        "\n",
        "    prompt = prompt_template.format(text=text, target_lang=target_lang, entity_list=entity_list)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3DPVQyZw0aJP"
      },
      "outputs": [],
      "source": [
        "def translate_with_gemma(prompt: str) -> str:\n",
        "    response = gemma_lm.generate(prompt,  max_length=500)\n",
        "\n",
        "    start_tag = \"<start_of_turn>model\"\n",
        "    end_tag = \"<end_of_turn>\"\n",
        "\n",
        "    start_index = response.find(start_tag)\n",
        "    end_index = response.find(end_tag, start_index + len(start_tag))\n",
        "\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        extracted_text = response[start_index + len(start_tag):end_index]\n",
        "    else:\n",
        "        extracted_text = response.strip()\n",
        "    extracted = extracted_text.strip()\n",
        "    return extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yOf2XhuV0fDA"
      },
      "outputs": [],
      "source": [
        "def get_language_name(short_code):\n",
        "    lang_map = {\n",
        "        'ar': 'Arabic',\n",
        "        'zh': 'Chinese (Traditional)',\n",
        "        'fr': 'French',\n",
        "        'de': 'German',\n",
        "        'it': 'Italian',\n",
        "        'ja': 'Japanese',\n",
        "        'ko': 'Korean',\n",
        "        'es': 'Spanish',\n",
        "        'th': 'Thai',\n",
        "        'tr': 'Turkish',\n",
        "        'en': 'English',\n",
        "    }\n",
        "    return lang_map.get(short_code, short_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "thERE8Yy9bJi"
      },
      "outputs": [],
      "source": [
        "input_data_folder = \"./data/references/validation/\"\n",
        "jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n",
        "model_name = \"gemma3_instruct_4b_text_NER_wikidata\"\n",
        "output_prediction_dir = os.path.join(\"data/predictions\", model_name, \"validation\")\n",
        "os.makedirs(output_prediction_dir, exist_ok=True)\n",
        "\n",
        "wikidata_cache_path = os.path.join(\"./data\", \"wikidata_cache.csv\")\n",
        "wikidata_cache = load_wikidata_cache(wikidata_cache_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PJOW3AFY81L7"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "log_path = os.path.join(output_prediction_dir, \"run.log\")\n",
        "logf = open(log_path, \"a\", encoding=\"utf-8\")  # Changed to append mode\n",
        "\n",
        "def log(message: str):\n",
        "    timestamp = datetime.datetime.now().isoformat()\n",
        "    logf.write(f\"[{timestamp}] {message}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9YgWcY30moM",
        "outputId": "b79e3fec-520b-47e9-8dbc-862a3a7e58e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 732/732 [1:00:27<00:00,  4.96s/it]\n",
            "100%|██████████| 722/722 [59:48<00:00,  4.97s/it]\n",
            "100%|██████████| 731/731 [59:21<00:00,  4.87s/it]\n",
            "100%|██████████| 739/739 [1:01:24<00:00,  4.99s/it]\n"
          ]
        }
      ],
      "source": [
        "for file_path in jsonl_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    outfile_path = os.path.join(output_prediction_dir, filename)\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    pbar = tqdm.tqdm(total=len(data))\n",
        "    results = []\n",
        "    for idx, record in enumerate(data, 1):\n",
        "        id = record['id']\n",
        "        source = record['source']\n",
        "        source_locale = record['source_locale']\n",
        "        source_language = get_language_name(source_locale)\n",
        "        target_locale = record['target_locale']\n",
        "        target_language = get_language_name(target_locale)\n",
        "\n",
        "        log(f\"\\nProcessing ID: {id} | Text : {source} | Source : {source_language} | Target: {target_language}\\n\")\n",
        "        # --- Entity-aware translation ---\n",
        "        entities = extract_entities(source)\n",
        "\n",
        "        log(\"\\n🔎 Named Entities:\\n\")\n",
        "        for e in entities:\n",
        "            log(f\"- {e['text']} ({e['type']})\\n\")\n",
        "\n",
        "        enriched = enrich_entities(entities, target_locale[:2], wikidata_cache)  # pass cache\n",
        "\n",
        "        logf.write(\"\\n🌐 Wikidata Enriched Entities:\\n\")\n",
        "        for orig, trans in enriched:\n",
        "            log(f\"- {orig} → {trans}\\n\")\n",
        "\n",
        "        prompt = create_translation_prompt(source, enriched, target_language)\n",
        "\n",
        "        log(f\"\\n📝 Prompt Sent to Gemma:\\n{prompt}\\n\")\n",
        "\n",
        "        model_translation = translate_with_gemma(prompt).strip()\n",
        "\n",
        "\n",
        "\n",
        "        log(f\"\\n🗣️ Final Translated Output:\\n{model_translation}\\n\")\n",
        "\n",
        "        results.append({\n",
        "            \"id\": id,\n",
        "            \"source_language\": source_language,\n",
        "            \"target_language\": target_language,\n",
        "            \"text\": source,\n",
        "            \"prediction\": model_translation,\n",
        "        })\n",
        "        pbar.update(1)\n",
        "        if idx % 10 == 0 or idx == len(data):\n",
        "            with open(outfile_path, 'w', encoding='utf-8') as f:\n",
        "                for res in results:\n",
        "                    f.write(json.dumps(res, ensure_ascii=False) + '\\n')\n",
        "            # Periodically persist cache\n",
        "            save_wikidata_cache(wikidata_cache, wikidata_cache_path)\n",
        "    log(f\"Translations saved to {outfile_path}\\n\")\n",
        "    pbar.close()\n",
        "save_wikidata_cache(wikidata_cache, wikidata_cache_path)\n",
        "logf.flush()\n",
        "logf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/raghavendra/da2250-dl-semeval2025-ea-mt/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/raghavendra/da2250-dl-semeval2025-ea-mt/.venv/lib/python3.12/site-packages/torchmetrics/utilities/imports.py:23: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import DistributionNotFound, get_distribution\n",
            "2025-06-18 13:55:52.763388: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-18 13:55:52.776313: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750254952.792652    6024 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750254952.797736    6024 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1750254952.811258    6024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750254952.811295    6024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750254952.811297    6024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750254952.811298    6024 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-06-18 13:55:52.816067: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 73326.99it/s]\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../../../../home/raghavendra/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
            "Encoder model frozen.\n",
            "/home/raghavendra/da2250-dl-semeval2025-ea-mt/.venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
          ]
        }
      ],
      "source": [
        "from framework import download_comet_model\n",
        "comet_model = download_comet_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "from framework import calculate_comet_scores, calculate_meta_score\n",
        "\n",
        "model_name = \"gemma3_instruct_4b_text\"\n",
        "output_prediction_dir = os.path.join(\"data/predictions\", model_name, \"validation\")\n",
        "os.makedirs(output_prediction_dir, exist_ok=True)\n",
        "\n",
        "input_data_folder = \"data/references/validation\"\n",
        "jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n",
        "\n",
        "def calculate_scores(template_id):\n",
        "    scores_dir = os.path.join(output_prediction_dir, template_id, \"scores\")\n",
        "\n",
        "    if not os.path.exists(scores_dir):\n",
        "        os.makedirs(scores_dir, exist_ok=True)\n",
        "\n",
        "    for file_path in jsonl_files:\n",
        "        references_path = file_path\n",
        "        filename = os.path.basename(file_path)\n",
        "        predictions_path = os.path.join(output_prediction_dir, template_id, filename)\n",
        "\n",
        "        comet_score = calculate_comet_scores(\n",
        "            comet_model, \n",
        "            references_path, \n",
        "            predictions_path\n",
        "        )\n",
        "\n",
        "        correct_instances, total_instances, meta_score = calculate_meta_score(\n",
        "            references_path,\n",
        "            predictions_path)\n",
        "\n",
        "        evaluation_results = {\n",
        "            \"correct_instances\": correct_instances,\n",
        "            \"total_instances\": total_instances,\n",
        "            \"comet_score\": comet_score,\n",
        "            \"meta_score\": meta_score\n",
        "        }\n",
        "\n",
        "        evaluation_output_path = os.path.join(scores_dir, f\"{os.path.splitext(filename)[0]}.json\")\n",
        "        with open(evaluation_output_path, 'w', encoding='utf-8') as json_file:\n",
        "            json.dump(evaluation_results, json_file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All references have a corresponding prediction\n",
            "Created 1177 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA RTX A2000 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 37/37 [00:15<00:00,  2.35it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 91.33\n",
            "Loaded 722 instances.\n",
            "Loaded 722 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1260 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 40/40 [00:17<00:00,  2.23it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 90.97\n",
            "Loaded 731 instances.\n",
            "Loaded 731 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1229 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 39/39 [00:17<00:00,  2.20it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 92.97\n",
            "Loaded 739 instances.\n",
            "Loaded 739 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1316 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 42/42 [00:20<00:00,  2.06it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 90.00\n",
            "Loaded 724 instances.\n",
            "Loaded 724 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1268 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 40/40 [00:18<00:00,  2.15it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 92.23\n",
            "Loaded 730 instances.\n",
            "Loaded 730 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1409 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 45/45 [00:22<00:00,  1.99it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 92.88\n",
            "Loaded 723 instances.\n",
            "Loaded 723 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1660 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 52/52 [00:23<00:00,  2.20it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 91.98\n",
            "Loaded 745 instances.\n",
            "Loaded 745 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1654 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 52/52 [00:38<00:00,  1.37it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 87.19\n",
            "Loaded 710 instances.\n",
            "Loaded 710 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1260 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s]\n",
            "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 91.41\n",
            "Loaded 732 instances.\n",
            "Loaded 732 predictions.\n",
            "All references have a corresponding prediction\n",
            "Created 1544 instances\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Predicting DataLoader 0: 100%|██████████| 49/49 [00:21<00:00,  2.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average COMET score: 90.22\n",
            "Loaded 722 instances.\n",
            "Loaded 722 predictions.\n"
          ]
        }
      ],
      "source": [
        "calculate_scores(\"rag-wikidata\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
