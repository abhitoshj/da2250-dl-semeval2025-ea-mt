{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bba307",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d3b2e-5136-4bdc-a86f-e95a85cd5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import demjson3\n",
    "from langchain_ollama import OllamaLLM\n",
    "from comet.models import download_model, load_from_checkpoint\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from ftfy import fix_text\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from entity_extraction import (\n",
    "    extract_capitalized_phrases,\n",
    "    extract_after_prepositions,\n",
    "    extract_quoted_entities,\n",
    "    extract_hyphenated_entities,\n",
    "    extract_entities_with_numbers_or_roman,\n",
    "    validate_entities\n",
    ")\n",
    "from framework import extract_entity_translation, calculate_comet_scores, calculate_meta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcbd18",
   "metadata": {},
   "source": [
    "### 2. Create prompt to extract entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"texts\"],\n",
    "    template='''You are a named entity recognition (NER) expert.\n",
    " \n",
    "For each of the following English sentences, extract all named entities (e.g., people, places, organizations, TV series, movies, books).\n",
    " \n",
    "Instructions:\n",
    "- ONLY extract named entities that appear EXACTLY and VERBATIM in the sentence.\n",
    "- DO NOT return alternate names, inferred references, or canonical forms.\n",
    "- DO NOT perform translation, rewriting, or guessing.\n",
    "- DO NOT infer likely entities or use context to deduce names.\n",
    "- An entity is valid ONLY if it is an exact substring match found in the sentence.\n",
    "- If an entity is not present word-for-word in the sentence, DO NOT include it.\n",
    "- DO NOT return partial entities or reformatted names.\n",
    " \n",
    "Output format:\n",
    "- Return a single JSON array.\n",
    "- Each item must be an object with these two fields:\n",
    "  - \"Source\": the original sentence.\n",
    "  - \"Entities\": a list of string values. Each string must be copied directly from the sentence. Each must be an exact substring of the Source.\n",
    " \n",
    "Rules:\n",
    "- DO NOT include any reasoning or commentary.\n",
    "- DO NOT include any Markdown formatting like triple backticks.\n",
    "- The output MUST be valid JSON that can be parsed by Python's json.loads().\n",
    " \n",
    "Texts:\n",
    "{texts}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0899dc6",
   "metadata": {},
   "source": [
    "### 3. Create prompt to verify and refine locally extracted candidate entities from a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02603c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_rethinking_prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"candidate\"],\n",
    "    template=\"\"\"\n",
    "You are an expert in Named Entity Recognition (NER). Named entities can be people, places, organizations, TV series, movies, books, etc.\n",
    " \n",
    "Task:\n",
    "Given a sentence and a candidate phrase, your goal is to identify the **most complete named entity** from the sentence that includes the candidate. This helps verify whether the candidate is a full entity, a partial one, or invalid.\n",
    " \n",
    "Guidelines:\n",
    "- If the candidate is a **subset of a longer named entity**, return the **full entity** from the sentence as-is.\n",
    "- If the candidate **fully matches** a named entity in the sentence, return it.\n",
    "- If the candidate is **not part of any valid named entity** in the sentence, return an empty list.\n",
    "- Always return the named entity **verbatim**, exactly as it appears in the sentence (including casing, punctuation, etc.).\n",
    "- Do not add inferred terms or modify the sentence.\n",
    "- Named entities should not be a category or type.\n",
    " \n",
    "Response format:\n",
    "Return a valid JSON object with the following keys:\n",
    "- \"sentence\": the original input sentence.\n",
    "- \"entities\": a list with either the corrected entity string or an empty list if not found.\n",
    " \n",
    "Constraints:\n",
    "- Output only the JSON. No markdown, no code blocks, no commentary.\n",
    " \n",
    "Input:\n",
    "Sentence: \"{sentence}\"\n",
    "Candidate: \"{candidate}\"\n",
    " \n",
    "Output:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e83fd6",
   "metadata": {},
   "source": [
    "### 4. Create prompt to translate a sentence to respective language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"language\", \"entities\", \"exampleSentence\", \"exampleEntities\", \"exampleTranslation\", \"exampleTranslatedEntities\"], # type: ignore\n",
    "    template=\"\"\"\n",
    "You are a professional translator with expertise in high-fidelity, fluent translations that preserve named entities.\n",
    " \n",
    "Translate the following English sentence into {language}. The translation MUST meet the following constraints:\n",
    " \n",
    "1. The meaning is preserved **accurately** and the sentence reads naturally to native speakers.\n",
    "2. All named entities from the list {entities} MUST appear in the translated sentence.\n",
    "3. Use **natural phrasing and correct grammar** in {language}.\n",
    "4. Avoid literal word-for-word translation and aim for native-like fluency.\n",
    "5. Do NOT hallucinate or modify entity names. Only translate using the list provided.\n",
    "6. Do not include any code fragments such as ``` or ```json in the output.\n",
    "7. Do not include any additional text or explanations.\n",
    "8. Ensure all JSON fields are correctly separated by commas. Do not omit commas between items or key-value pairs.\n",
    "9. Ensure consistent determiners (e.g., \"la\", \"l’\") and capitalization for entities across translations.\n",
    "10. Use provided translated entity names exactly; if multiple entities exist, treat each one distinctly.\n",
    "\n",
    "Example:\n",
    "sentence: {exampleSentence}\n",
    "entities: {exampleEntities}\n",
    "language: {language}\n",
    "\n",
    "Expected output:\n",
    "{{\n",
    "  \"translation\": {exampleTranslation},\n",
    "  \"entities\": {exampleTranslatedEntities}\n",
    "}}\n",
    "\n",
    "Format your response strictly as:\n",
    "\n",
    "{{\n",
    "  \"translation\": \"<natural and accurate translated sentence>\"\n",
    "  \"entities\": [\"<translated_entity1>\", \"<translated_entity2>\", ...]\n",
    "}}\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {entities}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96b447",
   "metadata": {},
   "source": [
    "### 5. Create prompt to retry translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_retry_prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"language\", \"entities\", \"exampleSentence\", \"exampleEntities\", \"exampleTranslation\", \"exampleTranslatedEntities\"], # type: ignore\n",
    "    template=\"\"\"\n",
    "You are a professional translator. The original translation did not accurately preserve named entities.\n",
    "\n",
    "Retry translating the English sentence below into {language}, ensuring all named entities in {entities} are:\n",
    "1. Correctly translated into {language} (not hallucinated or omitted).\n",
    "2. Placed naturally in the sentence with fluent grammar.\n",
    "3. Return the response only for {sentence}. Do not include translations for any other sentence.\n",
    "4. The output must be a valid JSON string that can be parsed by the Python json.loads() function.\n",
    "5. Do not include any code fragments such as ``` or ```json in the output.\n",
    "6. Do not include any additional text or explanations.\n",
    "7. Ensure all JSON fields are correctly separated by commas. Do not omit commas between items or key-value pairs.\n",
    "\n",
    "Example:\n",
    "sentence: {exampleSentence}\n",
    "entities: {exampleEntities}\n",
    "language: {language}\n",
    "\n",
    "Expected output:\n",
    "{{\n",
    "  \"translation\": {exampleTranslation},\n",
    "  \"entities\": {exampleTranslatedEntities}\n",
    "}}\n",
    "\n",
    "Format your response strictly as:\n",
    "\n",
    "{{\n",
    "  \"translation\": \"<natural and accurate translated sentence>\"\n",
    "  \"entities\": [\"<translated_entity1>\", \"<translated_entity2>\", ...]\n",
    "}}\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {entities}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcd95a",
   "metadata": {},
   "source": [
    "### 6. Examples for one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ae3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = {\n",
    "    \"ar\": {\n",
    "        \"exampleSentence\": \"Where is the Burj Khalifa located?\",\n",
    "        \"exampleEntities\": [\"Burj Khalifa\"],\n",
    "        \"exampleTranslation\": \"أين يقع برج خليفة؟\",\n",
    "        \"exampleTranslatedEntities\": [\"برج خليفة\"]\n",
    "    },\n",
    "    \"zh\": {\n",
    "        \"exampleSentence\": \"When was the Great Wall of China built?\",\n",
    "        \"exampleEntities\": [\"Great Wall of China\"],\n",
    "        \"exampleTranslation\": \"中國長城是什麼時候建造的？\",\n",
    "        \"exampleTranslatedEntities\": [\"中國長城\"]\n",
    "    },\n",
    "    \"fr\": {\n",
    "        \"exampleSentence\": \"Who painted the Mona Lisa?\",\n",
    "        \"exampleEntities\": [\"Mona Lisa\"],\n",
    "        \"exampleTranslation\": \"Qui a peint la Joconde ?\",\n",
    "        \"exampleTranslatedEntities\": [\"la Joconde\"]\n",
    "    },\n",
    "    \"de\": {\n",
    "        \"exampleSentence\": \"Which river flows through Berlin?\",\n",
    "        \"exampleEntities\": [\"Berlin\"],\n",
    "        \"exampleTranslation\": \"Welcher Fluss fließt durch Berlin?\",\n",
    "        \"exampleTranslatedEntities\": [\"Berlin\"]\n",
    "    },\n",
    "    \"it\": {\n",
    "        \"exampleSentence\": \"Where is the Colosseum located?\",\n",
    "        \"exampleEntities\": [\"Colosseum\"],\n",
    "        \"exampleTranslation\": \"Dove si trova il Colosseo?\",\n",
    "        \"exampleTranslatedEntities\": [\"Colosseo\"]\n",
    "    },\n",
    "    \"ja\": {\n",
    "        \"exampleSentence\": \"Which city is Mount Fuji near?\",\n",
    "        \"exampleEntities\": [\"Mount Fuji\"],\n",
    "        \"exampleTranslation\": \"富士山はどの都市の近くにありますか？\",\n",
    "        \"exampleTranslatedEntities\": [\"富士山\"]\n",
    "    },\n",
    "    \"ko\": {\n",
    "        \"exampleSentence\": \"Who is the lead actor in Squid Game?\",\n",
    "        \"exampleEntities\": [\"Squid Game\"],\n",
    "        \"exampleTranslation\": \"오징어 게임의 주연 배우는 누구입니까?\",\n",
    "        \"exampleTranslatedEntities\": [\"오징어 게임\"]\n",
    "    },\n",
    "    \"es\": {\n",
    "        \"exampleSentence\": \"Where was Pablo Picasso born?\",\n",
    "        \"exampleEntities\": [\"Pablo Picasso\"],\n",
    "        \"exampleTranslation\": \"¿Dónde nació Pablo Picasso?\",\n",
    "        \"exampleTranslatedEntities\": [\"Pablo Picasso\"]\n",
    "    },\n",
    "    \"th\": {\n",
    "        \"exampleSentence\": \"Where can you see the Grand Palace in Thailand?\",\n",
    "        \"exampleEntities\": [\"Grand Palace\", \"Thailand\"],\n",
    "        \"exampleTranslation\": \"พระบรมมหาราชวังตั้งอยู่ที่ไหนในประเทศไทย?\",\n",
    "        \"exampleTranslatedEntities\": [\"พระบรมมหาราชวัง\", \"ประเทศไทย\"]\n",
    "    },\n",
    "    \"tr\": {\n",
    "        \"exampleSentence\": \"In which city is the Hagia Sophia located?\",\n",
    "        \"exampleEntities\": [\"Hagia Sophia\"],\n",
    "        \"exampleTranslation\": \"Ayasofya hangi şehirde bulunur?\",\n",
    "        \"exampleTranslatedEntities\": [\"Ayasofya\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33bf5a",
   "metadata": {},
   "source": [
    "### 7. Get folder and file path for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27c478-bd17-4db3-80ee-6f912a364a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_filepaths = {}\n",
    "\n",
    "def load_all_jsonl_files_by_language(folder_path):\n",
    "    lang_data = {}\n",
    "\n",
    "    for file_path in glob.glob(f\"{folder_path}/*.jsonl\"):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        lang_code = file_name.split(\"_\")[0]\n",
    "        language_filepaths[lang_code] = os.path.splitext(file_name)[0] \n",
    "\n",
    "        if lang_code not in lang_data:\n",
    "            lang_data[lang_code] = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                lang_data[lang_code].append(json.loads(line))\n",
    "\n",
    "    return lang_data\n",
    "\n",
    "\n",
    "def get_language_name(short_code):\n",
    "    lang_map = {\n",
    "        'ar': 'Arabic', 'zh': 'Chinese (Traditional)', 'fr': 'French', 'de': 'German',\n",
    "        'it': 'Italian', 'ja': 'Japanese', 'ko': 'Korean', 'es': 'Spanish',\n",
    "        'th': 'Thai', 'tr': 'Turkish', 'en': 'English'\n",
    "    }\n",
    "    return lang_map.get(short_code, short_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b44fa4",
   "metadata": {},
   "source": [
    "### 8. Retrieve data from JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01059ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_folder = \"data/references/validation\"\n",
    "all_lang_data  = load_all_jsonl_files_by_language(jsonl_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd69f0d",
   "metadata": {},
   "source": [
    "### 9. Verify the loaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a25dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_code, records in all_lang_data.items():\n",
    "    print(f\"Loaded {len(records)} records for {get_language_name(lang_code)} ({lang_code})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ffced",
   "metadata": {},
   "source": [
    "### 10. Handle comma related errors during parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_commas(raw_text):\n",
    "    fixed = re.sub(r'(\"\\s*)(\")', r'\\1,\\2', raw_text)  # insert missing comma between two quoted fields\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ba05d",
   "metadata": {},
   "source": [
    "### 11. Handle parsing JSON objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_model_output_parse(raw_output):\n",
    "\n",
    "    if isinstance(raw_output, dict):\n",
    "        return raw_output\n",
    "\n",
    "    if not isinstance(raw_output, str):\n",
    "        try:\n",
    "            raw_output = raw_output.decode('utf-8')\n",
    "        except:\n",
    "            raw_output = str(raw_output)\n",
    "\n",
    "    try:\n",
    "        fixed = fix_text(raw_output.strip())\n",
    "        \n",
    "        return json.loads(fixed)\n",
    "    except Exception:\n",
    "        try:\n",
    "            fixed = fix_text(raw_output.strip())\n",
    "            \n",
    "            return demjson3.decode(fixed)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to recover batch with demjson3: {e}\")\n",
    "            \n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff06c3d",
   "metadata": {},
   "source": [
    "### 12. Define LangChain with prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98049b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ollama\n",
    "llm = OllamaLLM(model=\"mistral\")\n",
    "chain_extract = entity_extraction_prompt | llm\n",
    "chain_rethink = entity_rethinking_prompt | llm\n",
    "chain_translate = translation_prompt | llm\n",
    "chain_retry_translate = translation_retry_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec8dc6",
   "metadata": {},
   "source": [
    "### 13. Extract named entities from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities_from_model(source):\n",
    "    try:\n",
    "        raw_entities = chain_extract.invoke({\"texts\": source})\n",
    "        entity_data = json.loads(raw_entities['text'])\n",
    "\n",
    "        return entity_data\n",
    "    except Exception:\n",
    "        try:\n",
    "            entity_data = demjson3.decode(raw_entities['text'])\n",
    "\n",
    "            return entity_data\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to recover batch with demjson3: {e2}\")\n",
    "            \n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f264151",
   "metadata": {},
   "source": [
    "### 14. Extract named entities locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities_locally(source):\n",
    "    return set(\n",
    "            extract_capitalized_phrases(source) +\n",
    "            extract_after_prepositions(source) +\n",
    "            extract_quoted_entities(source) +\n",
    "            extract_hyphenated_entities(source) +\n",
    "            extract_entities_with_numbers_or_roman(source)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb367a8",
   "metadata": {},
   "source": [
    "### 15. Refine locally extracted entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d15d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_locally_extracted_entities(source, local_entities, cleaned_entity_list):\n",
    "    for entity in local_entities:\n",
    "        if entity not in cleaned_entity_list:\n",
    "            correction = chain_rethink.invoke({\"sentence\": source, \"candidate\": entity})\n",
    "                \n",
    "            try:\n",
    "                new_data = json.loads(correction['text'])\n",
    "            except json.JSONDecodeError as e:\n",
    "                try:\n",
    "                    new_data = demjson3.decode(correction['text'])\n",
    "                except Exception as e2:\n",
    "                    print(f\"Failed to recover batch with demjson3: {e2}\")\n",
    "                    continue\n",
    "                \n",
    "            if new_data.get('entities'):\n",
    "                cleaned_entity_list.extend(new_data['entities'])\n",
    "                cleaned_entity_list.append(entity)\n",
    "\n",
    "    return list(set([x.strip() for x in cleaned_entity_list if x.strip()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c54fbc",
   "metadata": {},
   "source": [
    "### 16. Remove duplicate entities (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_entities(cleaned_entity_list):\n",
    "    duplicate_entities = []\n",
    "    \n",
    "    for i in range(len(cleaned_entity_list)):\n",
    "        for j in range(len(cleaned_entity_list)):\n",
    "            if i != j and cleaned_entity_list[i] in cleaned_entity_list[j]:\n",
    "                duplicate_entities.append(cleaned_entity_list[i])\n",
    "\n",
    "    final_entity_list = []\n",
    "    for ent in cleaned_entity_list:\n",
    "        if ent not in duplicate_entities:\n",
    "            final_entity_list.append(ent)\n",
    "\n",
    "    return final_entity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ec97f",
   "metadata": {},
   "source": [
    "### 17. Entity Translation via Retrieval (RAG Component), also includes finding the best match entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8038362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_named_entities(final_entity_list, lang_code):\n",
    "    model_entities = []\n",
    "    \n",
    "    for item in final_entity_list:\n",
    "        ent = extract_entity_translation(item, lang_code)\n",
    "            \n",
    "        if ent['qid']:\n",
    "            model_entities.append(ent['translated'])\n",
    "    \n",
    "    return model_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07b745",
   "metadata": {},
   "source": [
    "### 18. Perform translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(source, language, model_entities, record):\n",
    "    retry_cnt = 0\n",
    "    \n",
    "    # retry till entities are not part of translated sentence\n",
    "    while retry_cnt < 5:\n",
    "        retry_cnt += 1\n",
    "        \n",
    "        try:\n",
    "            if retry_cnt == 0:\n",
    "                raw_translated = chain_translate.invoke({\n",
    "                    \"sentence\": source,\n",
    "                    \"language\": language,\n",
    "                    \"entities\": \", \".join(model_entities),\n",
    "                    \"exampleSentence\": examples[record['target_locale']]['exampleSentence'],\n",
    "                    \"exampleEntities\": examples[record['target_locale']]['exampleEntities'],\n",
    "                    \"exampleTranslation\": examples[record['target_locale']]['exampleTranslation'],\n",
    "                    \"exampleTranslatedEntities\": examples[record['target_locale']]['exampleTranslatedEntities']\n",
    "                })\n",
    "            else:\n",
    "                raw_translated = chain_retry_translate.invoke({\n",
    "                    \"sentence\": source,\n",
    "                    \"language\": language,\n",
    "                    \"entities\": \", \".join(model_entities),\n",
    "                    \"exampleSentence\": examples[record['target_locale']]['exampleSentence'],\n",
    "                    \"exampleEntities\": examples[record['target_locale']]['exampleEntities'],\n",
    "                    \"exampleTranslation\": examples[record['target_locale']]['exampleTranslation'],\n",
    "                    \"exampleTranslatedEntities\": examples[record['target_locale']]['exampleTranslatedEntities']\n",
    "                })\n",
    "            \n",
    "            raw_translated = json.loads(raw_translated['text'])\n",
    "        except Exception:\n",
    "            \n",
    "            try:\n",
    "                cleaned = fix_missing_commas(raw_translated['text'])\n",
    "                fixed = ast.literal_eval(cleaned.replace(\"'\", '\"'))\n",
    "                raw_translated = safe_model_output_parse(fixed)\n",
    "\n",
    "                if raw_translated is None:\n",
    "                    print(\"Failed to parse model output, retrying...\")\n",
    "                    retry_cnt -= 1\n",
    "                    \n",
    "                    continue\n",
    "            except Exception as e2:\n",
    "                print(f\"Failed to recover batch with demjson3: {e2}\")\n",
    "                \n",
    "                continue\n",
    "            \n",
    "        generated_entities = raw_translated['entities']\n",
    "        entities_found = True\n",
    "\n",
    "        for entity in generated_entities:\n",
    "            if entity not in raw_translated['translation']:\n",
    "                entities_found = False\n",
    "                break\n",
    "            \n",
    "        for entity in model_entities:\n",
    "            if entity not in raw_translated['translation']:\n",
    "                entities_found = False\n",
    "                break\n",
    "            \n",
    "        if entities_found:\n",
    "            break\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168c7cd",
   "metadata": {},
   "source": [
    "### 19. Begin one - shot + RAG translation using LangChain for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_code, records in all_lang_data.items():\n",
    "    language = get_language_name(lang_code)\n",
    "\n",
    "    output_file = f\"data/predictions/mistral7b/validation/one_shot_rag_wikidata/{language_filepaths[lang_code]}.jsonl\"\n",
    "    results = []\n",
    "    \n",
    "    for record in records:\n",
    "\n",
    "        source = record['source']\n",
    "        \n",
    "        # Extract named entities\n",
    "        entity_data = extract_named_entities_from_model(source)\n",
    "        \n",
    "        if (entity_data == None):\n",
    "            continue\n",
    "\n",
    "        local_entities = extract_named_entities_locally(source)\n",
    "\n",
    "        cleaned_entity_list = []\n",
    "        if isinstance(entity_data, dict):\n",
    "            cleaned_entity_list.extend(validate_entities(entity_data.get('Entities', []), source))\n",
    "        elif isinstance(entity_data, list):\n",
    "            for item in entity_data:\n",
    "                cleaned_entity_list.extend(validate_entities(item.get('Entities', []), source))\n",
    "\n",
    "        # Refine entities\n",
    "        cleaned_entity_list = refine_locally_extracted_entities(source, local_entities, cleaned_entity_list)\n",
    "        \n",
    "        # Remove duplicate entries\n",
    "        final_entity_list = remove_duplicate_entities(cleaned_entity_list)\n",
    "\n",
    "        # Translate named entities using Wikidata\n",
    "        model_entities = translate_named_entities(final_entity_list, record['target_locale'])\n",
    "\n",
    "        # Translate sentence with constraint\n",
    "        raw_translated = translate_sentence(source, language, model_entities, record)\n",
    "\n",
    "        if (raw_translated == None):\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"id\": record['id'],\n",
    "            \"text\": source,\n",
    "            \"source_language\": record['source_locale'],\n",
    "            \"target_language\": record['target_locale'],\n",
    "            \"prediction\": raw_translated['translation']\n",
    "        })\n",
    "        \n",
    "        # Record the results to a file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for res in results:\n",
    "                f.write(json.dumps(res, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6614f3c",
   "metadata": {},
   "source": [
    "### 20. Define folder and file structure to save M-ETA and COMET scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16798a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)\n",
    "model_name = \"mistral7b\"\n",
    "output_prediction_dir = os.path.join(\"data/predictions\", model_name)\n",
    "os.makedirs(output_prediction_dir, exist_ok=True)\n",
    "\n",
    "input_data_folder = \"data/references/validation\"\n",
    "jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556f0e7",
   "metadata": {},
   "source": [
    "### 21. COMET and M-ETA scores calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(template_id):\n",
    "    scores_dir = os.path.join(output_prediction_dir, template_id, \"scores\")\n",
    "    \n",
    "    if not os.path.exists(scores_dir):\n",
    "        os.makedirs(scores_dir, exist_ok=True)\n",
    "\n",
    "    for file_path in jsonl_files:\n",
    "        references_path = file_path\n",
    "        filename = os.path.basename(file_path)\n",
    "        predictions_path = os.path.join(output_prediction_dir, template_id, filename)\n",
    "\n",
    "        comet_score = calculate_comet_scores(\n",
    "            comet_model, \n",
    "            references_path, \n",
    "            predictions_path\n",
    "        )\n",
    "\n",
    "        correct_instances, total_instances, meta_score = calculate_meta_score(\n",
    "            references_path,\n",
    "            predictions_path)\n",
    "\n",
    "        evaluation_results = {\n",
    "            \"correct_instances\": correct_instances,\n",
    "            \"total_instances\": total_instances,\n",
    "            \"comet_score\": comet_score,\n",
    "            \"meta_score\": meta_score\n",
    "        }\n",
    "\n",
    "        new_filename = filename.replace(\".jsonl\", \".json\")\n",
    "        evaluation_output_path = os.path.join(scores_dir, new_filename)\n",
    "        with open(evaluation_output_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(evaluation_results, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f318d25",
   "metadata": {},
   "source": [
    "### 22. Calculate COMET and M-ETA scores for quality evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_scores(\"one_shot_rag_wikidata\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
