{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import demjson3\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import LLMChain\n",
    "from comet.models import download_model, load_from_checkpoint\n",
    "import os\n",
    "\n",
    "from entity_extraction import (\n",
    "    extract_capitalized_phrases,\n",
    "    extract_after_prepositions,\n",
    "    extract_quoted_entities,\n",
    "    extract_hyphenated_entities,\n",
    "    extract_entities_with_numbers_or_roman,\n",
    "    validate_entities\n",
    ")\n",
    "from framework import extract_entity_translation, fetch_wikidata_label, calculate_comet_scores, calculate_meta_score\n",
    "from prompt_templates import (\n",
    "    entity_extraction_prompt,\n",
    "    entity_rethinking_prompt,\n",
    "    translation_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27c478-bd17-4db3-80ee-6f912a364a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving translated outputs to JSONL file\n",
    "\n",
    "language_filepaths = {}\n",
    "\n",
    "def load_all_jsonl_files_by_language(folder_path):\n",
    "    lang_data = {}\n",
    "\n",
    "    for file_path in glob.glob(f\"{folder_path}/*.jsonl\"):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        lang_code = file_name.split(\"_\")[0]\n",
    "        language_filepaths[lang_code] = os.path.splitext(file_name)[0] \n",
    "\n",
    "        if lang_code not in lang_data:\n",
    "            lang_data[lang_code] = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                lang_data[lang_code].append(json.loads(line))\n",
    "\n",
    "    return lang_data\n",
    "\n",
    "\n",
    "def get_language_name(short_code):\n",
    "    lang_map = {\n",
    "        'ar': 'Arabic', 'zh': 'Chinese (Traditional)', 'fr': 'French', 'de': 'German',\n",
    "        'it': 'Italian', 'ja': 'Japanese', 'ko': 'Korean', 'es': 'Spanish',\n",
    "        'th': 'Thai', 'tr': 'Turkish', 'en': 'English'\n",
    "    }\n",
    "    return lang_map.get(short_code, short_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01059ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_folder = \"data/references/validation\"\n",
    "all_lang_data  = load_all_jsonl_files_by_language(jsonl_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_code, records in all_lang_data.items():\n",
    "    print(f\"Loaded {len(records)} records for {get_language_name(lang_code)} ({lang_code})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LangChain chains with prompt templates\n",
    "\n",
    "# Use Ollama\n",
    "llm = OllamaLLM(model=\"mistral\")\n",
    "chain_extract = LLMChain(llm=llm, prompt=entity_extraction_prompt)\n",
    "chain_rethink = LLMChain(llm=llm, prompt=entity_rethinking_prompt)\n",
    "chain_translate = LLMChain(llm=llm, prompt=translation_prompt)\n",
    "\n",
    "results = []\n",
    "\n",
    "for lang_code, records in all_lang_data.items():\n",
    "    language = get_language_name(lang_code)\n",
    "\n",
    "    if lang_code == 'ar':\n",
    "        continue\n",
    "    elif lang_code == 'de':\n",
    "        records = records[240:]\n",
    "\n",
    "    output_file = f\"data/predictions/mistral7b/zero_shot/{language_filepaths[lang_code]}.jsonl\"\n",
    "    results = []\n",
    "\n",
    "    for record in records:\n",
    "\n",
    "        source = record['source']\n",
    "        wikidata_ids = [record['wikidata_id']]\n",
    "        \n",
    "        # Extract named entities\n",
    "        try:\n",
    "            raw_entities = chain_extract.invoke({\"texts\": source})\n",
    "            entity_data = json.loads(raw_entities['text'])\n",
    "        except Exception:\n",
    "            try:\n",
    "                entity_data = demjson3.decode(raw_entities['text'])\n",
    "            except Exception as e2:\n",
    "                print(f\"Failed to recover batch with demjson3: {e2}\")\n",
    "                continue\n",
    "\n",
    "        local_entities = set(\n",
    "            extract_capitalized_phrases(source) +\n",
    "            extract_after_prepositions(source) +\n",
    "            extract_quoted_entities(source) +\n",
    "            extract_hyphenated_entities(source) +\n",
    "            extract_entities_with_numbers_or_roman(source)\n",
    "        )\n",
    "\n",
    "        cleaned_entity_list = []\n",
    "        \n",
    "        if isinstance(entity_data, dict):\n",
    "            cleaned_entity_list.extend(validate_entities(entity_data.get('Entities', []), source))\n",
    "        elif isinstance(entity_data, list):\n",
    "            for item in entity_data:\n",
    "                cleaned_entity_list.extend(validate_entities(item.get('Entities', []), source))\n",
    "\n",
    "        # Rethink entities\n",
    "        for entity in local_entities:\n",
    "            if entity not in cleaned_entity_list:\n",
    "                correction = chain_rethink.invoke({\"sentence\": source, \"candidate\": entity})\n",
    "                \n",
    "                try:\n",
    "                    new_data = json.loads(correction['text'])\n",
    "                except json.JSONDecodeError as e:\n",
    "                    try:\n",
    "                        new_data = demjson3.decode(correction['text'])\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Failed to recover batch with demjson3: {e2}\")\n",
    "                        continue\n",
    "                \n",
    "                if new_data.get('entities'):\n",
    "                    cleaned_entity_list.extend(new_data['entities'])\n",
    "                    cleaned_entity_list.append(entity)\n",
    "\n",
    "        cleaned_entity_list = list(set([x.strip() for x in cleaned_entity_list if x.strip()]))\n",
    "        \n",
    "        # Remove duplicate entries\n",
    "        duplicate_entities = []\n",
    "        for i in range(len(cleaned_entity_list)):\n",
    "            for j in range(len(cleaned_entity_list)):\n",
    "                if i != j and cleaned_entity_list[i] in cleaned_entity_list[j]:\n",
    "                    duplicate_entities.append(cleaned_entity_list[i])\n",
    "\n",
    "        final_entity_list = []\n",
    "        for ent in cleaned_entity_list:\n",
    "            if ent not in duplicate_entities:\n",
    "                final_entity_list.append(ent)\n",
    "\n",
    "        # Translate named entities using Wikidata\n",
    "        model_entities = []\n",
    "        for item in final_entity_list:\n",
    "            ent = extract_entity_translation(item, record['target_locale'])\n",
    "            if ent['qid']:\n",
    "                model_entities.append(ent['translated'])\n",
    "        \n",
    "        wikidata_entity_names = [fetch_wikidata_label(qid, record['target_locale']) for qid in wikidata_ids]\n",
    "\n",
    "        # Translate sentence with constraint\n",
    "        try:\n",
    "            raw_translated = chain_translate.invoke({\n",
    "                \"sentence\": source,\n",
    "                \"language\": language,\n",
    "                \"entities\": \", \".join(model_entities)\n",
    "            })\n",
    "            raw_translated = json.loads(raw_translated['text'])\n",
    "        except Exception:\n",
    "            try:\n",
    "                raw_translated = demjson3.decode(raw_translated['text'])\n",
    "            except Exception as e2:\n",
    "                print(f\"Failed to recover batch with demjson3: {e2}\")\n",
    "                continue\n",
    "\n",
    "        results.append({\n",
    "            \"id\": record['id'],\n",
    "            \"text\": source,\n",
    "            \"source_language\": record['source_locale'],\n",
    "            \"target_language\": record['target_locale'],\n",
    "            \"prediction\": raw_translated['translation'],\n",
    "        })\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for res in results:\n",
    "                f.write(json.dumps(res, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1210a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate COMET and M-ETA scores for quality evaluation\n",
    "\n",
    "comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)\n",
    "model_name = \"mistral7b\"\n",
    "output_prediction_dir = os.path.join(\"data/predictions\", model_name)\n",
    "os.makedirs(output_prediction_dir, exist_ok=True)\n",
    "\n",
    "input_data_folder = \"data/references/validation\"\n",
    "jsonl_files = glob.glob(f\"{input_data_folder}/*.jsonl\")\n",
    "\n",
    "def calculate_scores(template_id):\n",
    "    scores_dir = os.path.join(output_prediction_dir, template_id, \"scores\")\n",
    "    \n",
    "    if not os.path.exists(scores_dir):\n",
    "        os.makedirs(scores_dir, exist_ok=True)\n",
    "\n",
    "    for file_path in jsonl_files:\n",
    "        references_path = file_path\n",
    "        filename = os.path.basename(file_path)\n",
    "        predictions_path = os.path.join(output_prediction_dir, template_id, filename)\n",
    "\n",
    "        comet_score = calculate_comet_scores(\n",
    "            comet_model, \n",
    "            references_path, \n",
    "            predictions_path\n",
    "        )\n",
    "\n",
    "        correct_instances, total_instances, meta_score = calculate_meta_score(\n",
    "            references_path,\n",
    "            predictions_path)\n",
    "\n",
    "        evaluation_results = {\n",
    "            \"correct_instances\": correct_instances,\n",
    "            \"total_instances\": total_instances,\n",
    "            \"comet_score\": comet_score,\n",
    "            \"meta_score\": meta_score\n",
    "        }\n",
    "\n",
    "        evaluation_output_path = os.path.join(scores_dir, filename)\n",
    "        with open(evaluation_output_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(evaluation_results, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7360495",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_scores(\"zero_shot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
